 1/1: x[1,2,3]
 1/2: x=[1,2,3]
 1/3: x=[1,2,3]
 1/4: x=[1,2,3]
 1/5:
x=[1,2,3]
x
 1/6:
x=[1,2,3]
x
 1/7:
x=[1,2,3]
x
 1/8:  x)5
 1/9:  x=5
1/10: x
1/11: y=8
1/12: y
1/13: Y
1/14: print(y)
1/15: print y
1/16: x,y=(1,2)
1/17: x
1/18: y
1/19: x1=5
1/20: type(x)
1/21: type(-6)
1/22: x2=2.333
1/23: type(x2)
1/24: int(x2)
1/25: float(x1)
1/26: x3=true
1/27: x3=True
1/28: type(x3)
1/29: "lalala"
1/30: 'f'
1/31: print (fff)
1/32: x4='ttt'
1/33: x4
1/34: print (x4)
1/35: y=10
1/36: z="dollars"
1/37: print(y+z)
1/38: str(y)
1/39: print (y+z)
1/40: yy=str(y)
1/41: print(yy+z)
1/42: z=" dollars"
1/43: print(yy+z)
1/44: print peter
1/45: 5+8
1/46: 8+8
1/47: 15/8
1/48: 16/3
1/49: 16%3
1/50: 5**3
1/51: 10 % 5
1/52: y = 5**3
1/53: y
1/54: y==125
1/55: y==126
1/56: if y==125 r=5
1/57: 10==100
1/58: 10==10
1/59:
#dddddddddddddddddddddddddddddddd
x=5
1/60: x
1/61: x="Friday"
1/62: x(3)
1/63: x[3]
1/64:
def formula(x):
    x=4
    return x

print (formula(3))
 2/1: False or not True and not False
 2/2: True and not False and True or not False
1/65:
def formula(x)
    if x<5:
        print ("si")
        
    elif x>10:
        print(">10")
        
    else:
        print("nada")
1/66:
def formula(x):
    if x<5:
        print ("si")
        
    elif x>10:
        print(">10")
        
    else:
        print("nada")
1/67: print(formula(100))
1/68: print(formula(100))
1/69:
def formula(x):
    if x<5:
        print ("si")
        
    elif x>10:
        print(">10gfggfgf")
        
    else:
        print("nada")
1/70: print(formula(100))
1/71: print(formula(100))
1/72:
def formula(x):
    if x<5:
        return "<5"
        
    elif x>10:
        return ">10"
        
    else:
        return "nada"
1/73: print(formula(100))
1/74:
def formula(x):
    if x<5:
        return "<5"
        
    elif x>10:
        return ">10"
        
    else:
        return "nada"
1/75: print(formula(100))
1/76: print(formula(-888))
1/77: print(formula(-8ù))
1/78: print(formula(-8))
1/79: print(formula(8))
1/80:
def aliquote(y,x,z):
    y=25
    x=20
    z=15
    
    return total=x+y+z+25
1/81:
def aliquote(y,x,z):
    y=25
    x=20
    z=15
    total=x+y+z+25
    
    return total
1/82:
def aliquote(y,x,z):
   
    total=x+y+z+25
    
    return total
1/83: aliquote(20,28,30)
1/84:
def aliquote(y,x,z):
   
    total=x+y+z
    
    return total
1/85: aliquote(20,28,30)
1/86: return total
1/87: totasl
1/88: total
1/89: print(total)
1/90:
def aliquote(y,x,z):
   
    total=x+y+z
    
    return total
1/91: aliquote(20,28,30)
1/92: aloquote
1/93: aliquote(x,y,z)
1/94: aliquote(0,1,0)
1/95:
def aliquote(x):
   
    total=x+50
    
    return total
1/96:
def aliquote(x):
   
    total=40*4*x
    
    return total
def LR(total):
    total_2=total*0.8
1/97: aliquote(21)
1/98: aliquote(22)
1/99: LR
1/100: aliquote(22)
1/101: aliquote(22)
1/102: LR(aliquote(22))
1/103:
def aliquote(x):
   
    total=40*4*x
    
    return total
def LR(total):
    total_2=total*0.8
1/104: aliquote(x)
1/105: aliquote(20)
1/106: aliquote(20), LR(total)
1/107:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.8
1/108:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.8
1/109: aliquote(20), LR(20)
1/110:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.8
    return total_2
1/111: aliquote(20), LR(20)
1/112: aliquote(22), LR(22)
1/113:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.9
    return total_2
1/114: aliquote(22), LR(22)
1/115:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.95
    return total_2
1/116: aliquote(22), LR(22)
1/117:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)/160*80/160
    return total_2
1/118: aliquote(22), LR(22)
1/119:
def aliquote(x):
   
    total=160*x
    
    return total
def LR(x):
    total_2=aliquote(x)*0.8
    return total_2
1/120: aliquote(22), LR(22)
1/121:
def aliquote(x):
   
    total=x*22
    
    return total
def LR(x):
    total_2=aliquote(x)-aliquote(x)*0.9
    return total_2
1/122: aliquote(160), LR(160)
1/123:
def aliquote(x):
   
    total=x*22
    
    return total
def LR(x):
    total_2=aliquote(x)*0.9
    return total_2
1/124: aliquote(160), LR(160)
1/125:
def aliquote(x):
   
    total=x*22
    
    return total
def LR(x):
    total_2=aliquote(x)*0.95
    return total_2
1/126: aliquote(160), LR(160)
 3/1:
def function(x):
    add=x+5
    return add
def other(x):
    add2=function(x)*3
    return add2
 3/2: function(5), other(7)
 3/3: function(5), other(5)
 3/4:
def John(x):
    total=x+10
    return total
 3/5:
if x>=100:
    John(x)
else:
    x
 3/6:
def John(x):
    total=x+10
    return total
 3/7:
if x>=100:
    John(x)
else:
    x
 3/8:
x=14
if x>=100:
    John(x)
else:
    x
 3/9:
x=140
if x>=100:
    John(x)
else:
    x
3/10:
x=140
if x>=100:
    John(x)
else:
    x
3/11:
x=140
if x>=100:
    return John(x)
else:
    x
3/12:
def John(x):
    total=x+10
    return total
3/13:
x=140
if x>=100:
    return total
else:
    x
3/14:
x=140
if x>=100:
    Print(Jonh(x))
else:
    x
3/15:
def John(x):
    total=x+10
    return total
3/16:
x=140
if x>=100:
    Jonh(x)
else:
    x
3/17:
x=140
if x>=100:
    Jonh(x)
else:
    x
3/18:
if x>=100:
    Jonh(x)
else:
    x
3/19:
def John(x):
    total=x+10
    return total
3/20: John(130)
3/21:
def John(x):
    if x>=100:
    total=x+10
    else:
    total=x
    return total
3/22:
def John(x):
    if x>=100:
    total=x+10
else:
    total=x
    return total
3/23:
def John(x):
    if x>=100:
    total=x+10
else:
    total=x
    
    return total
3/24:
def John(x):
    if x>=100:
    total=x+10
    else:
    total=x
    
    return total
3/25:
def John(x):
    if x>=100:
    total=x+10
    else:
    total=x
return total
3/26:
def John(x):
    if x>=100:
    total=x+10
    else:
    total=x
    return total
3/27:
def John(x):
        if x>=100:
        total=x+10
        else:
        total=x
    return total
3/29:
def John(x):
    
        if x>=100:
        total=x+10
        else:
        total=x
    return total
3/31:
def John(x):
    
        if x>=100:
        total=x+10
        else:
        total=x
        
    return total
3/33:
def John(x):
    
        if x>=100:
        total=x+10
        else:
        total=x
        return total
3/34:
def John(x):
    
        if x>=100:
            total=x+10
        else:
            total=x
        return total
3/35:
def John(x):
    
        if x>=100:
            total=x+10
        else:
            total=x
        return total
3/36: John(100)
3/37: John(10)
3/38:
def John(x):
    
        if x>=100:
            total=x+10
            message=str(total)+" with bonus"
        else:
            total=x
            message=str(total)+" is not enough for bonus"
        return message
3/39: John(10)
3/40: John(100)
3/41:
def John(x):
    
        if x>=100:
            total=x+10
            message=str(total)+" with 10 bonus"
        else:
            total=x
            message=str(total)+" NO bonus"
        return message
3/42: John(100)
3/43: John(99)
3/44:
def lala(x,y,z):
    print (x)
    print (y)
    Print (z)
    result=x+y+z
    return result
3/45: lala(1,2,3)
3/46:
def lala(x,y,z):
    print (x)
    print (y)
    print (z)
    result=x+y+z
    return result
3/47: lala(1,2,3)
3/48:
def lala(x,y,z):
    print ("lala",x)
    print (y)
    print (z)
    result=x+y+z
    return result
3/49: lala(1,2,3)
3/50:
def lala(x,y,z):
    print ("lala",x)
    print ("lalaq",y)
    print (z)
    result=x+y+z
    return result
3/51: lala(1,2,3)
3/52: type(x)
3/53: type()
3/54: avrg(1,2,3)
3/55: sum(1,2,3,4,4,4,4,4,4)
3/56: list[1,2,3,4,4,4]
3/57: list=[1,2,3,4,4,4]
3/58: sum(list)
3/59: sum([1,1,1,1,1,1,1,1,1,1,1])
3/60: len(sjfkdshffdsguiwd)
3/61: len("sjfkdshffdsguiwd")
3/62: len("sjfkdshffdsgdgggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggguiwd")
3/63: array=["uno","dos","tres"]
3/64: array(1)
3/65: array[1]
3/66: array[-8]
3/67: array[-1]
3/68: array[-2]
3/69: append.array["cuatro"]
3/70: print(array[1])
3/71: array[3]=lalalala
3/72: array[3]="lalalala"
3/73: array[3]= "lalalala"
3/74: array[2]= "lalalala"
3/75: array
3/76: array
3/77: del array[1]
3/78: array
3/79: append(array["lalalala","yyyyyyyyyy"])
3/80: array.append("fffffff","sssssssssss")
3/81: array.append(["fffffff","sssssssssss"])
3/82: array
3/83: array[4]
3/84: array.append(["fffffff","sssssssssss","33333333"])
3/85: array
3/86: array[4]
3/87: array[4]
3/88: array.append(["fffffff","sssssssssss","33333333"])
3/89: array
3/90: array
3/91: array=["uno","dos","tres"]
3/92: array
3/93: array.append(["fffffff"])
3/94: array
3/95: array=["uno","dos","tres"]
3/96: array.append("fffffff")
3/97: array
3/98: array.extend(["rrrrrrrrrrrr","wwwwwwwwww"])
3/99: array
3/100: array[1:3]
3/101: array[0:3]
3/102: array[-1:]
3/103: array[::]
3/104: array[-2:]
3/105: array[-3:]
3/106: array2=["22222222222","33333333333"]
3/107: BIG=[array,array2]
3/108: BIG
3/109: BIG[-1]
3/110: sort(array)
3/111: array.sort()
3/112: array
3/113: array.sort(reverse=true)
3/114: array.sort(reverse=True)
3/115: array
3/116: ARRAY=1,2,3
3/117: ARRAY
3/118: A,B,C=1,2,3
3/119: A
3/120: AGE, YEAR="17,4".split(,)
3/121: AGE, YEAR="17,4".split(",")
3/122: age
3/123: AGE
3/124: YEAR
3/125: AGE, YEAR
3/126:
def salario(x):
    con=x*160
    sin=x*80
    return con,sin
3/127: salario(22)
3/128: array={k1:1,k2:2,k3:3}
3/129: array={"k1":1}
3/130: array={"k1":1,"k2":2;"k3"=3}
3/131: array={"k1":1,"k2":2;"k3":3}
3/132: array={"k1":1,"k2":2,"k3":3}
3/133: k1
3/134: array
3/135: array["k1"]
3/136: array["k5":5]
3/137: array=["k5":5]
3/138: array["k5"]=5
3/139: array
3/140:
for n in even:
    print(n)
3/141:
for n in even
    print(n)
3/142:
for n in even:
    print(n)
3/143: even
3/144: even
3/145: even
3/146: even[1]
3/147: even
3/148: print(even_)
3/149: print(even)
3/150: list=[1,2,3,4,4,4]
3/151: list
3/152: even
3/153: uuuu
3/154: list
3/155:
for n in list:
    print(n)
3/156:
for n in list:
    print(n)
3/157:
for n in list:
    print(n, end("-"))
3/158:
for n in list:
    print(n, end(" "))
3/159:
for n in list:
    print(n, end="-")
3/160:
for n in list:
    print(n, end="popopopo")
3/161:
x=0
while x<=20:
    print(x)
    x=x+1
3/162:
x=0
while x<=20:
    print(x, end=" ")
    x=x+1
3/163:
x=0
while x<=20:
    print(x, end=" ")
    x=x+0.001
3/164:
x=[]
while x<=20:
    print(x, end=" ")
    x=x+0.001
    x=[x]
3/165:
x=0
while x<=20:
    print(x, end=" ")
    x=x+0.001
    x=[x]
3/166:
x=0
while x<=20:
    print(x, end=" ")
    x=x+0.001
3/167:
x=0
while x<=20:
    print(x, end=" ")
    x=x+0.00
    1
3/168:
x=0
while x<=20:
    print(x)
    x=x+2
3/169:
x=0
while x<=20:
    print(x,end=" ")
    x=x+2
3/170:
x=0
while x<=20:
    print(x,end=" ")
    x=x+0.01
3/171:
x=0
array=[]
while x<=20:
    print(x,end=" ")
    x=x+0.01
    array[x]=x
    print(array)
3/172:
x=0
array=
while x<=20:
    print(x,end=" ")
    x=x+0.01
    array[x]=x
    print(array)
3/173:
x=0
array=[]
while x<=20:
    print(x,end=" ")
    x=x+0.01
    array.append(x)
    print(array)
3/174:
x=0
array=[1]
while x<=20:
    print(x,end=" ")
    x=x+0.01
    array.append(x)
    print(array)
3/175:
x=0
array=[1]
while x<=20:
    print(x,end=" ")
    x=x+0.1
    array.append(x)
    print(array)
3/176:
x=0
array=[]
while x<=20:
    print(x,end=" ")
    x=x+0.1
    array.append(x)
    print(array)
3/177:
x=0
array=[]
while x<=20:
    print(x,end=" ")
    x=x+0.1
    array.append(x)
    
print(array)
3/178:
x=0
array=[]
while x<=20:
    print(x,end=" ")
    x=x+1
    array.append(x)
    
print(array)
3/179:
x=1
array=[]
while x<=20:
    print(x,end=" ")
    x=x+1
    array.append(x)
    
print(array)
3/180:
x=1
array=[]
while x<=20:
    print(x,end=" ")
    x=x+1
    array.append(x)
    
print(array)
3/181:
x=1
array=[]
while x<=20:
    print(x,end=" ")
    array.append(x)
    x=x+1

    
print(array)
3/182:
x=1
array=[]
while x<=20:
    #print(x,end=" ")
    array.append(x)
    x=x+1

    
print(array)
3/183: array
3/184: array
3/185: array
3/186: range(10)
3/187: range(1,20,5)
3/188: lklkl=range(1,20,5)
3/189: lklkl
3/190: lklkl(range(1,20,5))
3/191: list(range(1,20,5))
3/192: lis(range(10))
3/193: list(range(10))
3/194: list(range(10))
3/195: range(13)
3/196: list(range(10))
3/197: list(range(10))
3/198: list(range(10))
3/199: list(range(10))
3/200: lis
3/201: list(range(10))
 4/1: range(1,10)
 4/2: list(range(0,19))
3/202: list(range(0,10))
 4/3: list(range(0,20))
3/203: list(range(0,20))
 4/4: list
 4/5: print(list)
 4/6: range(0,31,2)
 4/7: list(range(0,31,2))
 4/8: import math
 4/9: sqrt(9)
4/10: math.sqrt(9)
4/11: import sqrt from math
4/12: from math import sqrt
4/13: sqrt(8)
4/14: from math import sqrt as raiz
4/15: sqrt(8)
4/16: raiz(8)
4/17: help(math)
4/18: help(math.sqrt)
4/19: help(math.sum)
4/20: help(math.floor)
4/21: import numpy
4/22:
import pandas as pd
import matplot.pyplot as plt
4/23:
import pandas as pd
import matplotlib.pyplot as plt
4/24: help(matplotlib)
4/25: hel(pandas)
4/26: help(pandas)
4/27: help(pd)
4/28: help(plt)
4/29:
import pandas as pd
import matplotlib.pyplot as plt
import statmodels.api as sm
4/30:
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
4/31:
import numpy
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
 5/1: import matplot.lib as plt
 5/2: import matplotlib as plt
 5/3:
import matplotlib as plt
import pandas as pd
 5/4: data=pd.read_csv("1.01. Simple linear regression")
 5/5: data=pd.read_csv("1.01. Simple linear regression.csv")
 5/6: data=pd.read_csv("C:\Users\aleja\Desktop\MBA\Carrer devlp\2. Python\The Data Science Course 2020 - All Resources\Part_5_Advanced_Statistical_Methods_(Machine_Learning)\S32_L187\1.01. Simple linear regression.csv")
 5/7: mypath
 5/8: data=pd.read_csv("1.01. Simple linear regression.csv")
 5/9: data
5/10: data.describe()
5/11:
y="SAT"
x1="GPA"
5/12:
y=data["SAT"]
x1=data["GPA"]
5/13: y
5/14: x1
5/15: plt(y,x1)
5/16: plt.lines(y,x1)
5/17: plt.lines(x1,y)
5/18: plt.scatter(x1,y)
5/19:
plt.scatter(x1,y)
plt.show()
5/20:
import matplotlib as plt
import pandas as pd
5/21:
import matplotlib.pyplot as plt
import pandas as pd
5/22:
plt.scatter(x1,y)
plt.show()
5/23:
plt.line(x1,y)
plt.show()
5/24:
plt.lines(x1,y)
plt.show()
5/25: help(matplotlib.pyplot)
5/26: help(pyplot)
5/27: help(plt)
5/28:
plt.scatter(x1,y)
plt.show()
5/29:
plt.scatter(x1,y)
plt.xlabel("GPA")
plt.show()
5/30:
plt.scatter(x1,y)
plt.xlabel("GPA")
plt.ylabel("GPA")
plt.show()
5/31:
plt.scatter(x1,y)
plt.xlabel("GPA")
plt.ylabel("SAT")
plt.show()
5/32:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize(15))
plt.ylabel("SAT")
plt.show()
5/33:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15
plt.ylabel("SAT")
plt.show()
5/34:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT")
plt.show()
5/35:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/36:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels as sm
5/37: x=sm.add_constant(x1)
5/38:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
5/39:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels as sm
5/40: help(sm)
5/41:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
5/42:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
5/43:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
5/44:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/45:
x=sm.add_constant(x1)
results=sm.OLS(x,y).fit()
results.summary()
5/46:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/47:
x=sm.add_constant(y)
results=sm.OLS(x,y).fit()
results.summary()
5/48:
y=data["SAT"]
x1=data["GPA"]
5/49:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/50:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/51:
plt.scatter(x1,y)
yyy=1028.6407+245.2176*x1
fig=plt.plot(yyy)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/52:
plt.scatter(x1,y)
yyy=1028.6407+245.2176*x1
fig=plt.plot(x1,yyy)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/53:
plt.scatter(x1,y)
yyy=1028.6407+245.2176*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/54:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
5/55:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/56:
plt.scatter(x1,y)
yyy=1028.6407+245.2176*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/57:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/58: rrr=(1850-1028.6407)/24532176
5/59: rrr
5/60: rrr=(1850-1028.6407)/245.32176
5/61: rrr
5/62: rrr=(1850-1028.6407)/245.2176
5/63: rrr
5/64:
y=data["GPA"]
x1=data[SATGPA"]
5/65:
y=data["GPA"]
x1=data["SAT"]
5/66:
plt.scatter(x1,y)
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/67:
plt.scatter(x1,y)
plt.xlabel("SAT", fontsize=15)
plt.ylabel("GPA", fontsize=15)
plt.show()
5/68:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/69:
plt.scatter(x1,y)
yyy=1028.6407+245.2176*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/70:
plt.scatter(x1,y)
yyy=0.275+0.0017*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("GPA", fontsize=15)
plt.ylabel("SAT", fontsize=15)
plt.show()
5/71: rrr=(1850-0.275)/0.0017
5/72: rrr
5/73:
x1=1850
yyy=0.275+0.0017*x1
5/74:
x1=1850
yyy=0.275+0.0017*x1
5/75: yyy
5/76:
x1=1850
yyy=0+0.0017*x1
5/77: yyy
5/78: data=pd.read_csv("1.02. Simple linear regression.csv")
5/79: data=pd.read_csv("1.02. Multiple linear regression")
5/80:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/81: data=pd.read_csv("1.02. Multiple linear regression")
5/82: data=pd.read_csv("1.02. Multiple linear regression")
5/83: data=pd.read_csv("1.01. Simple linear regression")
5/84: data=pd.read_csv("1.01. Simple linear regression")
5/85:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/86: data=pd.read_csv("1.01. Simple linear regression")
5/87:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/88: data=pd.read_csv("1.01. Simple linear regression")
5/89: data
5/90: data.describe()
5/91: data2=pd.read_csv("1.01. Simple linear regression")
 2/3:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
 2/4:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/92:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/93: data=pd.read_csv("1.01. Simple linear regression")
5/94:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/95: data=pd.read_csv("1.01. Simple linear regression")
5/96: data=pd.read_csv("1.01. Simple linear regression.csv")
5/97: data=pd.read_csv("1.02. Multiple linear regression.csv")
5/98: data
5/99: data.describe()
5/100:
y=data["GPA"]
x1=data[["SAT","Rand 1,2,3"]
5/101:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/102:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/103:
y=data["GPA"]
x1=data[["SAT","Rand 1,2,3"]
5/104:
y=data["GPA"]
x1=data[["SAT","Rand 1,2,3"]]
5/105:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
 7/1:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
 7/2: data=pd.read_csv("real_estate_price_size_year.csv")
 7/3: data
 7/4: data.describe()
 7/5: data.summary()
 7/6:
y=data["price"]
x1=data["size"]
 7/7:
y=data["price"]
x1=data["size"]
 7/8:
y=data["price"]
x1=data["size"]
 7/9:
plt.scatter(x1,y)
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
7/10:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
7/11:
plt.scatter(x1,y)
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
yyy=1.019e+05+223.1787*x1
fig=plt.plot(x1,yyy, c="red")
plt.show()
7/12:
y=data["price"]
x2=data[["size", "year"]]
7/13:
x=sm.add_constant(x2)
results=sm.OLS(y,x).fit()
results.summary()
7/14:
y=data["price"]
x1=data["year"]
7/15:
plt.scatter(x1,y)
plt.xlabel("price", fontsize=15)
plt.ylabel("year", fontsize=15)
plt.show()
7/16:
y=data["price"]
x1=data["year"]
5/106: data=pd.read_csv("1.01. Simple linear regression.csv")
5/107: data=pd.read_csv("real_estate_price_size_year_view.csv")
5/108: data=pd.read_csv("real_estate_price_size_year_view.csv")
5/109: data
5/110: data=raw_data.copy()
5/111:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
sns.set()
5/112: data=raw_data.copy()
5/113: data=data.copy()
5/114: data
5/115: data["view"]=data["view"].map({"No sea view"=0,"Sea view"=1})
5/116: data["view"]=data["view"].map({"No sea view":0,"Sea view":1})
5/117: data
5/118: data.describe()
5/119:
y=data["price"]
x1=data[["size","view"]]
5/120:
x=sm.add_constant(x1)
results=sm.OLS(y,x).fit()
results.summary()
5/121:
plt.scatter(x1,y)
yyy=7.748e+04+218.7521*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/122:
plt.scatter(data["price"],y)
yyy=7.748e+04+218.7521*x1
fig=plt.plot(x1,yyy, c="red")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/123:
plt.scatter(data["price"],y)
yyy_wo=7.748e+04+218.7521*x1
yyy_w=7.748e+04+218.7521*x1+5.756e+04
fig=plt.plot(x1,yyy_wo, c="red")
fig=plt.plot(x1,yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/124:
plt.scatter(data["price"],y)
yyy_wo=7.748e+04+218.7521*data["price"]
yyy_w=7.748e+04+218.7521*data["price"]+5.756e+04
fig=plt.plot(x1,yyy_wo, c="red")
fig=plt.plot(x1,yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/125:
plt.scatter(data["price"],y)
yyy_wo=7.748e+04+218.7521*data["price"]
yyy_w=7.748e+04+218.7521*data["price"]+5.756e+04
fig=plt.plot(data["price"],yyy_wo, c="red")
fig=plt.plot(data["price"],yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/126: y
5/127: x1
5/128: data["price"]
5/129:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4
fig=plt.plot(data["price"],yyy_wo, c="red")
fig=plt.plot(data["price"],yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/130: yyy_wo
5/131: yyy_wo,yyy_w
5/132:
plt.scatter(data["price"],y)

plt.show()
5/133:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4

plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/134: 7.748*10**4
5/135:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4
fig=plt.plot(data["price"],yyy_wo, c="red")
fig=plt.plot(data["price"],yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/136:
plt.scatter(data["price"],y)

fig=plt.plot(data["price"],yyy_wo, c="red")
fig=plt.plot(data["price"],yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/137:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4

plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/138:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4
fig=plt.plot(data["price"],yyy_wo, c="red")

plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/139:
plt.scatter(data["price"],y)
yyy_wo=7.748*10**4+218.7521*data["price"]
yyy_w=7.748*10**4+218.7521*data["price"]+5.756*10**4

plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/140:
plt.scatter(data["size"],y)
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("price", fontsize=15)
plt.ylabel("size", fontsize=15)
plt.show()
5/141:
plt.scatter(data["size"],y)
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/142:
plt.scatter(data["size"],y,c=data["view"],cmap="red")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/143:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn_r")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/144:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn_r")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/145:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/146:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn_r")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/147:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/148:
plt.scatter(data["size"],y,c=data["view"],cmap="GnYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/149:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="green")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/150:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="red")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/151:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/152:
plt.scatter(data["size"],y,c=data["view"],cmap="redYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/153:
plt.scatter(data["size"],y,c=data["view"],cmap="RdYlGn")
yyy_wo=7.748*10**4+218.7521*data["size"]
yyy_w=7.748*10**4+218.7521*data["size"]+5.756*10**4
fig=plt.plot(data["size"],yyy_wo, c="red")
fig=plt.plot(data["size"],yyy_w, c="green")
plt.xlabel("size", fontsize=15)
plt.ylabel("price", fontsize=15)
plt.show()
5/154: x
5/155: new_data=pd.DataFrame({"const":1,"size":[4000,660],"view":[1,0]})
5/156: new_data
5/157: new_data.rename(index={0:"Tucacas",1:"la Dolorita"}
5/158: new_data.rename(index={0:"Tucacas",1:"la Dolorita"})
5/159: Predictions=DataFrame{"predictions":Predictions}
5/160: Predictions=DataFrame({"predictions":Predictions})
5/161: Predictions=pd.DataFrame({"predictions":Predictions})
5/162:
Predictions=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictions)
5/163:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictions)
5/164:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
5/165:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
5/166: Predictions=results.predict(new_data)
5/167:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
5/168:
Predictions=results.predict(new_data)
Predictions
5/169: new_data=pd.DataFrame({"const":1,"size":[000,660],"view":[1,0]})
5/170: new_data=pd.DataFrame({"const":1,"size":[2000,660],"view":[1,0]})
5/171: new_data
5/172: new_data.rename(index={0:"Tucacas",1:"la Dolorita"})
5/173:
Predictions=results.predict(new_data)
Predictions
5/174:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
5/175: new_data
5/176:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
5/177: new_data
5/178:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
joined.rename(index={0:"Tucacas",1:"la Dolorita"})
5/179: joined
5/180: new_data=pd.DataFrame({"const":1,"size":[500,660],"view":[1,0]})
5/181: new_data
5/182: new_data.rename(index={0:"Tucacas",1:"la Dolorita"})
5/183:
Predictions=results.predict(new_data)
Predictions
5/184:
Predictionsdf=pd.DataFrame({"predictions":Predictions})
joined=new_data.join(Predictionsdf)
joined.rename(index={0:"Tucacas",1:"la Dolorita"})
5/185: joined
 8/1:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data
 8/2:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data_head()
 8/3:
# For these lessons we will need NumPy, pandas, matplotlib and seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# and of course the actual regression (machine learning) module
from sklearn.linear_model import LinearRegression
 8/4:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data_head()
 8/5:
# There is a single independent variable: 'SAT'
x = data['SAT']

# and a single depended variable: 'GPA'
y = data['GPA']
 8/6:
# For these lessons we will need NumPy, pandas, matplotlib and seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# and of course the actual regression (machine learning) module
from sklearn.linear_model import LinearRegression
 8/7:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data
 8/8:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data
 8/9:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's explore the top 5 rows of the df
data
8/10:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
data_heads()
8/11:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
data_head()
8/12:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
dataù
8/13:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
dataù
8/14:
# For these lessons we will need NumPy, pandas, matplotlib and seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# and of course the actual regression (machine learning) module
from sklearn.linear_model import LinearRegression
8/15:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
data
8/16:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
data_head()
8/17:
# We start by loading the data
data = pd.read_csv('1.01. Simple linear regression.csv')

# Let's exp§lore the top 5 rows of the df
data.head()
8/18:
# There is a single independent variable: 'SAT'
x = data['SAT']

# and a single depended variable: 'GPA'
y = data['GPA']
8/19:
# Often it is useful to check the shapes of the features
x.shape
8/20: y.shape
8/21:
# We start by creating a linear regression object
reg = LinearRegression()
8/22:
# The whole learning process boils down to fitting the regression
# Note that the first argument is the independent variable, while the second - the dependent (unlike with StatsModels)
reg.fit(x_matrix,y)
8/23:
# In order to feed x to sklearn, it should be a 2D array (a matrix)
# Therefore, we must reshape it 
# Note that this will not be needed when we've got more than 1 feature (as the inputs will be a 2D array by default)

# x_matrix = x.values.reshape(84,1)
x_matrix = x.values.reshape(-1,1)

# Check the shape just in case
x_matrix.shape
8/24:
# We start by creating a linear regression object
reg = LinearRegression()
8/25:
# We start by creating a linear regression object
reg = LinearRegression()
8/26:
# The whole learning process boils down to fitting the regression
# Note that the first argument is the independent variable, while the second - the dependent (unlike with StatsModels)
reg.fit(x_matrix,y)
8/27:
# Getting the coefficients of the regression
# Note that the output is an array, as we usually expect several coefficients
reg.coef_
8/28:
# To get the R-squared in sklearn we must call the appropriate method
reg.score(x_matrix,y)
8/29:
# Getting the intercept of the regression
# Note that the result is a float as we usually expect a single value
reg.intercept_
8/30:
# There is a dedicated method should we want to predict values
# Note that the result is an array, as we can predict more than one value at a time
reg.predict(1740)
8/31:
# Getting the intercept of the regression
# Note that the result is a float as we usually expect a single value
reg.intercept_
8/32:
# To be in line with our knowledge so far, we can create a pandas data frame with several different values of SAT
new_data = pd.DataFrame(data=[1740,1760],columns=['SAT'])
new_data
8/33:
# There is a dedicated method should we want to predict values
# Note that the result is an array, as we can predict more than one value at a time
reg.predict(1740,1700)
8/34:
# There is a dedicated method should we want to predict values
# Note that the result is an array, as we can predict more than one value at a time
reg.predict(1740)
8/35:
# We can predict the whole data frame in bulk
# Note that the result is an array, this time with 2 elements
reg.predict(new_data)
8/36:
# Finally, we can directly store the predictions in a new series of the same dataframe
new_data['Predicted_GPA'] = reg.predict(new_data)
new_data
8/37:
# There are different ways to plot the data - here's the matplotlib code
plt.scatter(x,y)

# Parametrized version of the regression line
yhat = reg.coef_*x_matrix + reg.intercept_

# Non-parametrized version of the regression line
#yhat = 0.0017*x + 0.275

# Plotting the regression line
fig = plt.plot(x,yhat, lw=1, c='red', label ='regression line')

# Labelling our axes
plt.xlabel('SAT', fontsize = 20)
plt.ylabel('GPA', fontsize = 20)
plt.show()
8/38: data_2=pd.read_csv("1.02. Multiple linear regression.csv")
8/39: data_2.head()
8/40: data_2.head(2)
8/41: data_2.head(10)
8/42:
# There is a single independent variable: 'SAT'
x = data['SAT', "Rand 1,2,3"]

# and a single depended variable: 'GPA'
y = data['GPA']
8/43:
# There is a single independent variable: 'SAT'
x2 = data['SAT', "Rand 1,2,3"]

# and a single depended variable: 'GPA'
y2 = data['GPA']
8/44:
# There is a single independent variable: 'SAT'
x2 = data_2['SAT', "Rand 1,2,3"]

# and a single depended variable: 'GPA'
y2 = data_2['GPA']
8/45: data_2.describe
8/46: data_2.describe()
8/47:
# There is a single independent variable: 'SAT'
x = data[['SAT', 'Rand 1,2,3']]

# and a single depended variable: 'GPA'
y = data['GPA']
8/48:
# There is a single independent variable: 'SAT'
x = data_2[['SAT', 'Rand 1,2,3']]

# and a single depended variable: 'GPA'
y = data_2['GPA']
8/49: reg = LinearRegression()
8/50: x.shape
8/51: y.shape
8/52: reg.fit(x_matrix,y)
8/53: reg.score(x,y)
8/54: reg.fit(x,y)
8/55: shape(x)
8/56: x.shape
8/57: y.shape
8/58: reg.fit(x,y)
8/59: reg.score(x,y)
8/60: reg.coeff_
8/61: reg.coeff_(x,y)
8/62: reg.coef_
8/63: reg.intercept_
8/64: from sklearn.feature_selection import f_regression
8/65: f_regression(x,y)
8/66: p_values=f_regression(x,y)[1]
8/67: p_values
8/68: p_values.float(3)
8/69: p_values.round(3)
8/70: reg_summary=pd.DataFrame(data_2{"SAT","Rand 1,2,3"})
8/71: reg_summary=pd.DataFrame(data_2=["SAT","Rand 1,2,3"])
8/72: reg_summary=pd.DataFrame(data_2=["SAT","Rand 1,2,3"], columns["Features"])
8/73: data_2
8/74: reg_summary=pd.DataFrame(data_2=["SAT","Rand 1,2,3"], columns=["Features"])
8/75: reg_summary=pd.DataFrame(data_2=["SAT","Rand 1,2,3"])
8/76: reg_summary=pd.DataFrame(data_2=['SAT','Rand 1,2,3'])
8/77: reg_summary=pd.DataFrame(data_2=['SAT','Rand 1,2,3'], columns=["Features"])
8/78: reg_summary=pd.DataFrame(data=['SAT','Rand 1,2,3'], columns=["Features"])
8/79:
reg_summary=pd.DataFrame(data=['SAT','Rand 1,2,3'], columns=["Features"])
reg_summary
8/80: data
8/81:
reg_summary=pd.DataFrame(data_2=['SAT','Rand 1,2,3'], columns=["Features"])
reg_summary
8/82:
reg_summary=pd.DataFrame(data=['SAT','Rand 1,2,3'], columns=["Features"])
reg_summary
8/83:
reg_summary=pd.DataFrame(data=['SAT','Rand 1,2,3'])
reg_summary
8/84:
reg_summary=pd.DataFrame(data=['SAT','Rand 1,2,3'], columns=['Features'])
reg_summary
8/85:
reg_summary=pd.DataFrame(data=x.columns.values, columns=['Features'])
reg_summary
8/86: x.columns.values
8/87: reg_summary["coeff"]=reg.coef_
8/88: reg_summary["p_values"]=p_values(round(3))
8/89: reg_summary["p_values"]=p_values
8/90: reg_summary
8/91: reg_summary["p_values"]=p_values.round(3)
8/92: reg_summary
8/93: from sklearn.preprocessing import StandardScaler
8/94: scaler=StandardScaler()
8/95: scaler
8/96: scaler.fit(x)
8/97: x_scaled=scaler.transform(x)
8/98: x_scaled
8/99: reg.fit(x_scaled,y)
8/100: reg.intercept_
8/101: reg.coef_
8/102:
summary=pd.DataFrame(data=["Bias", "SAT" "Rand 1,2,3"], columns=['weights'])
summary
8/103:
summary=pd.DataFrame(data=["Bias", "SAT" "Rand 1,2,3"], columns=['Features'])
summary
8/104:
summary=pd.DataFrame(data=["Bias", "SAT" "Rand 1,2,3"], columns=['Features'])
summary["Weights"]=reg.intercept_, reg.coef_[0], reg.coef_[1]
8/105:
summary=pd.DataFrame(data=[["Bias"],[ "SAT"], ["Rand 1,2,3"]], columns=['Features'])
summary["Weights"]=reg.intercept_, reg.coef_[0], reg.coef_[1]
8/106: summary
8/107:
predict=pd.DataFrame(data=[[1600, 2],[1700.1]]), columns['SAT','Rand 1,2,3'])
predict
8/108:
predict=pd.DataFrame(data=[[1600, 2],[1700.1]]), columns=['SAT','Rand 1,2,3'])
predict
8/109:
predict=pd.DataFrame(data=[[1600, 2],[1700.1]],columns=['SAT','Rand 1,2,3'])
predict
8/110:
predict=pd.DataFrame(data=[[1600, 2],[1700,1]],columns=['SAT','Rand 1,2,3'])
predict
8/111: reg.predict(predict)
8/112: predict_scaled=scaler.transform(predict)
8/113:
predict_scaled=pd.DataFrame(data=[[1600, 2],[1700,1]],columns=['SAT','Rand 1,2,3'])
predict_scaled
8/114: reg.predict(predict_scaled)
8/115: predict_scaled=scaler.transform(predict)
8/116:

predict_scaled
8/117: reg.predict(predict_scaled)
11/1: var = '01234567'
11/2: print(var[::2])
11/3: print(var[::1])
11/4: print(var[::3])
11/5: print(var[::2])
11/6: L.append(['a','b'])
11/7: append(['a','b'])
11/8:
x=1

if(x!=1):

print('Hello')

else:

print('Hi')

print('Mike')
11/9:
x=1

    if(x!=1):

            print('Hello')

    else:

        print('Hi')

        print('Mike')
11/10:
x=1
    if(x!=1):
        print('Hello')

    else:
        print('Hi')

        print('Mike')
11/11:
x=1
    if(x!=1):
        print('Hello')

    else:
        print('Hi')

print('Mike')
11/12:
x=1
    if x!=1:
        print('Hello')

    else:
        print('Hi')

print('Mike')
11/13:
x=1
    if x!=1:
        print("hello")
        
        else:
            print("hi")
            
            print("lala")
11/14:
x=1
    if x!=1:
        print("hello")
        
        else:
            print("hi")
            
print("lala")
11/15:
x=1
    if x!=1:
        print("hello")
        else:
            print("hi")
            
print("lala")
11/16:
x=1
    if x=1:
        print("hello")
        else:
            print("hi")
            
print("lala")
11/17:
x=1
    if x=1:
        print("hello")
        else:
            print("hi")
11/18:
x=1
    if x=1:
        print "hello"
        else:
            print "hi"
11/19: print("hello")
11/20:
x=1
    if x=1:
        print("hello")
        else:
            print("lalala")
11/21:
x=1
    if x!=1:
        print("hello")
        else:
            print("lalala")
11/22:
x=1
    if x=!1:
        print("hello")
        else:
            print("lalala")
11/23:
x=1
    if x==1:
        print("hello")
        else:
            print("lalala")
11/24: x
11/25:
xxxx=1
    if xxxx==1:
        print("hello")
        else:
            print("lalala")
11/26: xxxxx
11/27: xxxx
11/28: xxxx=1
11/29: xxxx
11/30:
xxxx=1
    if xxxx==1:
        print("hello")
        else:
            print("lalala")
11/31:
xxxx=1
    if xxxx=1:
        print("hello")
        else:
            print("lalala")
10/1:
xxxx=1
    if xxxx=1:
        print("hello")
        else:
            print("lalala")
10/2:
xxxx=1
    if xxxx==1:
        print("hello")
        else:
            print("lalala")
10/3:
xxxx=1
if xxxx==1:
        print("hello")
        else:
            print("lalala")
10/4:
xxxx=1
if xxxx==1:
    print("hello")
    else:
        print("lalala")
10/5:
xxxx=1
if xxxx!=1:
    print("hello")
    else:
        print("lalala")
11/32:
x=1

if(x!=1):

print('Hello')

else:

print('Hi')

print('Mike')
11/33:
x=1

if(x!=1):

    print('Hello')

else:

print('Hi')

print('Mike')
11/34:
x=1

if(x!=1):

    print('Hello')

else:

    print('Hi')

print('Mike')
11/35:
A=['1','2','3']

for a in A:

print(2*a)
11/36:
A=['1','2','3']

for a in A:

    print(2*a)
11/37: A=[4,5,8]
11/38: sort.A
11/39: sort(A)
11/40: A.sort()
11/41: A
11/42: A=[14,5,8]
11/43: A.sort()
11/44: A
11/45:
a=np.array([0,1,0,1,0])

b=np.array([1,0,1,0,1])

a*b
11/46: import numpy as nu
11/47: import numpy as np
11/48:
a=np.array([0,1,0,1,0])

b=np.array([1,0,1,0,1])

a*b
11/49:
a=np.array([0,1])

b=np.array([1,0])

np.dot(a,b)
11/50:
a=np.array([1,1,1,1,1])

a+10
11/51: int(3.2)
11/52: A='1234567'
11/53: A[1::2]
11/54: Name="Michael Jackson"
11/55: Name.find('el')
11/56: A=((11,12),[21,22])
11/57: A[1]
11/58: '1,2,3,4'.split(',')
11/59: A=[1,'a'] and B=[2,1,'d']
11/60: A=[1,'a']
11/61: B=[2,1,'d']
11/62: A+B
11/63: A*B
11/64: A-B
11/65:  V={'A','B'}
11/66: V.add('C')
11/67: V
11/68: V.add('C')
11/69: V
11/70:
for n in range(3):

print(n)
11/71:
for n in range(3):
    print(n)
11/72:
def Add(x,y):

z=y+x

return(y)
11/73:
def Add(x,y):
    =y+x
    return(y)
11/74:
def Add(x,y):
    =y+x
    
    return(y)
11/75:
def Add(x,y):
    z=y+x
    
    return(y)
11/76: Add(1,1)
11/77:
def __init__(self,x,y):
    self.x=x
    self.y=y

def print_point(self):
    print('x=',self.x,' y=',self.y)

p1=Points(1,2)

p1.print_point()
11/78:
def __init__(self,x,y):
    self.x=x
    self.y=y

def print_point(self):
    print('x=',self.x,' y=',self.y)
    p1=Points(1,2)
    p1.print_point()
11/79: p1=Points(1,2)
11/80: pi
11/81: p1
11/82:
def __init__(self,x,y):
    self.x=x
    self.y=y

def print_point(self):
    print('x=',self.x,' y=',self.y)
    p1=Points(1,2)
    p1.print_point()
11/83:
def __init__(self,x,y):
    self.x=x
    self.y=y

def print_point(self):
    print('x=',self.x,' y=',self.y)
    p1=Points(1,2)
    p1.print_point()
11/84:     p1=Points(1,2)
11/85:
def __init__(self,x,y):
    self.x=x
    self.y=y

def print_point(self):
    print('x=',self.x,' y=',self.y)
11/86:
class Points(object):

    def __init__(self,x,y):
        self.x=x
        self.y=y

    def print_point(self):
        print('x=',self.x,' y=',self.y)
11/87:     p1=Points(1,2)
11/88: p1.print_point()
11/89:
class Points(object)
    
    def __init__(self,x,y):
        self.x=x
        self.y=y

    def print_point(self):
        print('x=',self.x,' y=',self.y)

p2=Points(1,2)

p2.x=2

p2.print_point()
11/90:
class Points(object)
    
    def __init__(self,x,y):
        self.x=x
        self.y=y

    def print_point(self):
        print('x=',self.x,' y=',self.y)
11/91:
class Points(object):
    
    def __init__(self,x,y):
        self.x=x
        self.y=y

    def print_point(self):
        print('x=',self.x,' y=',self.y)
11/92:

p2=Points(1,2)

p2.x=2

p2.print_point()
11/93:
import numpy as np
from sklearn.model_selection import train_test_split
11/94: a=np.arange(1,101)
11/95: a
11/96: a=np.arange(1,50)
11/97: a
11/98: a=np.arange(1,100)
11/99: a
11/100: a=np.arange(1,101)
11/101: a
11/102: a=np.arange(1,101,2)
11/103: a
11/104: a=np.arange(1,101,1)
11/105: a
11/106: a
11/107: b=np.arange(501,600,1)
11/108: b
11/109: b=np.arange(501,601,1)
11/110: b
11/111: train_test_split(a)
11/112: a_train,a_test=train_test_split(a)
11/113: train_test_split(a, test_size(0.2))
11/114: train_test_split(a, testSize(0.2))
11/115: train_test_split(a, test.size(0.2))
11/116: train_test_split(a, test_size(0.2))
11/117: train_test_split(a, test_size=0.2)
11/118: a_train.shape()
11/119: train_test_split(a, test_size=0.2, random_state=42)
11/120: a
11/121: b=np.arange(501,601,1)
11/122: b
11/123: train_test_split(a, test_size=0.2, random_state=42)
11/124: b
11/125: train_test_split(a, test_size=0.2, random_state=42)
11/126: a_train.shape()
11/127: a_train.shape
11/128: a_train.shape, a_test.shape
11/129: train_test_split(a,b, test_size=0.2, random_state=42)
12/1: b_test
12/2:
# In this lesson we will explore the train_test_split module
# Therefore we need no more than the module itself and NumPy
import numpy as np
from sklearn.model_selection import train_test_split
12/3:
# Let's generate a new data frame 'a' which will contain all integers from 1 to 100
# The method np.arange works like the built-in method 'range' with the difference it creates an array
a = np.arange(1,101)
12/4:
# Let's check it out
a
12/5:
# Similarly, let's create another ndarray 'b', which will contain integers from 501 to 600
# We have intentionally picked these numbers so we can easily compare the two
# Obviously, the difference between the elements of the two arrays is 500 for any two corresponding elements
b = np.arange(501,601)
b
12/6:
# Let's check out how this works
train_test_split(a)
12/7:
# There are several different arguments we can set when we employ this method
# Most often, we have inputs and targets, so we have to split 2 different arrays
# we are simulating this situation by splitting 'a' and 'b'

# You can specify the 'test_size' or the 'train_size' (but the latter is deprecated and will be removed)
# essentially the two have the same meaning 
# Common splits are 75-25, 80-20, 85-15, 90-10

# Finally, you should always employ a 'random_state'
# In this way you ensure that when you are splitting the data you will always get the SAME random shuffle

# Note 2 arrays will be split into 4
# The order is train1, test1, train2, test2 
# It is very useful to store them in 4 variables, so we can later use them
a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=365)
12/8:
# Let's check the shapes
# Basically, we are checking how does the 'test_size' work
a_train.shape, a_test.shape
12/9:
# Explore manually
a_train
12/10:
# Explore manually
a_test
12/11: b_train.shape, b_test.shape
12/12: b_train
12/13: b_test
12/14: c=no.arrange(1,801)
12/15: c=np.arrange(1,801)
12/16: c=np_arrange(1,801)
12/17: c=np.arange(1,801)
12/18: c
12/19: c_train, c_test=train:test_split(c)
12/20: c_train, c_test=train_test_split(c)
12/21: c_train, c_test
12/22: c_test.shape
12/23: c_train.shape, c_test.shape
12/24: c_train, c_test=train_test_split(c, test_size=0.2)
12/25: c_train, c_test
12/26: c_test.shape
12/27: c_train.shape, c_test.shape
12/28: c_train, c_test=train_test_split(c, test_size=0.2, ransom_state=42)
12/29: c_train, c_test=train_test_split(c, test_size=0.2, random_state=42)
12/30: c_train, c_test
12/31: c_test.shape
12/32: c_train.shape, c_test.shape
12/33: data_set=pd.read_csv("1.04. Real-life example.csv")
12/34:
# In this lesson we will explore the train_test_split module
# Therefore we need no more than the module itself and NumPy
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
12/35: data_set=pd.read_csv("1.04. Real-life example.csv")
12/36: data_set
12/37: data_set(head)
12/38: data_set.head
12/39: data_set.head(5)
12/40: data_set.describe
12/41: data_set.describe(include="all")
13/1:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
13/2:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('1.04. Real-life example.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
13/3:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
13/4:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
13/5:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Registration'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
13/6:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
13/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
13/8:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull()
13/9:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum
13/10:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
13/11:
# Let's check the descriptives without the missing values
data_no_mv.describe(include='all')
13/12:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
13/13:
# Let's simply drop all missing values
# This is not always recommended, however, when we remove less than 5% of the data, it is okay
data_no_mv = data.dropna(axis=0)
13/14:
# Let's check the descriptives without the missing values
data_no_mv.describe(include='all')
13/15:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
13/16:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('1.04. Real-life example.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
13/17:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
13/18:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
13/19:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
13/20:
# Let's simply drop all missing values
# This is not always recommended, however, when we remove less than 5% of the data, it is okay
data_no_mv = data.dropna(axis=0)
13/21:
# Let's check the descriptives without the missing values
data_no_mv.describe(include='all')
13/22:
# A great step in the data exploration is to display the probability distribution function (PDF) of a variable
# The PDF will show us how that variable is distributed 
# This makes it very easy to spot anomalies, such as outliers
# The PDF is often the basis on which we decide whether we want to transform a feature
sns.distplot(data_no_mv['Price'])
13/23:
# Obviously there are some outliers present 

# Without diving too deep into the topic, we can deal with the problem easily by removing 0.5%, or 1% of the problematic samples
# Here, the outliers are situated around the higher prices (right side of the graph)
# Logic should also be applied
# This is a dataset about used cars, therefore one can imagine how $300,000 is an excessive price

# Outliers are a great issue for OLS, thus we must deal with them in some way
# It may be a useful exercise to try training a model without removing the outliers

# Let's declare a variable that will be equal to the 99th percentile of the 'Price' variable
q = data_no_mv['Price'].quantile(0.99)
# Then we can create a new df, with the condition that all prices must be below the 99 percentile of 'Price'
data_1 = data_no_mv[data_no_mv['Price']<q]
# In this way we have essentially removed the top 1% of the data about 'Price'
data_1.describe(include='all')
13/24:
# Let's see what's left
data_cleaned.describe(include='all')
13/25:
# Let's see what's left
data_cleaned.describe(include='all')
13/26:
# When we remove observations, the original indexes are preserved
# If we remove observations with indexes 2 and 3, the indexes will go as: 0,1,4,5,6
# That's very problematic as we tend to forget about it (later you will see an example of such a problem)

# Finally, once we reset the index, a new column will be created containing the old index (just in case)
# We won't be needing it, thus 'drop=True' to completely forget about it
data_cleaned = data_4.reset_index(drop=True)
13/27:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
13/28:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('1.04. Real-life example.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
13/29:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
13/30:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
13/31:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
13/32:
# Let's simply drop all missing values
# This is not always recommended, however, when we remove less than 5% of the data, it is okay
data_no_mv = data.dropna(axis=0)
13/33:
# Let's check the descriptives without the missing values
data_no_mv.describe(include='all')
13/34:
# A great step in the data exploration is to display the probability distribution function (PDF) of a variable
# The PDF will show us how that variable is distributed 
# This makes it very easy to spot anomalies, such as outliers
# The PDF is often the basis on which we decide whether we want to transform a feature
sns.distplot(data_no_mv['Price'])
13/35:
# Obviously there are some outliers present 

# Without diving too deep into the topic, we can deal with the problem easily by removing 0.5%, or 1% of the problematic samples
# Here, the outliers are situated around the higher prices (right side of the graph)
# Logic should also be applied
# This is a dataset about used cars, therefore one can imagine how $300,000 is an excessive price

# Outliers are a great issue for OLS, thus we must deal with them in some way
# It may be a useful exercise to try training a model without removing the outliers

# Let's declare a variable that will be equal to the 99th percentile of the 'Price' variable
q = data_no_mv['Price'].quantile(0.99)
# Then we can create a new df, with the condition that all prices must be below the 99 percentile of 'Price'
data_1 = data_no_mv[data_no_mv['Price']<q]
# In this way we have essentially removed the top 1% of the data about 'Price'
data_1.describe(include='all')
13/36:
# We can check the PDF once again to ensure that the result is still distributed in the same way overall
# however, there are much fewer outliers
sns.distplot(data_1['Price'])
13/37:
# We can treat the other numerical variables in a similar way
sns.distplot(data_no_mv['Mileage'])
13/38:
q = data_1['Mileage'].quantile(0.99)
data_2 = data_1[data_1['Mileage']<q]
13/39:
# This plot looks kind of normal, doesn't it?
sns.distplot(data_2['Mileage'])
13/40:
# The situation with engine volume is very strange
# In such cases it makes sense to manually check what may be causing the problem
# In our case the issue comes from the fact that most missing values are indicated with 99.99 or 99
# There are also some incorrect entries like 75
sns.distplot(data_no_mv['EngineV'])
13/41:
# A simple Google search can indicate the natural domain of this variable
# Car engine volumes are usually (always?) below 6.5l
# This is a prime example of the fact that a domain expert (a person working in the car industry)
# may find it much easier to determine problems with the data than an outsider
data_3 = data_2[data_2['EngineV']<6.5]
13/42:
# Following this graph, we realize we can actually treat EngineV as a categorical variable
# Even so, in this course we won't, but that's yet something else you may try on your own
sns.distplot(data_3['EngineV'])
13/43:
# Finally, the situation with 'Year' is similar to 'Price' and 'Mileage'
# However, the outliers are on the low end
sns.distplot(data_no_mv['Year'])
13/44:
# I'll simply remove them
q = data_3['Year'].quantile(0.01)
data_4 = data_3[data_3['Year']>q]
13/45:
# Here's the new result
sns.distplot(data_4['Year'])
13/46:
# When we remove observations, the original indexes are preserved
# If we remove observations with indexes 2 and 3, the indexes will go as: 0,1,4,5,6
# That's very problematic as we tend to forget about it (later you will see an example of such a problem)

# Finally, once we reset the index, a new column will be created containing the old index (just in case)
# We won't be needing it, thus 'drop=True' to completely forget about it
data_cleaned = data_4.reset_index(drop=True)
13/47:
# Let's see what's left
data_cleaned.describe(include='all')
14/1:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
14/2:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('1.04. Real-life example.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
14/3:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('1.04. Real-life example.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
14/4:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
14/5:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
14/6:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
14/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
14/8:
# Let's simply drop all missing values
# This is not always recommended, however, when we remove less than 5% of the data, it is okay
data_no_mv = data.dropna(axis=0)
14/9:
# Let's check the descriptives without the missing values
data_no_mv.describe(include='all')
14/10:
# A great step in the data exploration is to display the probability distribution function (PDF) of a variable
# The PDF will show us how that variable is distributed 
# This makes it very easy to spot anomalies, such as outliers
# The PDF is often the basis on which we decide whether we want to transform a feature
sns.distplot(data_no_mv['Price'])
14/11:
# We can check the PDF once again to ensure that the result is still distributed in the same way overall
# however, there are much fewer outliers
sns.distplot(data_1['Price'])
14/12:
# Obviously there are some outliers present 

# Without diving too deep into the topic, we can deal with the problem easily by removing 0.5%, or 1% of the problematic samples
# Here, the outliers are situated around the higher prices (right side of the graph)
# Logic should also be applied
# This is a dataset about used cars, therefore one can imagine how $300,000 is an excessive price

# Outliers are a great issue for OLS, thus we must deal with them in some way
# It may be a useful exercise to try training a model without removing the outliers

# Let's declare a variable that will be equal to the 99th percentile of the 'Price' variable
q = data_no_mv['Price'].quantile(0.99)
# Then we can create a new df, with the condition that all prices must be below the 99 percentile of 'Price'
data_1 = data_no_mv[data_no_mv['Price']<q]
# In this way we have essentially removed the top 1% of the data about 'Price'
data_1.describe(include='all')
14/13:
# We can check the PDF once again to ensure that the result is still distributed in the same way overall
# however, there are much fewer outliers
sns.distplot(data_1['Price'])
14/14:
# We can treat the other numerical variables in a similar way
sns.distplot(data_no_mv['Mileage'])
14/15:
q = data_1['Mileage'].quantile(0.99)
data_2 = data_1[data_1['Mileage']<q]
14/16:
# This plot looks kind of normal, doesn't it?
sns.distplot(data_2['Mileage'])
14/17:
# The situation with engine volume is very strange
# In such cases it makes sense to manually check what may be causing the problem
# In our case the issue comes from the fact that most missing values are indicated with 99.99 or 99
# There are also some incorrect entries like 75
sns.distplot(data_no_mv['EngineV'])
14/18:
# A simple Google search can indicate the natural domain of this variable
# Car engine volumes are usually (always?) below 6.5l
# This is a prime example of the fact that a domain expert (a person working in the car industry)
# may find it much easier to determine problems with the data than an outsider
data_3 = data_2[data_2['EngineV']<6.5]
14/19:
# Following this graph, we realize we can actually treat EngineV as a categorical variable
# Even so, in this course we won't, but that's yet something else you may try on your own
sns.distplot(data_3['EngineV'])
14/20:
# Finally, the situation with 'Year' is similar to 'Price' and 'Mileage'
# However, the outliers are on the low end
sns.distplot(data_no_mv['Year'])
14/21:
# Following this graph, we realize we can actually treat EngineV as a categorical variable
# Even so, in this course we won't, but that's yet something else you may try on your own
sns.distplot(data_3['EngineV'])
14/22:
# Finally, the situation with 'Year' is similar to 'Price' and 'Mileage'
# However, the outliers are on the low end
sns.distplot(data_no_mv['Year'])
14/23:
# I'll simply remove them
q = data_3['Year'].quantile(0.01)
data_4 = data_3[data_3['Year']>q]
14/24:
# Here's the new result
sns.distplot(data_4['Year'])
14/25:
# When we remove observations, the original indexes are preserved
# If we remove observations with indexes 2 and 3, the indexes will go as: 0,1,4,5,6
# That's very problematic as we tend to forget about it (later you will see an example of such a problem)

# Finally, once we reset the index, a new column will be created containing the old index (just in case)
# We won't be needing it, thus 'drop=True' to completely forget about it
data_cleaned = data_4.reset_index(drop=True)
14/26:
# Let's see what's left
data_cleaned.describe(include='all')
14/27:
# From the subplots and the PDF of price, we can easily determine that 'Price' is exponentially distributed
# A good transformation in that case is a log transformation
sns.distplot(data_cleaned['Price'])
14/28:
# Here we decided to use some matplotlib code, without explaining it
# You can simply use plt.scatter() for each of them (with your current knowledge)
# But since Price is the 'y' axis of all the plots, it made sense to plot them side-by-side (so we can compare them)
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'Price' as y
ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])
ax1.set_title('Price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])
ax2.set_title('Price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])
ax3.set_title('Price and Mileage')


plt.show()
14/29:
# Let's transform 'Price' with a log transformation
log_price = np.log(data_cleaned['Price'])

# Then we add it to our data frame
data_cleaned['log_price'] = log_price
data_cleaned
14/30:
# Let's check the three scatters once again
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))
ax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])
ax1.set_title('Log Price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])
ax2.set_title('Log Price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])
ax3.set_title('Log Price and Mileage')


plt.show()

# The relationships show a clear linear relationship
# This is some good linear regression material

# Alternatively we could have transformed each of the independent variables
14/31:
# Since we will be using the log price variable, we can drop the old 'Price' one
data_cleaned = data_cleaned.drop(['Price'],axis=1)
14/32:
# Let's quickly see the columns of our data frame
data_cleaned.columns.values
14/33:
# To include the categorical data in the regression, let's create dummies
# There is a very convenient method called: 'get_dummies' which does that seemlessly
# It is extremely important that we drop one of the dummies, alternatively we will introduce multicollinearity
data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)
14/34:
# Since Year has the highest VIF, I will remove it from the model
# This will drive the VIF of other variables down!!! 
# So even if EngineV seems with a high VIF, too, once 'Year' is gone that will no longer be the case
data_no_multicollinearity = data_cleaned.drop(['Year'],axis=1)
14/35:
# To include the categorical data in the regression, let's create dummies
# There is a very convenient method called: 'get_dummies' which does that seemlessly
# It is extremely important that we drop one of the dummies, alternatively we will introduce multicollinearity
data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)
14/36:
# To make the code a bit more parametrized, let's declare a new variable that will contain the preferred order
# If you want a different order, just specify it here
# Conventionally, the most intuitive order is: dependent variable, indepedendent numerical variables, dummies
cols = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',
       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',
       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',
       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',
       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']
14/37:
# To implement the reordering, we will create a new df, which is equal to the old one but with the new order of features
data_preprocessed = data_with_dummies[cols]
data_preprocessed.head()
14/38:
# The target(s) (dependent variable) is 'log price'
targets = data_preprocessed['log_price']

# The inputs are everything BUT the dependent variable, so we can simply drop it
inputs = data_preprocessed.drop(['log_price'],axis=1)
14/39:
# Import the scaling module
from sklearn.preprocessing import StandardScaler

# Create a scaler object
scaler = StandardScaler()
# Fit the inputs (calculate the mean and standard deviation feature-wise)
scaler.fit(inputs)
14/40:
# Scale the features and store them in a new variable (the actual scaling procedure)
inputs_scaled = scaler.transform(inputs)
14/41:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)
14/42:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)
14/43:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

x_train, x_test, y_train, y_test
14/44:
# Create a linear regression object
reg = LinearRegression()
# Fit the regression with the scaled TRAIN inputs and targets
reg.fit(x_train,y_train)
14/45:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

x_train
x_test
14/46:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

x_train.shape()
x_test
14/47:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

x_train.shape
x_test
14/48:
# Import the module for the split
from sklearn.model_selection import train_test_split

# Split the variables with an 80-20 split and some random state
# To have the same split as mine, use random_state = 365
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

x_train.shape
x_test.shape
14/49:
# Create a linear regression object
reg = LinearRegression()
# Fit the regression with the scaled TRAIN inputs and targets
reg.fit(x_train,y_train)
14/50:
# Let's check the outputs of the regression
# I'll store them in y_hat as this is the 'theoretical' name of the predictions
y_hat = reg.predict(x_train)
14/51:
# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot
# The closer the points to the 45-degree line, the better the prediction
plt.scatter(y_train, y_hat)
# Let's also name the axes
plt.xlabel('Targets (y_train)',size=18)
plt.ylabel('Predictions (y_hat)',size=18)
# Sometimes the plot will have different scales of the x-axis and the y-axis
# This is an issue as we won't be able to interpret the '45-degree line'
# We want the x-axis and the y-axis to be the same
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()
14/52:
# Create a linear regression object
reg = LinearRegression()
# Fit the regression with the scaled TRAIN inputs and targets
reg.fit(x_train,y_train)
14/53:
# Let's check the outputs of the regression
# I'll store them in y_hat as this is the 'theoretical' name of the predictions
y_hat = reg.predict(x_train)
14/54:
# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot
# The closer the points to the 45-degree line, the better the prediction
plt.scatter(y_train, y_hat)
# Let's also name the axes
plt.xlabel('Targets (y_train)',size=1)
plt.ylabel('Predictions (y_hat)',size=1)
# Sometimes the plot will have different scales of the x-axis and the y-axis
# This is an issue as we won't be able to interpret the '45-degree line'
# We want the x-axis and the y-axis to be the same
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()
14/55:
# Another useful check of our model is a residual plot
# We can plot the PDF of the residuals and check for anomalies
sns.distplot(y_train - y_hat)

# Include a title
plt.title("Residuals PDF", size=18)

# In the best case scenario this plot should be normally distributed
# In our case we notice that there are many negative residuals (far away from the mean)
# Given the definition of the residuals (y_train - y_hat), negative values imply
# that y_hat (predictions) are much higher than y_train (the targets)
# This is food for thought to improve our model
14/56:
# Find the R-squared of the model
reg.score(x_train,y_train)

# Note that this is NOT the adjusted R-squared
# in other words... find the Adjusted R-squared to have the appropriate measure :)
14/57:
# Obtain the bias (intercept) of the regression
reg.intercept_
14/58:
# Obtain the weights (coefficients) of the regression
reg.coef_

# Note that they are barely interpretable if at all
14/59:
# Check the different categories in the 'Brand' variable
data_cleaned['Brand'].unique()

# In this way we can see which 'Brand' is actually the benchmark
14/60:
# Once we have trained and fine-tuned our model, we can proceed to testing it
# Testing is done on a dataset that the algorithm has never seen
# Luckily we have prepared such a dataset
# Our test inputs are 'x_test', while the outputs: 'y_test' 
# We SHOULD NOT TRAIN THE MODEL ON THEM, we just feed them and find the predictions
# If the predictions are far off, we will know that our model overfitted
y_hat_test = reg.predict(x_test)
14/61:
# Create a scatter plot with the test targets and the test predictions
# You can include the argument 'alpha' which will introduce opacity to the graph
plt.scatter(y_test, y_hat_test, alpha=0.2)
plt.xlabel('Targets (y_test)',size=18)
plt.ylabel('Predictions (y_hat_test)',size=18)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()
14/62:
# Create a scatter plot with the test targets and the test predictions
# You can include the argument 'alpha' which will introduce opacity to the graph
plt.scatter(y_test, y_hat_test, alpha=0.05)
plt.xlabel('Targets (y_test)',size=18)
plt.ylabel('Predictions (y_hat_test)',size=18)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()
14/63:
# Create a scatter plot with the test targets and the test predictions
# You can include the argument 'alpha' which will introduce opacity to the graph
plt.scatter(y_test, y_hat_test, alpha=0.25)
plt.xlabel('Targets (y_test)',size=18)
plt.ylabel('Predictions (y_hat_test)',size=18)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()
14/64:
# Finally, let's manually check these predictions
# To obtain the actual prices, we take the exponential of the log_price
df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])
df_pf.head()
14/65:
# Finally, let's manually check these predictions
# To obtain the actual prices, we take the exponential of the log_price
df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])
df_pf.head()
14/66:
# We can also include the test targets in that data frame (so we can manually compare them)
df_pf['Target'] = np.exp(y_test)
df_pf

# Note that we have a lot of missing values
# There is no reason to have ANY missing values, though
# This suggests that something is wrong with the data frame / indexing
14/67:
# After displaying y_test, we find what the issue is
# The old indexes are preserved (recall earlier in that code we made a note on that)
# The code was: data_cleaned = data_4.reset_index(drop=True)

# Therefore, to get a proper result, we must reset the index and drop the old indexing
y_test = y_test.reset_index(drop=True)

# Check the result
y_test.head()
14/68:
# Let's overwrite the 'Target' column with the appropriate values
# Again, we need the exponential of the test log price
df_pf['Target'] = np.exp(y_test)
df_pf
14/69:
# Additionally, we can calculate the difference between the targets and the predictions
# Note that this is actually the residual (we already plotted the residuals)
df_pf['Residual'] = df_pf['Target'] - df_pf['Prediction']

# Since OLS is basically an algorithm which minimizes the total sum of squared errors (residuals),
# this comparison makes a lot of sense
14/70:
# Finally, it makes sense to see how far off we are from the result percentage-wise
# Here, we take the absolute difference in %, so we can easily order the data frame
df_pf['Difference%'] = np.absolute(df_pf['Residual']/df_pf['Target']*100)
df_pf
14/71:
# Exploring the descriptives here gives us additional insights
df_pf.describe()
14/72:
# Sometimes it is useful to check these outputs manually
# To see all rows, we use the relevant pandas syntax
pd.options.display.max_rows = 999
# Moreover, to make the dataset clear, we can display the result with only 2 digits after the dot 
pd.set_option('display.float_format', lambda x: '%.2f' % x)
# Finally, we sort by difference in % and manually check the model
df_pf.sort_values(by=['Difference%'])
15/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
15/2:
# Load the country clusters data
data = pd.read_csv('3.01. Country clusters.csv')
15/3:
# Check out the data manually 
data
15/4:
# Use the simplest code possible to create a scatter plot using the longitude and latitude
# Note that in order to reach a result resembling the world map, we must use the longitude as x, and the latitude as y
plt.scatter(data['Longitude'],data['Latitude'])
# Set limits of the axes, again to resemble the world map
plt.xlim(-180,180)
plt.ylim(-90,90)
plt.show
15/5:
# Check out the data manually 
data
15/6:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
15/7:
# Load the country clusters data
data = pd.read_csv('3.01. Country clusters.csv')
15/8:
# Check out the data manually 
data
15/9:
# iloc is a method used to 'slice' data 
# 'slice' is not technically correct as there are methods 'slice' which are a bit different
# The term used by pandas is 'selection by position'
# The first argument of identifies the rows we want to keep
# The second - the columns
# When choosing the columns, e.g. a:b, we will keep columns a,a+1,a+2,...,b-1 ; so column b is excluded
x = data.iloc[:,1:3]
# for this particular case, we are choosing columns 1 and 2
# Note column indices in Python start from 0
15/10:
# Check if we worked correctly
x
15/11:
# Create an object (which we would call kmeans)
# The number in the brackets is K, or the number of clusters we are aiming for
kmeans = KMeans(3)
15/12:
# Fit the input data, i.e. cluster the data in X in K clusters
kmeans.fit(x)
15/13:
# Create a variable which will contain the predicted clusters for each observation
identified_clusters = kmeans.fit_predict(x)
# Check the result
identified_clusters
15/14:
# Create a copy of the data
data_with_clusters = data.copy()
# Create a new Series, containing the identified cluster for each observation
data_with_clusters['Cluster'] = identified_clusters
# Check the result
data_with_clusters
15/15:
# Plot the data using the longitude and the latitude
# c (color) is an argument which could be coded with a variable 
# The variable in this case has values 0,1,2, indicating to plt.scatter, that there are three colors (0,1,2)
# All points in cluster 0 will be the same colour, all points in cluster 1 - another one, etc.
# cmap is the color map. Rainbow is a nice one, but you can check others here: https://matplotlib.org/users/colormaps.html
plt.scatter(data_with_clusters['Longitude'],data_with_clusters['Latitude'],c=data_with_clusters['Cluster'],cmap='rainbow')
plt.xlim(-180,180)
plt.ylim(-90,90)
plt.show()
15/16:
data_mapped=data.copy()
data_mapped['Language']=data_mapped['Language'].map({'English':0, 'French':1,'German':2})
data_mapped
15/17: x=data_mapped.iloc[:,3:4]
15/18: x
15/19: kmeans = KMeans(3)
15/20: kmeans.fit(x)
15/21:
identified_clusters = kmeans.fit_predict(x)
# Check the result
identified_clusters
15/22:
data_with_clusters = data.copy()
# Create a new Series, containing the identified cluster for each observation
data_with_clusters['Cluster'] = identified_clusters
# Check the result
data_with_clusters
15/23:
plt.scatter(data_with_clusters['Longitude'],data_with_clusters['Latitude'],c=data_with_clusters['Cluster'],cmap='rainbow')
plt.xlim(-180,180)
plt.ylim(-90,90)
plt.show()
15/24: x=data_mapped.iloc[:,1:4]
15/25: x
15/26: kmeans = KMeans(3)
15/27: kmeans.fit(x)
15/28:
identified_clusters = kmeans.fit_predict(x)
# Check the result
identified_clusters
15/29:
data_with_clusters = data.copy()
# Create a new Series, containing the identified cluster for each observation
data_with_clusters['Cluster'] = identified_clusters
# Check the result
data_with_clusters
15/30:
plt.scatter(data_with_clusters['Longitude'],data_with_clusters['Latitude'],c=data_with_clusters['Cluster'],cmap='rainbow')
plt.xlim(-180,180)
plt.ylim(-90,90)
plt.show()
15/31: kmeans = KMeans(2)
15/32: kmeans.fit(x)
15/33:
identified_clusters = kmeans.fit_predict(x)
# Check the result
identified_clusters
15/34:
data_with_clusters = data.copy()
# Create a new Series, containing the identified cluster for each observation
data_with_clusters['Cluster'] = identified_clusters
# Check the result
data_with_clusters
15/35:
plt.scatter(data_with_clusters['Longitude'],data_with_clusters['Latitude'],c=data_with_clusters['Cluster'],cmap='rainbow')
plt.xlim(-180,180)
plt.ylim(-90,90)
plt.show()
15/36: #WCSS
15/37:
#WCSS
kmeans.inertia_
15/38:
wcss=[]
for i in range (1,7):
    kmeans=KMeans(i)
    kmeans.fit(x)
    wcss_iter=kmeans.inertia_
    wcss.append(wcss_iter)
15/39: wcss
15/40:
number_cluster=range(1,7)
plt.plot(number_clusters, wcss)
15/41:
number_clusters=range(1,7)
plt.plot(number_clusters, wcss)
16/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
16/2:
# Load the data
data = pd.read_csv ('3.12. Example.csv')
16/3:
# Check what's inside
data
16/4:
# Check what's inside
data
16/5:
# We are creating a scatter plot of the two variables
plt.scatter(data['Satisfaction'],data['Loyalty'])
# Name your axes 
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
16/6:
# We are creating a scatter plot of the two variables
plt.scatter(data['Satisfaction'],data['Loyalty'])
# Name your axes 
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
16/7:
# Select both features by creating a copy of the data variable
x = data.copy()
16/8:
# Create an object (which we would call kmeans)
# The number in the brackets is K, or the number of clusters we are aiming for
kmeans = KMeans(2)
# Fit the data
kmeans.fit(x)
16/9:
# Create a copy of the input data
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=kmeans.fit_predict(x)
16/10:
# Plot the data using the longitude and the latitude
# c (color) is an argument which could be coded with a variable 
# The variable in this case has values 0,1, indicating to plt.scatter, that there are two colors (0,1)
# All points in cluster 0 will be the same colour, all points in cluster 1 - another one, etc.
# cmap is the color map. Rainbow is a nice one, but you can check others here: https://matplotlib.org/users/colormaps.html
plt.scatter(clusters['Satisfaction'],clusters['Loyalty'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
16/11:
# Plot the data using the longitude and the latitude
# c (color) is an argument which could be coded with a variable 
# The variable in this case has values 0,1, indicating to plt.scatter, that there are two colors (0,1)
# All points in cluster 0 will be the same colour, all points in cluster 1 - another one, etc.
# cmap is the color map. Rainbow is a nice one, but you can check others here: https://matplotlib.org/users/colormaps.html
plt.scatter(clusters['Satisfaction'],clusters['Loyalty'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
16/12:
# Import a library which can do that easily
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
16/13:
# Createa an empty list
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    kmeans = KMeans(i)
    # Fit the STANDARDIZED data
    kmeans.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(kmeans.inertia_)
    
# Check the result
wcss
16/14:
# Plot the number of clusters vs WCSS
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
17/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
17/2:
# Load the data
data = pd.read_csv ('3.12. Example.csv')
17/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
17/4:
# Load the data
data = pd.read_csv ('3.12. Example.csv')
17/5:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
17/6:
# Load the data
data = pd.read_csv ('3.12. Example.csv')
17/7:
# Check what's inside
data
17/8:
# We are creating a scatter plot of the two variables
plt.scatter(data['Satisfaction'],data['Loyalty'])
# Name your axes 
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/9:
# Select both features by creating a copy of the data variable
x = data.copy()
17/10:
# Create an object (which we would call kmeans)
# The number in the brackets is K, or the number of clusters we are aiming for
kmeans = KMeans(2)
# Fit the data
kmeans.fit(x)
17/11:
# Create a copy of the input data
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=kmeans.fit_predict(x)
17/12:
# Plot the data using the longitude and the latitude
# c (color) is an argument which could be coded with a variable 
# The variable in this case has values 0,1, indicating to plt.scatter, that there are two colors (0,1)
# All points in cluster 0 will be the same colour, all points in cluster 1 - another one, etc.
# cmap is the color map. Rainbow is a nice one, but you can check others here: https://matplotlib.org/users/colormaps.html
plt.scatter(clusters['Satisfaction'],clusters['Loyalty'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/13:
# Import a library which can do that easily
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
17/14:
# Createa an empty list
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    kmeans = KMeans(i)
    # Fit the STANDARDIZED data
    kmeans.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(kmeans.inertia_)
    
# Check the result
wcss
17/15:
# Plot the number of clusters vs WCSS
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
17/16:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(9)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/17:
# Check if everything seems right
clusters_new
17/18:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/19:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(9)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/20:
# Check if everything seems right
clusters_new
17/21:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/22:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(2)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/23:
# Check if everything seems right
clusters_new
17/24:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/25:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(3)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/26:
# Check if everything seems right
clusters_new
17/27:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/28:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(4)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/29:
# Check if everything seems right
clusters_new
17/30:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/31:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(5)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/32:
# Check if everything seems right
clusters_new
17/33:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/34:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(9)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/35:
# Check if everything seems right
clusters_new
17/36:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/37:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(2)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/38:
# Check if everything seems right
clusters_new
17/39:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
17/40:
# Fiddle with K (the number of clusters)
kmeans_new = KMeans(4)
# Fit the data
kmeans_new.fit(x_scaled)
# Create a new data frame with the predicted clusters
clusters_new = x.copy()
clusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)
17/41:
# Check if everything seems right
clusters_new
17/42:
# Plot
plt.scatter(clusters_new['Satisfaction'],clusters_new['Loyalty'],c=clusters_new['cluster_pred'],cmap='rainbow')
plt.xlabel('Satisfaction')
plt.ylabel('Loyalty')
18/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Set the styles to Seaborn
sns.set()
# Import the KMeans module so we can perform k-means clustering with sklearn
from sklearn.cluster import KMeans
18/2: data=pd.read_csv('iris_dataset.csv)
18/3: data=pd.read_csv('iris_dataset.csv')
18/4: data=pd.read_csv ('iris_dataset.csv')
18/5: data=pd.read_csv ('iris-dataset.csv')
18/6:
data=pd.read_csv ('iris-dataset.csv')
data
18/7:
# We are creating a scatter plot of the two variables
plt.scatter(data['sepal_length'],data['sepal_width'])
# Name your axes 
plt.xlabel('length')
plt.ylabel('width')
18/8: k_means=KMeans(2)
18/9: k_means.fit()
18/10: k_means.fit(x)
18/11:
k_means=KMeans(2)
x = data.copy()
18/12: k_means.fit(x)
18/13:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=kmeans.fit_predict(x)
18/14: k_means.fit(x)
18/15:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=kmeans.fit_predict(x)
18/16:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)
18/17:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)
18/18:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)
clusters
18/19:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)
clusters
plt.scatter(clusters['Satisfaction'],clusters['sepal_length'],c=clusters['sepal_width'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/20:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)
plt.scatter(clusters['Satisfaction'],clusters['sepal_length'],c=clusters['sepal_width'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/21:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['sepal_width'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/22:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/23:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
18/24:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = pd.preprocessing.scale(x)
x_scaled
18/25:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
18/26:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled=pd.x_scaled
18/27:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled=pd.x_scaled()
18/28:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
18/29:
k_means=KMeans(2)
x = x_scaled.copy()
16/15:
# Createa an empty list
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    kmeans = KMeans(i)
    # Fit the STANDARDIZED data
    kmeans.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(kmeans.inertia_)
    
# Check the result
wcss
18/30:
k_means=KMeans(2)
x = x_scaled.copy()
18/31: x
18/32:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/33:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/34:
k_means=KMeans(3)
x = data.copy()
18/35: k_means.fit(x)
18/36:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/37:
from sklearn import preprocessing
# Scale the inputs
# preprocessing.scale scales each variable (column in x) with respect to itself
# The new result is an array
x_scaled = preprocessing.scale(x)
x_scaled
18/38:
k_means=KMeans(2)
x = x_scaled.copy()
18/39:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/40:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/41:
clusters = x_scaled.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x_scaled)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/42:
clusters = x_scaled.copy()
# Take note of the predicted clusters
18/43:
clusters = x_scaled.copy()
clusters
# Take note of the predicted clusters
18/44:
k_means_scaled=KMeans(2)
k_means_scaled.fit(x_scaled)
18/45:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/46:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/47:
clusters = x_scaled.copy()

# Take note of the predicted clusters 
clusters['cluster_pred']=k_means_scaled.fit_predict(x_scaled)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
# Take note of the predicted clusters
18/48:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=kmeans_scaled.fit_predict(x_scaled)
18/49:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=k_means_scaled.fit_predict(x_scaled)
18/50:
plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/51:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=k_means_scaled.fit_predict(x_scaled)
clusters_scaled
18/52:
plt.scatter(clusters_scaled['sepal_length'],clusters_scaled['sepal_width'],c=clusters_scaled['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/53:
k_means=KMeans(2)
x = data.copy()
18/54: k_means.fit(x)
18/55:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)

plt.scatter(clusters['sepal_length'],clusters['sepal_width'],c=clusters['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/56:
k_means_scaled=KMeans(4)
k_means_scaled.fit(x_scaled)
18/57:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/58:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/59:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=k_means_scaled.fit_predict(x_scaled)
clusters_scaled
18/60:
plt.scatter(clusters_scaled['sepal_length'],clusters_scaled['sepal_width'],c=clusters_scaled['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/61:
k_means_scaled=KMeans(9)
k_means_scaled.fit(x_scaled)
18/62:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/63:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/64:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=k_means_scaled.fit_predict(x_scaled)
clusters_scaled
18/65:
plt.scatter(clusters_scaled['sepal_length'],clusters_scaled['sepal_width'],c=clusters_scaled['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/66:
k_means_scaled=KMeans(5)
k_means_scaled.fit(x_scaled)
18/67:
wcss =[]

# Create all possible cluster solutions with a loop
# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish
for i in range(1,10):
    # Clsuter solution with i clusters
    k_means = KMeans(i)
    # Fit the STANDARDIZED data
    k_means.fit(x_scaled)
    # Append the WCSS for the iteration
    wcss.append(k_means.inertia_)
    
# Check the result
wcss
18/68:
plt.plot(range(1,10),wcss)
# Name your axes
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
18/69:
clusters_scaled = data.copy()
# predict the cluster for each observation
clusters_scaled['cluster_pred']=k_means_scaled.fit_predict(x_scaled)
clusters_scaled
18/70:
plt.scatter(clusters_scaled['sepal_length'],clusters_scaled['sepal_width'],c=clusters_scaled['cluster_pred'],cmap='rainbow')
plt.xlabel('Length')
plt.ylabel('Width')
18/71:
clusters = x.copy()
# Take note of the predicted clusters 
clusters['cluster_pred']=k_means.fit_predict(x)

# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)
plt.scatter(clusters_scaled['sepal_length'], clusters_scaled['sepal_width'], c= clusters_scaled ['cluster_pred'], cmap = 'rainbow')
18/72: sns.clustermap(x_scaled)
18/73: sns.clustermap(x_scaled, cmap='mako')
18/74: sns.clustermap(x, cmap='mako')
20/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
20/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
20/3:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)

inputs
21/1:
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
%matplotlib inline
21/2: !wget -O teleCust1000t.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/teleCust1000t.csv
21/3:
df = pd.read_csv('teleCust1000t.csv')
df.head()
22/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
22/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions.сх of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
22/3:
# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case.
print (targets.shape)
22/4:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/5:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(112, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/6:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3hb5dn/PxqW994jiZ04dpzY2RMCDSNsKKNAWlpooUCADqDtr/SF0vWWVcpb2obSAS0zIQQoaYBASKBkELKHR7ztOI73lGRt6feHco41LclxbAc/n+vyZek5Q+fY53x1n/u5h8LhcCAQCASC0UE51gcgEAgEEwkhugKBQDCKCNEVCASCUUSIrkAgEIwiQnQFAoFgFFEHWC5CGwQCgSB0FP4WCEtXIBAIRhEhugKBQDCKCNEVCASCUUSIrkAgEIwiQnQFAoFgFBGiKxAIBKOIEF2BQCAYRYToCgQCwSgiRFcgEAhGESG6AoFAMIoI0RUIBIJRRIiuQCAQjCJCdAUCgWAUEaIrEAgEo4gQXYFAIBhFhOgKBALBKCJEVyAQCEYRIboCgUAwigRq1yMYQxwOByaTCQCFQiH/uL739VogEIxfhOiOY2w2GwMDA8CgsDocDr/C2t/fj1KpJDY21kuQpR+l0vlwI/0WIi4QjC5CdMcpdrsdi8UiC6UkkkOh1+tRKBTExcUBTkvZ4RjsLer5OpCgtra2kpqailqtdhNhV8EWIi4QhIYQ3XGIw+HAbDa7iVowKJVK7Hb7iAlcR0cHKSkp8n4kEbfb7W4CLi0L9HnNzc1kZ2d7uUoCibjn+QgRF5zNCNEdZzgcDrq7u2ltbSU/Pz+kbZVKJVardcSOxZf1ejq0tbUxadIk+b3dbpd/hyriZrOZnp4e0tPTvVwovgTdn4j7EnSB4EwiRHecYbPZ3Hy5oSBZuiOFQqEY0f1J+/T1ejh0dXWRmZkpv3c9Vl9ulaE+z2g0otPpZMt+qB9w/q2DscCFiAs8EaI7jpD8uCqVShYQafJsKFr6jPxjVxPfKIlFMcKiG+izx5qREnGLxUJvby9paWny2FAiHuizDAYDBoOB5ORk+diEiAtAiO64weFwyBNnarUam83mtmwoqtr1rD/QQvnJPh45N37EjulMWLrjGVeRk94PF7PZTF9fHykpKfLY6Yi40WhEr9e7+dgDWeRCxMcnQnTHAZLg2u12lEqlm6UbDJOTIgEobR3g95/b+FuBA7Xy9G8mpVI5ri3dkTy2YIRvOPsbaUs8NTVVHgsk4kOdk9Vqpbu7W/aJS8cXrIj7en265zhREKI7DjCZTFitVtRq579DqVTKlm4wF3FOQgQqBaTGhLH3pIkH3yonJyGCK2alUZwVO+zjGs/uhfF+c4+0iEtfyCMlcCaTic7OTjIyMoDgwgsDnVNLSwtZWVl+Rdx1UrOnp4f29nZKSkqGfQ5nK0J0xxi73U5tbS3R0dHypFCoE2JhKiU5iZFMSw5nSaaJd6u6AHhlTzNLchPQqJTkp0XxwAV52B1Q3a6nID0aZYCbdqQn5sYzZ8LSDSXcLxCS6I7k/lQq1Yi6GVpaWsjOzgYCi/jevXvZsmULzz777Gl/7tmGEN0xRHIreLoTPG+AYKzNKUmR1HUNcOlkldv4Fw29AGyv7WZnbQ/3njeF+98qJz81ilsWZbO7vpfFufHcOC/T63PHs6U73nGNlx4JRvpLwWazoVKpAq8YJNL5BiviAwMDchLPREOI7hjhcDiwWq3Y7XaviTNXgr3RcpMj+aymm792+1+nql3P/W+VA9DQZeBX71cD8GFFB/852s7vrptBRlyE22dPFNE9Uz7dkeJMWLojub9QRVyr1RIbO3zX19mMqDI2RthsNqxWKwqFIuSJM19MOTWZ5kpKtMbv+la7u5geOtHPyj/t4Z+fN8lC2zFgp6nHeFrHNVERojs0Op1OiK5g9HCtqyBNMAxl6QZz8+YmRQEQqxlc1+ZwEKUJfCMsnBxPcnQYAM9sq2f2Y9s52NTHc3t7uPedepp6DMGc1lnNRLN0bTbbmIvuRHUvCNEdZRwOBx0dHcCg60ClUvkUXcniDOYRPzfZaelqzYPr9gxYGDD7FnOAjLhwLpuZyoGmPvqNVuZPGrwJbn35MIdaTfSbbPxwQ/mQ+xF4czaI+Ej6dIXoBo8Q3VFE8uOWlZW5WRm+RDdUf2pqjH9XgiuJUWHy69Z+E2UtWn52ST5XF6dTelKLwmMdcEY7XPjsbuzjyL870iFjZ4NIftksXeFeEJxxXP24rj5cXz5dSYiDvXEVCgVFGTEB1+sZsLi9b+ox8tsPa3j7cCsz0mMoyYrFZPX2L+vNNuY8tp33Stvd/MF2h4MOrcnv5+1t7KXspDaocwiV8TzJN95FdzxYuhNVdEX0wijh6seVohVcg8U9LV1JdPv6+qioqJAz1dRqtdtvlUpFtxEqOs2kRymoOLV9ZpyGln5zSMd4JAhxfOjdY/zp03q+tSSH6+ZksLWyk4c3VnLDvAzuvyCP+Eh3K/n/ttVT06Hnn9+aA4y8GI0UEzFOV1i6Y4MQ3VFAiscF5GgFm81GWJhToHy5F5RKJSaTiWPHjlFcXExUVJRsKXv+/qiuiz/v7nDb/uppGqbEhvPwZyNvZTb3mXjio1qe395IQVo0DmDDwVa2VnbxwAV5fHVOupx4EaZSYLDYue+NUn46XzVuRXekORvidEXI2Ngg3AtnGNd4XOki93Qn+BPdqqoq8vPzCQ8Ply3kiIgIoqOjiY+PJykpibS0NG5YMs2r1sKy4qlcc948JiVGcKZIDLOxp7FvcMBu5dH3qrjpr7v5eN8xGhsbUdmdXzZdegv/b7uRpvZu9Hq9nPo8XlwEE82nK9wLY4ewdM8wer0eg8HgdoF5uhN8ia7RaESj0ZCWlobROHSsbEqMhkuKUni/bNDa/c4rR7i0KJXr52Tw7KcNp3UO15SkcaLXyIGmfrfx+n53wewzOd9Xdln40UftXDszEQfuwnPVC+WszAtnWaaavDhw+IhPdnWdeLpTpN+Su0an07mtP5LCdDqcDaI70pZueHh40Ovr9Xqio6NH7PPPJoTonkHsdju9vb10dXUxY8YMedxTZD1rHGi1WvR6PdOmTQv6s76+MNtNdMGZafZhRYefLYJn49F2AFYvn8zNC7K46I+7sfswUF3H7A54u6xHfp8dH05zn3PCbUu9iS31JlKiNVxYmMxFhSksmhJPmMr5d5AKuXu6UqTXJpNJ/jl+/Ljbep5IYjyUgKtUKnQ6HXa7HYPB4CbgwxXO8S66I50GPJz9TQQ3ky+E6J4h/NVVAN+iKz1m22w2SktLSUtLC+nGnZN95h/Vnt9xnOd3HOfyWak0dBmoaNUFva0kuFOTI6nrMpAUFca8SXH852gb6w+0EBehZsX0JC4qTOGcqYlERIQNuT+TyYTRaGTmzJl+13E4HAEF3GKxYLVa0Wq1WCwWjEajvMyzSMtQAu45ZjQa5S8GafnpiMzZYOmOpIh/mRGiewZw9eOGhYX5jUzwRWVlJdnZ2XJ9XWl/gfC0PHf96BzO+f2u4Z1AAD4oG771XNdl4IKCZD6p6iIrPoLPHljGrroetlZ28ml1NxuPthMZpmT5tCQunpHC+flJxIR7X6btOjPtA0OnTkt+cLVaHfDRt6WlBavV6tbDzRWpIedQAm42m+UxnU6HyWSio6NDvhY8v0SHsrw9xywWC2azGY1G49adebiM9UTaREaI7hnAs65CsKLb1taGyWSiqKiIhoYGv8LsixO97n7f2Ag1z91czL1vlA7vJE6DxKgwzs9PoqZDT1mLtzX8yanSky99cYKmHgPP3jiLCwtTMFntLHxyBwaLnS3HOtlyrJMwlYKluYlcPCOZFdOTSTpVT+LprY18Uq1lZUs53z1nclAxyqeD9L9UqVRoNIETUaqqqkhPTyc+3ncnD08B9xRzq9UqW902mw29Xk9DQ4NsvXsKeLAWuPRb+lIfKYs3FNG1Wq3DEmibzcbChQvJzs5m06ZN1NfXs2rVKrq7u5k/fz6vvPJKUP+bsUaI7gjjWVchWNG12+3U1NSwaNEityI40o0VyNVQ06F3e//kR7W8urd5hM4qNHoGLLx7pI3Vyyczf1I8r+zxfxzbqrq47m/7eOeuhWhUCmZlxrgJtcXmYHttN9tru4FqojQqfrAilxXTE9lW3c1HFZ18VNHJ8mmJ3Hmu8/OGw2hHLyiVSpRKpRw2GAiDwcDMmTN9ispQbhTpt9lsdltnYGCAyspKWXhdr7NQBNxVxIM5b3DOWQwnBfjZZ5+lqKiI/n7nhO5Pf/pTHnjgAVatWsXq1at54YUXuOeee0Le72gjRHcEkeoqxMXFudVV8Jzg8RRdh8OBwWBgwYIF8k0VaveIXXU9bu/HSnAnJ0bQ2m/CbHPw/I7jQW1T0zHAna8f4e/fmM3PL5vO1/95kGVTE1k0JZ7/Vndz+EQ/kvdkwGzjiY9q3bafmeEU6ttePsz8SXHcee5kzp2aOKYTNSMdpzuURRqKG0Vi7969zJ0712ufQ7lRXAV8YGDAbUyr1VJeXu7VUNUzqWfr1q2Ul5fT3d3Nn//8Z2JjY5k1axYLFy4c8nhPnDjBe++9x8MPP8wzzzyDw+Fg27ZtvP766wDcdttt/PKXvxSiO9GwWq2Ul5ezbNkyeczfRJrZPJgtVldXh1qtJiEhwe86gVh/oOU0jnzkON5j5MVvzgbgvjdKMViCK1m5u76Xkt9+xjUlaVw2K5UPyzv44YpcvnvOZE70Grh8zV6/25a36siKD8caoeZAUz/3rCulKCOGO8+dxEWFKQE7ZMD4j9MdrYy5UN0oEnv37mXBggVex+gp4Oeeey5hYWE0NzcTFRVFT0+PXABqKO6//36eeuoptFpnsk9XVxcJCQlyi6ucnByam8fG0AgVIbojhOSHk6xY6WLwVbjG1dLt6emhs7OTqKioIRMohkuEWonRRy2FM8ntrx7hilmpbLpnEZ/VdMvF0n0xNSWKus4B+b0UngZw84sH+eyBZTy2uRaVAv58czHTUqK45M97vPZzss+9/kNFq44H36ogNymSkuxYfrpyGtHh6hFp2BkM4z1k7Ezg63w93Sjz58/HarVSV1fH7bffHtR+N23aRFpaGgsWLODTTz8FfE8uny0haOP7v3iW4KuuwlBIomuxWCgvL6ekpMRru1DcC54FyV3Z9eNzeO7m4hDOZmR4v6yDq5/fx+f1PUOu5yq4vjj//z5ne203NoezklpGXDib7llEfITzS+2FW2YPuX1Dt4H/HG1n+TOfM+/x7Rgtvv83490yhZEVlbEUqFCz0Xbu3MnGjRvJzc1l1apVbNu2jfvvv5/e3l7ZdXfixAmysrLO1CGPKEJ0TxN/dRWGQvLzlpWVMXXqVKKiogJmqQ0VNnb4xGCm2PQk94mZtw62khqjIT1Ww5XFaSGd2+kyYLbxUUXniO3va/84wOzHtnPtX/fRZ3TebD9+p4L1d8wnQq1kTnYcq5dP5vJZqX73seipnZT89jPah6iMNhJMlBoTwyFU0X388cc5ceIEDQ0NrFu3jgsvvJDXXnuNCy64gA0bNgDw0ksv8dWvfvVMHfKIIkT3NJFiMIeaOPNEyoBSq9VyB+ChstRcb94fv13B/35Q7Vaicf2Bk/Lr6m730o2//bCGF3c3kZ0QwXulzkf35dMSh3Oq4wZXy75nwMJNLxxArVJwuLmf53ccp7ErcKeLi/74BSW//YwnPqpBb7KO+ypjI81Y1rwYqWI3Tz75JM888wz5+fl0dXVxxx13jMDRnXnG71VxFiBlHLm21AnUBULaTqfTDZka7Pq+olXH33YeZ1ddD7Wdet440MJVf9nL2n0nsdodcvpvXrJ3nzSA+IgwchKchW+mJEUyJ9sZrrPl+4tP5/RPi2tKfFvd/sYDoTMN/u3KT2XKSecJyO2IPHlt70mWPr2LK16u593yXno96g0Pl/Fs6Z4Jf3Mo+zsd0V2xYgWbNm0CYOrUqezZs4eamhrefPPNkGo/jCViIm2Y2O126uvrCQsLIycnRx4fKi5XrVZjt9upqqoiNjZWnmwD30VwJEt349F2Xt930m2f/UYrj31Yw3OfNchj9X4svPIWLRq18/s1LzmSNZ81ArDJZdJqtNno57P9jQ+Hw82DbpcuvYX4SDVv3jGf+zeUy8LsyprdHaw5VSLzfy7N56LCZNJih3cjj3TI2Egy1inAOp2O3NzcEfv8sw1h6Q4DyY/ra9JMrVYPGZdbWVlJRkaG1w3pGa3gKsJSp9/fXFXAzfMz3bbrNQztygBncfJ9x50lGL+xKFseP93qY2cbfQYrl63Zg9k2+Hc+d2oi/3t1gde6j31YI7sg/rHzeMjNOcezpTvWKcATuawjCNEdFpIf15foDmXpdnR0MDAwwJQpUwJu5/o+P9XZ6ddic5Dk8pj88q1zmJUZWvqr1Tbo5vj1VQXcf0FeSNuPFBqVgilJkWhUoytMi3MTSIgMI/yU5b+zrodH/lPFeVOiWL0klYI073KDz37awBXP7ZV9wNXt+oA+0fEsuqKW7tgi3AshIsXjSuFhgbLNpDGDwUBVVRWLFi1CpVINGbsL7hNp+alOIfjoWAe763vldW59+XDIx+9ai+HRTVV8feHYhNmYbQ4au53WY1J0GN36kfGlevLJD5dy5V/2MmC2EaZSsLu+l/RYDdfNyWByUgRvH2qlpmOA7Y3On0C8tvckr+11unpWTE/iznMnU5wV65WAMZKiO9KTXuPBvTBeOgFbrVY3N99oICzdEBhuXQWVSkVVVRWFhYV+s3yGKvcYq1GQFKkmOVLFY1cFX2M3GNZ6+IrHAl+Cu3jK8GooeHLBs7sZMNuYnhbtTLS4ppAZ6TFsONjCU1vqMHi0lg8lrO7T6m5u+dch5jy2nWue38vO2m45smIkRXesRXKk9zdWlq7JZOLf//63/PqNN97g5Zdflms5jBZCdIPEMx4XfPtvfY3pdDoiIyNJSUnxu39/GWh2u52jR4+SFaPgWEsfucpufn1u1OmezrjHrQ3QCFDdrmfZ07vY3dDLncsns3H1Qr7/lVy5zq+EFFZXlBHDrUuySYoKriBNfZeB1etKmff4duY+9hk7mq1YbCNjoY737LazRXRbW1t5+OGHAfjss8948MEH+e9//8v9998PjF4YnXAvBIlWq6WxsZHp06fLY8FYun19fRgMBiZPnjzk/n11BAaor68nMTGRkslW/n2kjVnFxZQoFOiiTvDUljq3ddVKBVOTNFR1ntnA/7OZjUfa2HikLeB6Fa26kIq0u2JzwItlFl4s2wHAXedO5o5zJhGlGZ51eSZEciI2pdRqtaSlpWE2m9mwYQPr16+nqKiIq666Chg9P7ywdIPAtZKSZw3ToURXyjrLzMz0+S3qOuZrXzabjc7OTnJzc8lPicJgsXOyz1k39xsLs/HEanf4FNwfXzw1hLMVuHLPeZPZ8//O5cj/nEd67KBraFpK8E8bf9t5nCW/c2bC/b93KugIMRvuy9aU0mq1jknd2+joaHJzc3nsscfo6uri3HPPpby83G/N4zOFsHQDEGp4mOt65eXl5ObmyqLtijRRJl2snqJrs9kwGAzMnz8flUrFtFMRDDXtAzz5US2fVncHfQ5Pf1xHRlw4rf3uN/sNczNo6Td5lYX8srNwcrwcQheIv2w/zvaaHtJiNbRpB6u+1QaoGeGPD8o7+KDcGQs8MyOGNTcXkxLjFCCb3YHOZCU+0t2lcTa06gllMmqssuEyMzO5/vrr+eyzz3jooYcAp/V72WWXjepxCNEdAofDIZdXDAsLCzpSwWazcfKkc4IqKyuL5uZm2R/suZ6r6Lr6dCsrK9FoNERFRWGz2ZiW4ozVrenQc3VJekiiC3gJLsBbh1pD2seXhWAFV6K0RQtDVM68aX4mRRkx7KjtZmtlF5FhSs7JVDC/YArP7ziO1ug7lrq8Vcfla/bw6BXTeftQq3xcu350DrERg7fmSD/2nomJtLMhG+zYsWPodDp+97vfyWPFxcXy32K00raFe2EIXOsq+LrofY2pVCpMJhMNDQ1y08RgfL+uPt2Ojg6MRiMRERHyWEy4msy4cKo79D5jSQVjR1dXJ/mqTu6epeQPl6awJDuCT47b+MO2Os6bEs2fr83j3nMyyYn3Fiaj1c7/bKx0+yI45/e7ZDcSnB2WbrAiPhZWrlarpa6ujldeeYW1a9fS1tZGeXk5JpOJdevW8eabbwKE1B7rdBCWrh9c43FDsTIUCgVarZbFixfLj1zBiK703mw2U1VVxcKFCzly5IibCyI/NZqajgGe/aTh9E9QMGJsbbSwtbGHKYnhXFqYwC8uzWXX4Uo+74nmw+o+Nlf1cU5OBD9YEElZh5KXjgbObrv0VM3gm4vjWJipIUVloL293a1djmvLnFCuUSmxZ6QIRXQNBgORkb5rhJwptFotW7duZcuWLVgsFp566il0Oh0KhYK6ujquvfbaUT0eIbo+cDgctLS0kJqaGvJjndQFwtU5H6zoShNv06dPJzw83MsFkZ8Wxfbabqra3fuhBUN8pJq+IFKGBcOnscfE33a3Ud9rI1et5EeXzeTBSx38bWcT6/afZEfToPU6f1IcB5rc40MTItVead1vlPbzRimoFPDAOR2cmxOOzWaj32Dmv41GlmcpUeEeaijFkPvrNtzb20tkZCQ9PT1ey5RKZcjXfCiiG2q4WFNTE7feeiutra0olUruuusufvjDH9Ld3c3NN99MQ0MDubm5rF+/nsRE39XzkpKSuPLKK7HZbGg0GpYtW8bx48ex2+0UFxfLHaBHq5uxEF0PJD9udXU1aWnegfK+/GvSWGdnJ1qt1mtm1p/ouvpwpay1pKQk+XM9w8jyU4bvVhCCO3psOeasIfz30t1+1/EUXHCvo1GSFcttS3P4x87jHGvTY3PA0zu7+ObPzsNss3PPulL2Hzdw6cK5lGS7Z3cF6nFmtVoxmUx0dnZ6dSP2jBX37HHm6/fAwAB6vd5L7H25MEINF1Or1fz+979n/vz5aLVaFixYwMqVK/nXv/7FRRddxEMPPcQTTzzBE088wZNPPulzHxEREWRlZbF69WqOHj3K7t27ueSSS8jOzqanp0dubT9aCNH1wNeFJ+HZigcGoxBsNhuVlZUsXLiQ/fv3e23nOQmnVCrdxoxGI0aj0avco2u1qrou54x5bITa7+SM4OzlgoJkDp/op3vAwtGTWn78doVXbO9X/7pPTp8GSPCRvBGo07Beryc5OZmkpKQhjyeYJpVGoxGz2UxPTw/d3d1uyzz9t6+99hqlpaX09PSwevVq4uLiuPTSS7nooov8HkNmZqZcczo2NpaioiKam5t599135dY9t912GytWrPArupIPe/369Xz44Ye8+uqrvPDCC3zzm9/k3nvv5cYbb+T6668ftZZIQnRdCOTHlcTTVXSl9tNlZWUUFBT4nMWVSjp67ksas9vtlJWVERkZ6bZvVwv577uaeGFXE4AQ3C8pn1R1eY0NeKQpuwouwGc13eQkRJASoyE1RkNStCZgH7hg43SDbVLZ1dXF9OnTh/QTOxwOCgoK+OSTT9iyZQu33347Wq3WrSxqIBoaGjh48CBLliyhra1NFuPMzEza2/2XBJXE/9VXX+XJJ58kMzPTrYfhSPQiDAUhuqdwOBxugiv9M1y/+fxVFWtqaiIqKorU1FS3/Q1V2Nx1rL6+nuTkZK8Lx7XoTaAQsdQYDR264LsHC74ceLajVyogMSqM1BgNKTEa0mLCZUFOjdGQEquhT2chZQR1JhifrkKhID4+noiICCZPnszixaEV0NfpdNxwww384Q9/CLlYjnQfxsfH09PTQ21tLStXrgSgt7dXtvhHqyqcEF0G/biuLVakxAfXb3hfyRB2u5329nafbdddY3D9xfj29fXR2dnJokWLvERXWqffaOVEr4k52XFuhbldEYI7sUmMCmPVgkzsDujUmek49VPZpqdLb8a7d2k38ZFqpqdG87dvlBCmOr3H6mAFazh1FywWCzfccAO33HIL119/PQDp6em0tLSQmZlJS0uLz/kXCemevvHGG9m1axcHDx5k//79vP/++2RmZlJYWBjSOZwuQnTx7nMGvkXX02K1Wq309fUxY8YMN4tYElnXoGtfpRxNJhNlZWXMmTPHpy9JEu+/bW+g32jl55fn84dP6tlRO7EyyASB6Rmw8Jftx7mkKIVfX1lAdPjgrW2zO+gZsDiFWGvi4LE66o0RbK3qwWixoxqltvQQuug6HA7uuOMOioqKePDBB+Xxa665hpdeeomHHnooqKaUdruda665hu3bt9Pf388XX3xBSUkJv/zlL0c9hG3Ci67NZuPQoUOUlJQErKvgaekeO3aMuLg4rwmLYDsCt7W1MWnSJKKjfUclSNEL7afST3/xXhVlLcMrwiL4cnLD3AzOy0/iz/9toKZjgI8qOvmoopOCtGj+eOMsVEroN9roN1rQGm1ojVaatVa21ju/uKemRGK1OdCoR0d4dTqdHKIVDDt37uSVV16hpKSEuXPnAvDYY4/x0EMPcdNNN/HCCy8wefJkOcHBH0qlkq1bt5Kens4dd9xBTEwMYWFh6PV6lErlqGbUTWjRleoqaLVar2WBCpS3tLRgtVpJSEgIqqauJzqdDovF4jaR4OlHlvbzxHUzWTQphud2NHntJyZcxbVzMnh1T3NwJy34UvHWoVaf6dxV7XouW7Mn4PYbj7Zz1/IpckuoM02olu7y5cv9ZrFt3bo1pM9+8cUXOXr0KLm5uSgUCjo6OsjJyWHWrFl8//vfDxjNMVJM2DRgVz+ur1q2Q9XKHRgYoK6ujlmzZgVdCMcVs9lMc3MzSUlJXta1Z+yu3W5HrVJydUka7969wGtfl8xIZdNRZ6nCmzz6pwkErlxQkCy3R3p+VTHfWZpDmEpB9qlO0Z4MmG1sKm3j3nWl/PjtCp/rjGYn4NMlPj6e73znO/zqV7/iF7/4BStWrCArKwutVsvdd9895D07kkxYS9fVjyuJpOsMrD/RNRqNHD16lJkzZxIWFhZ0yx4YDF0pLS1lypQp6HQ6r+36Bkwkxzn/LZ7JEU09Rjx5+xwqzlgAACAASURBVPCglbP+wBBVWQQTHteQtNXrBts2LXpyBza7AwcQGaZkWkq0s8iPCzPSfbvAzpZauna7nS1btvDcc8/JY/Pnz2fhwoXs27ePoqKiUWvbM2FF12azyaFhknC6+nX8hXl1dHSQkpIipxyqVCq5EtlQ20rhXy0tLYSHh5OSkkJfn3u1q7o+G9/4/edcOjONH144jQ+OdrOzthvL1i569Waaer1FVyA4XawuoQ0Gi91LcMHZ084XZ0vXCKvVyuzZs3n22WdZtGgRUVFR1NTUEBERgdVq9TuvciaYsKKrUChky9OfVetZjnFgYACDwcDUqVPd1gu25KNWq+X48eMsXrwYi8Xitc7UpHAuL0rmg/J2Pj7Wge3UzbA8P5qcOM2wRDdMpRixtjGCs4tleQlUtOq86jmESnFmLK99Z67PZWeL6IaFhfHTn/6URx99lJ07d9Lf349Wq+Wvf/0rfX193H777aN2LBNWdF3xJ7oGw2D2j9lspqmpicTExJCjHKT1KioqmDlzppyh5rldRJiaX1yaRcmkJJ7eUi2Paw0WcpS+i2ZnRiv5zflxfPeDXp/LheBOXD6v931NBMvs7FiONGu5bm66V7djibOlE3BbWxt79+5l8+bNcjJTcnKyvPzee+8dtWOZsKIbjKUrjTkcDtkP29vrfiH7s2pNJvei4QaDgcTERBISEuR1fBUXcTgc3HHuFArSY/juKwcBONzcj79m6xqNhqcPDLo3UqPVdOhFmrDg9DnS7HQz/PHTBipadUSEqYhUK4kIU3FJUQq5yVEhi+7AwMCoPspLaLVaPv/8c+67776QQtbOBBNedF19uq64jjU1NREREUF6ejqdnZ1+15Pw1ZzSZDKRlZUlj/lqRKlSqTBbrPy3upO1e08EdR6NHpNrQnAFI02fwcqGg+5haQoF3Hnu5JBF1zXrczSQ7nGTyUR1dTX3338/K1euJCYmhtjYWLeCOqPFhBVdV4YSTq1WS3NzM4sXL/bpEvDnXvBsTpmSkuK2nmeYTZfOzFvlWjbXtNCqtZAcreGbCzN4dd/EbKkjGD8smhLP3sbBSd8b5mZwYYHz0XykW/+MNNJ9plQqSUxM5MiRI+zYsQO73U5TUxOXX345L7/88qiehxBdnCLp6Q6QJtJKS0vlPkoKhWJIi1jCVYgrKyuZNGkSRqPRZxiZ1WbnV+9V8s6hk27+136jRQiuYFzw/KoSlEoFG4+08sRHtbx1qJV/H27l/fsWwzgXXXBauzNnzmTz5s1+1xnNc5iwoutZPcyXmGq1WqZNmybPtvqrj+DL+rVarbS3t2MymcjJyaG+vt5vllp9p57paTFosBGtUbKzUScLcGK4gqjIcJpFuJhgDPj0/qV8XNnJxiNtfF7fg90Bs7NiuW5uBplx4ZzoD74TsM1mG1XXgoRCocBsNrNjxw72798vl6tUq9Wcd955zJ49e1SPZ8KKriu+sso6Ozux2WwBne7+6u5aLBaqq6tZuHChXJfUl+iqVUpevX0hAJ8dqeU/5e4lHHtMDnpMQnAFY8OVz+1Fb7aRERfOd8+ZzFUlaeQlR8nLQ+kErNPpiImJOVOH6hMprX7z5s288847fP7550RFRZGWlsauXbv4v//7P2bPnj1qBcxhAqcBu+Jp6RqNRmpra4mMjBxWuTeVSkVfX5/c60waGypLDaCs3cjGitDagw+H2Vljk4YpOPuwOxzEhKvo1JmZmRHjJrgQmk/3dLPRNm/eTGFhIfn5+TzxxBNBbSPdc9u2beOqq67ioYce4u6772bz5s088sgjY9I6fsKKrq8yjuAUwSNHjlBUVORXcAO1kW5tdfpiXWt8BtWc8tQ3bVFKGGtWzWbZ1DNTgOPISe+MI4HAFwaLnQGzjUsKEpiZ6kyDt1qt8j0QalPK4Vq6NpuN++67jw8++IDy8nLWrl1LeXn5kNs4HA65AqDUFUar1VJeXo7ZbKasrMwrFX80EO4F3EW3rq6OxMREvxWHfPVJg8HQFL1eT3Nzs9c36FCiK+0rOy6MMCV0m5Skx0Xwr9vmc8Uz26jtG912IgIBODsQXzg1mutnxJAaCT1tJ+k4JV5SDzSj0Uh3d7fP1vCur48ePUpVVZVcuyQ+Pp7U1NSga9nu2bOH/Px8ORt01apVvPvuu8ycOdPvNgqFgtdff52lS5dy3XXXERcXx7x58zh48CBXXnklkZGRcrnI0SpgDkJ0gcGJr56eHrmLA3iXWpTW9decUqFQUFpaysyZM72+hQNZug6HgxRbF79cHsOaw1ZueXEf//vVIiG4gjHD5oAttXq21OoBWHf7PGZnursHSktLmTp1KhEREXJTSquLMEu/m5qaOHz4MMePH+fpp5+mv7+fW2+9leuuuy6oY2lubnabX8nJyeGLL74IuN3LL79MUlISl112mTy2Zs0ajh49yty5c+UmBUJ0RwHXP7KUKFFeXs78+fODatnjaslKE3HHjx8nOTlZzjpzJZDonjx5Eo1GQ0GqirfunsEP3jjCT94qG9FzFghOh7gIb7mQ3AuBOhDfeeedZGRkUFNTw6OPPhryZ/ty6QUjlGFhYTQ3N1NTU4PFYiE8PJyoqCiKiorQ6/Wo1epRj6iYsKLrisPhwGAwUFxc7Pa4M1Q4mOdYd3c3XV1dspXsyVCiazAYaGhooKSkhNraWpKiNfzztvkU/3rbCJ2hQBAcaqWCMJWCHQ+eg0YdWIxGayItJyeHpqbBIv4nTpxwy/D0R0dHBx9++CFVVVVYLBY53j4sLIyBgQEeeugh0tPTh3VMw2XCi67D4aC5uRmFQkFGRobbMn/xu57iqVAoqK6uZsGCBW7fmsF0BLZardTU1DBjxgzCw8PldZQKBZFhSgwW4V4QnFmW5iWw+1RxHJVSwbK8xKAEF0KfSIuPjx/WMS5atIjq6mrq6+vJzs5m3bp1vP766wG3UyqVXHvttWRmZtLX14fZbMZoNGKxWOjt7R31/mgwgUVXqqWr0+k4fvw4kZGRPluuB5OBptPpSEtLIypqMJxG8vNKF6S/zLW2tjaio6NJTk6WC6sDfFrVKQRXMCrsdqlGZrLaOT8/tKiZUDoBD7fYjFqt5s9//jOXXnopNpuN22+/nVmzZgXcrrOzk+uuu25MxNUfEzZkDJzf0lKab1hYmE9RDFTMpr29Hbvd7hXt4BUO5sPStdlsdHZ2UlBQALgXwXltj3c/tOFwQUFy4JUEAhfWH2gJGBY5HE63lu4VV1xBVVUVtbW1PPzww0Ftc//998uGlN1ux+FwuP2MBRNadFtaWsjMzCQuLi5oV4LrelLlovT09IA1dSXLV8LhcNDe3k5mZqZXq/baDj07a90z04aLa4sWgSAYylt13PdGGZVtIxvDOhYFzO+991550lupVMpPuNLPWDBh3QvgdM4HW1NXQkrxdTgclJWVMX36dAYGBgImPnj+gxsbG4mKiiIiwrsp4EhZuQLBcNle28322m6KM2P53fUzyEnwfjwP1VIcq64R440JbelK33wQvOhKYydOnCA8PJy0tLSQmlOC8+JraWkhIyPDax2D1cFre4KrpSsQnGlKW7RcvmYvHx/r9FrmOlEcDGPZCXg8MaFFFwjZ0pXa+DQ1NVFYWAgEl+IrYbfbKSsrk7sJe66z/YTvIuT5qaNfbV9w9vPczcUc/p/zeOP2ecPaPlqj4sriNFJiNF7Lzpb+aOONCe1e8Fd/QcJfV9+Ojg4WLFggZ6UFKmTuSkNDA0lJScTHx3s1p3Q4HKyttHhtA1DToQ/t5AQC4N43SgM2J81NjqShy+A1fmFBMs/e6D9CwGq1nhXt18cbE97SlQjW0m1paUGj0bjFG/oTbM8xu91OW1sb06ZNk9dxFd3P60Zm8kwgcCVQc1JJcG+Ym8Hh/zmPdaes4lgfGWiuuIZEBoPZbB6Tql7jjQktuoEsXc+x3t5e+vr6vGL+gnEv2O12DAYDM2bMwO5wFi5XePRJe9XDl5sa6/1IJxAEy+rlk8lJ8J6odeU7S3Pk14XpMSgVCj6v7wHgW4uzh9zWV+GnQIxVxMB4QrgXThFIdK1WK+Xl5cyaNYvKykq/60lIUQ4StbW1REREEBkZyc66bu569RDJ0WHMSVFyk6aDyUlRbD3W4baPzd87h8ue3U7HgO8JOYFgKJ7fcXzI5StnpPDgRVP5oLyD1n4Tj31Yw8k+I5uOtrM0L4HC9KHLMFqt1jHpBHG2M6FF15VAoltZWcnkyZOJjY0NyverVjtrj4KzG3B3dzcxMTFYrVaW5SVx88Js3tjXzDY9bGv0brA+JyeON/Y10TFgIz1GTZtOdPkVjAzzJ8VxoKkfi81Oh9ZEa7+Je8+bQpvWxL92O5+2fjp3WsD9hOJeCDXS4cuMEN1TDDWRJvU6y87O9nnhDOXTtdvtlJeXU1JSQkNDAzabDY1aya+vLqI4I4Zfv1+Jr2zfwyf6OXyiH0AIrmBEOdDkvK4+re4mLqIegAsLkylIi+atQ84C/H/d0UhxVozP+FwJq9UatHvBZDIJf+4pJvSzgWtWir9CNna7nerqaoqLi/1+Uw/l062uriYzM5OYmBiv9b62IJv/WTK0z00gOJNsPNoOwNf+cYDZj22Xx2s6Brh8zV72Nvb62zSkRpMiXGyQCS26MHScrlQZv6CgwK2mrif+rN+BgQH6+vqYMmUK4FucdzULK1Ywfrn91SN+l4XiXjidcLGf/OQnzJgxg9mzZ3PdddfR2zv4RfD444+Tn59PYWEhH3744bD2P9pMeNGV8BXideLECRQKBampqV7rB5MC2dvb62Yhe4rui7uOs+V4YNGdmeK7MLRAcKZIjVaxPDeG56/Po7e3F61Wi8FgwGKxyDVEQnEvnI6lu3LlSkpLSzly5AgFBQU8/vjjAJSXl7Nu3TrKysrYvHkz9957r98s0PHEhPbpulqoUrEZCb1eT1NTExEREUG17PGksbFRrlLvuR3AoaY+frelJqjjrOoejIIIVysxWUXJR0HwzM6KDboZ6Ys3TWNynNql9Y6F9vZ2t/Y7knFiNBpRq9U0Nja69UTz7JF29OhRjh07hsFgoKKigvj4eJKTk4P28V5yySXy66VLl7JhwwYA3n33XVatWkV4eDh5eXnk5+ezZ88eli1bFuJfaHSZ0KLrD7vdTmlpqRwe5um7CtScsqurC4PB4NW6xFV0JydFcsviHLnOQmyEGq3Rt9XrqrFCcAWh4im431qcjcXmYN3+k17rvnG0n6evLwpqv1VVVaSnpxMbG+vVH81VpOvq6ti/fz9NTU385je/ob+/n3vvvZcrrrgi5HN58cUXufnmmwFn37SlS5fKy3Jycmhubg55n6ONEF28w1nq6upISUkhPj5e9vW6CqivPmlS6UaHw8GxY8eYP38+Bw8edPsclUqFyWQCIClaQ1LUoJ/Yn+B6snxqIjvqeoZ1ngLBcxdHc96SaTT1GHyKblJ08K4sKQ04UH+0e+65h7S0NFpaWvjZz37mc52LL76Y1tZWr/Hf/va3fPWrX5Vfq9VqbrnlFmD4fdPGmgktur5qavb09Lj1Ogu2JoM0VlVVRW5uru9K9Qolx9oH2NF2nD0NPXxU0eG9TgCE4ApOh3s/1qPetp2vTHcW3V/7nXl8/Z+DxsHafSdZOSOFRVO8m6t6MpLRCx9//PGQ27/00kts2rSJrVu3yvfscPumjTViIs0FlUolx9RKF5OvCTZ/iRTt7e2YzWa///hXD3Zy//st/O8HVcMSXIFgJLDaHWytdBa3dxVcidtfPcIfP6kPuB+73R70RJpWqyUuLi60Az3F5s2befLJJ9m4caPbHMk111zDunXrMJlM1NfXU11dzeLFi4f1GaOJEF0XDAYDmZmZbv9YX/G7vixdhUJBfX09s2bN8vuIc05eYOtBIBhrYiPURIQFDgULpcqYXq8ftuh+73vfQ6vVsnLlSubOncvq1asBmDVrFjfddBMzZ87ksssuY82aNSEV4BkrJrR7AZxiKbXOAUhJSXFbHmz1Ma1WS2ZmpteMrKu/eEpSFEuzw9ndbBrp0xAIQuaq4jQ2lbZ7jWuNVjaVttFvtPKtxdmkx/mOMvCM6hmK0wkZq6nxH+Xz8MMPB90vbbww4S1dhUIh9zpLTk4O2OvM11hbWxsACQnulqxnGFpSTDgPLI7l6tnurd4FgrHAl+AWnSpyU99l4KUvTvB+mfc6rgQ7cSVq6Q4y4S1dgLKyMgoKCujp6Qm6I7BUQcxsNlNTU+OzOaW0rZTNplKp2HfSyH+OjGzDP4FgpNCarFxVnMbcnDjm5sQxPW1kOpaINOBBJrzo6nQ6oqKiSE1NRavV+rRqpTAv1zGDwVn4uby8XG5OGSjKobbLyO++GFpwVy3IYN1+Z+hMYlQYPQO+O0kIBCPJAxfmcU1Jus+2PCOBEN1BJrx7ITY2Vu51FkpHYJvNRktLC0qlkrS0tICFzMtO9vOPXUN3+f3wB+fwXulgVIMQXMFIo1RAUYZ7ndyUaA23L5sUkuCKTsDDZ8Jbup6FzH1Ztf6s37q6OjlExdX6dV3PZrPx3VcOsr2mK+CxGMw2dKbxnzsuOHuxO6Ci1f1p67alQ3eI8LmfECbRwCm6MTFDF0WfKEx4SzfUlj3gtGC7urooKCiQs3D8WbpWq5WcxEhUSvcJh7fvXsTX5rvH8177/BeEZj8IBMNHfeqavLTIu6BTIEJJjIDQ+6l9mZnwlq4rwWafdXZ2elUf8zfhZrPZ+OVVM1g2NZEfvHFUXnb9X/eegTMQCIIjJVpDQpSa2HA1mfER2OwOuvVmjFY7OQkRAaMShtMfTeBkwv/VQrV0DQYDzc3NXmm+gZIoXAX3zuVT+PuOxhE7B4EgVDLiwiltcRbCufDZ3XTpzdhPPWa9c9cC8lOHjlqw2WzCch0mE9694Eog0XU4HJSVlTk7+trdq335Sxe22WxuDSevylNz31fyyIwXrUsEY4ckuEXpMZw3LYnl05y1GJblJZCXHDXUpkBoomu328+KQjSjhRBdF3yJruvF0tTURHR0NCkpKV6zt/4sXZPZwr1rBxtPXjo1HIvVxvxJ/lOCl2SoWJKXeDqnIhDIfHOR74my6WnRrP/ufH54QS5V7Xoy48J58toir/kHX4QiuiJywR3hXgjgXpAYGBjgxIkTLFmyxOdyfxNpH1QOtha5eWE27YY+Fj253XNzAL63Io+aDj2by9oBUU1MMDK8unewxuzlMxKp6TZT3a6nul3PM1vr2F3fS7fezMu3zSUxKrjSjkJ0h4+wdF3wTNuVcDgclJaWMnPmTL8Xmi/3gtEGf9o9GCp2tLmf3+zyTo6Yl6HhyCMX8P0LpvHsTbPJiBaPYhOVpCBFb7h8cKyH6nY94IzZ/efuE1S06UiMCmNSov/Ov56Mtug+/fTTKBQKOjs7Aec9+YMf/ID8/Hxmz57NgQMHTmv/o8mEt3QlPAuZu2I2m0lJSfGqreC6ja/wmVf3u5dvLG/x3TLlJ0tiCD9V1emFnY206kXg2ESlexQTYuwul1mb1szFz+5iVWEY52WrUSqVbu13PFvx9PX1oVQq0el0buO+7qHTrbvQ1NTEli1bmDx5sjz2wQcfUF1dTXV1NV988QX33HMPX3zxxbA/YzSZ8KIbyMGv0+mwWq3k5ua6jUudIvx92zd1D/DK/ja/+71t6SRe2t2EWqmgpsvM55/U8adP60I+foFgKK6dnc5vrnZmXO47cIiXa1R8Uu3uurp1STYtfSYaug3ow+NYuDAfm83mswWP1WrFZDIxMDAAwPHjx93a87iiVCpZu3Ytx44dQ6vV8rOf/Yz4+HiuvfZaZsyYEfQ5PPDAAzz11FNyBwlw9ke79dZbUSgULF26lN7eXlpaWsjMzBzun2rUmPCiC97CK1mwdrudsrIyYmNjfU6cDVVP9OF3y3yO3zQrjhsXZPPN1ysBZ0HpR3foAFEER+DNt2eG8a/y4Vu/v7qqQH6txE5SlLcL4Zy8RM49Fb0gIVmv/ppH2u12YmJifHbKBuc9ZLfbyczM5L333qO8vJwVK1bQ19cXdENKgI0bN5Kdnc2cOXPcxpubm5k0aZL8XuqPJkT3LMS16WRDQwPJycno9Xq/oWSeF5DJYuP+N4/yRUOfz/2vL+tnfVn/GTt+wZeL0xFcgL/tOI7ZZqe2Y4CjTXo6DO5f7rnJkdjsobuzAvl0FQoFKpWKrKwsYmJiKCoq4tJLL/W57lD90R577DE++ugjr2Vna380EKILDBYyh0ExNRgMtLe3s3jxYioqKoLKVDuhc/Cd//1k1I5bIAjEms8aUSpgSlIkesugUF1VnMrPL52GRq1EoVBgs9m8RGuoNN+RnEjz1x/t6NGj1NfXy1buiRMnmD9/Pnv27Dlr+6OBiF7wQq1WYzab5Rbs0oRCoEy1g029/HynwXN3AsGY8/9WTmPdrbOI0ThF9bxpifzmyulo1M7bX3IFSH5c6cdisXj9mM1m+bXD4XDz59psNux2u9sPOEV3OK16SkpKaG9vp6GhgYaGBnJycjhw4AAZGRlcc801vPzyyzgcDnbv3k18fPxZ4VoAYekC3pZuY2Mj6enp8rdzMDUZMuMimJsexqE298fBq6eqOW9uIWmx4aTGaHjj8xpePtB5hs9IIBjkiY9qibak0D7gvMaf+Vox4eFD3/quGZfSveFwOOQfvV7v1ktQGvfEZrPx3//+l/j4+JE4FZkrrriC999/n/z8fKKiovjnP/85ovs/kwjRZVB0pces/v5+iouL5eW+XAmeQpwRH8GDCyLoM0fy/Y8HfbYfNlpRxXazPD+Zv25v4PO6bnnZuu8uZMOBk2w4cPIMnp1AAD//xPlF/5OV04gOILgwtGuhq6uLhIQEr/ojntjtdhwOBx0dHdx6662hHbAPGhoa5NcKhYI1a9ac9j7HAiG6LthsNrq6usjLywuYqeYpxGazGZ1Ox6SpRaiVFXxzySQum5XGXzYf5N+HW/j34RYA0mLCaNc5reHfvFdJmZ/YXYHgdFiRo+LTE961medG9VJRofcZfxsWFuYVj+trcqq5uZkpU6YEPAalUsnOnTtZuHAhiYkirV1CiK4LNTU1JCQkeJWs81fcXOqTBnDs2DESExPZWN6FzeHgG4tySI31Do2RBBcQgis4Y3zzK8U8mhrFhX/43G28yZbA+ZlxXnG3UoSO9GOxWHymtSuVSrRaLWFhYXR0dPhNoJC2feWVV7jzzjtH7bzPBoTo4nxU6e3tpb+/n8zMzKAKmbt2iujo6MBmsxERFcO/S09yfn4y/UYrd712iMYuG3eeO4VFeYnc9eqhUTsnwcQhMz6clj53o6AwI4a2fpPXug9tquXBi6by3XMDW6quOBwObDYbDQ0NxMXFkZqa6ibSBoPB7f2ePXv4+9//TnNzM2Vlzpj18847jz/84Q/DP9EvCUJ0cfqeKioqmDdvHlqtFqPR6LZ8qP5nVquVqqoqFi5cyOs7q+gesGKxOVj1j72kxGj42dIovnFBLiqVmhe+NY87Xjk4imcmmAh4Ci7AhX/YhcXmO/72ma11IYuuFHfb2dnJggUL5I4p/igpKSE8PJy2tjZ+/vOfA6H3VfuyIkLGcPqeSkpKiIqKCrpljzRWWVlJbm4u4eHh/KfCmRCxq66bi4tS2XjvUkrSI061NlEwYBb9zwSjgz/BlejUmUPeZ19fHzExMQEFV2Lt2rXcdttt8vuzJXnhTCNEF6foSk3zQumTNjAwgNFoJCsri5p2HWXtRiLUCh6/diZ/uLGE+Mgwt+pjK4tSeenWOfxqaRhv35RJ5a8u5vsr8ogNdwaZq5WQGasmPlz8WwRnhu+vyAPgt5urQt62ubmZ7OzgmljW19ejVqvditQInAj3ggfBiq5CoaCvr4/ly5ejUChIitbwtdnJXJIXzlfmDWbGuLomFAoFc7Ji6K81UVhYiN1up7yln8VTErhkVhpfmZ5MXEQYerOVZU/twOonPfPj7y/miY9q+Liy2+dygQCcX+LWU+G2CuC2pTmsPm8Kzb1G3j7Uwud13SybmjTkPiQsFgtardar0p4/Xn/9dTcrVzCIEF0Pgm1O2djYiEajISIiAoCkaA0PrphMe3v7kNtWVlYSHh4uRz88c32RPCssEa1Rc9fyKTz3WYPPY7z4T3vc3s/OjmP+pHj+tbvJ5/qCiYnVpaOUUgGzwzv54osuLk1xoJ+iprWhioN94X4jEFxDyNrb20lLSwvqc+12O5s2beLhhx8+Q2d2diNEl8DdIzyLm/f09DAwMOAztMxXEoU01tHhrK8bFhaG3W7HarWiVqt9BqLfuXyyl+gWp4VT2u49aXKkuZ8jzaKIjsA/18zJ4LKvFMnvly9zyNegFCI2VBhZR0cH0dHRXkaFp1Bv2LCBtrY24uPjef/994mPj2fRokVBW8gTASG6Hkh1cv1hs9moqKhg7ty5HDrkHgLmL8pBunClKId9+/ZhsVhQKpV+M380Ku9xX4IrEATDhYUpbu+laASVShWw1GJ/fz82m82rvKIURuYq2iUlJZSWlrJ48WKqq6vp7e0lLy9PiK4LQnRdGKp7hERNTQ3Z2dlueecSQ7kmqqurmTJlinyBOxwOn1WadtV28+BbZfQbffdqEwiGw0PvVPDYV52TuaHibwJNoVDIFq7kZrvwwgv53e9+x7vvviuiFfwgRBfnxRPMBdLb20tvby+LFy/2udyfpavT6dDpdHLrdrVazf79+73SLsPCwlAbHazMj+WtUvfq/tEaFbcszkatVKJUgM0BO2q6OHoy9Ky2vHgVF8zM4sXPhQ94oqA32/jhm6V895zJ/ODCPNRD1FZwxWaz0dvbG3Snh7fffpsbb7xRCO4QiNgkP3gGciuVSsrLyykuLna7oFzX8ye6ra2tzJw5E4VCgdVqZc6cOSxZsoTZs2dTUFDA5MmTSU1NJSYmhpzESL49L5HYU2FjGiU8vSKGUks+9AAAIABJREFU51dGsyy6k4URbcwPb2NRRBs/nuPgH5fHkxcf2nfn1PR4N8FNigrjgoKUIbbwz/y04GqqCsYH/9h1nPmPfcbx7uDKkLa1tZGWlha0iK5duzbo4jZvvvmmXD513759bssef/xx8vPzKSws5MMPP/S5fX19PUuWLGH69OncfPPNmM2hxx6PBcLS9YFKpfLqf2Y2m8nIyCA6Oloe8+yT5qubcEdHB5GRkURHR8s1R9VqNQqFgrCwMJ+B5gNmG7nJHSxIU3Lj7GTycr2zhxwOByaLle+9UUp9X2iuiK1V7qFm3QMWPqkaXrlJlVoNiKSP8cI9s6DGEMWWugF5TK1UuIUfWu0OLvvzbtbdsYDZ2UPXuW1ubqakpCSoz66trSUqKiroWN7i4mLefvtt7r77brfx8vJy1q1bR1lZGSdPnuTiiy+mqqrKyx3305/+lAceeIBVq1axevVqXnjhBe65556gPnssEaJ7Cl/dI6R/slarxWKxeBVJDtQnTa/X09fXR3JyMna7PeDkmUSURsULq2ZQXl5O7hTfweUKhYI//beRXfW9qJUKrp2Twa1LJ/HVv+zBAWQnRPDtpZO4YV4mEWEqmrt1vPnpIbZ3hlPROnL92PaeHPvJPU9RmcjExsYwLS5GFl2lAmx2B9+eE8tlU8Np7jPzo23OSJefrD/Ar5Y5fbEqlcrL3SVFN/T19aHX671cYZ7X8auvvsq3v/3toI+1qKjI5/i7777LqlWrCA8PJy8vj/z8fPbs2cOyZcvkdRwOB9u2beP1118H4LbbbuOXv/ylEN2zFSniIDw8XG5OmZyc7Lf+gi8cDgfl5eXk5eXR29srT7AFElxp22PHjlFYWDjkY11qjIZvLcnhtqWTyIp33jw5iRFEaVRcWJhKv9HKK1+cQKVU0NXRTmx8MjdPiUelVNDSZ/QbB3y2IQR3kKSUdJ7aVAvAk9cVUZAWw28/qOLtSj0PXj0f4/E+4BC/uLKA8/KTyYqPcItCcA0ha2pqIj4+noGBAZ9hZVKUz65du3jttdfo7e2lpqaGrVu3MmfOHL73ve8N6xyam5tZunSp/F5qOumKVNNXCtv0tc54RYjuKaTuv9JjvySSDQ0NpKam+ix15ytaAZyi2dzcTExMDAkJCXR2dspuhWBEt6WlhZiYmIDV9r+9zNsKPi8/mfX7T1LZ1uBjCy3g3QBQ8OXheP/gNXr5rDTUSiUv3TYPs82OWqnkvdJ2IsOUXF2SQZTG+YTmGoUgYbfbqampYe7cuQGv2aVLlzJz5kz+85//8Mgjj9Db2ys//Q3VdNK1pborwTSdFI0pvwT4SpDQ6XS0tbWxZMkS6urqgk4PNhqNNDY2smTJEsxmM2azOahwNHCmWzY2NrJw4cJhnccjlxfwyOUFTuvF4cBqs7Nn3wHypxcQHhmFze7AYnNgszt/rHY7VrsDq81Bn8HCXa8fGdbnCsYH0tNLQmQY2yo7iQ1Xs2BKAuFqFRabnY8q2rmwMEUWXH+0t7eTkpISlJEAsH79en70ox8xbdo0t3F/TSeHIpimkykpKfITpFqtPqsaUwrR9YGUoltTUzNkc0p/bXwqKiqYPn26vI1SqeTw4cNuSReu1oVrymVnZycJCQn09vZ6LfdXyd8XCoUCtUJBe2sraYlxZKUEDk5fu3fw8Sw1RkO33oJNlOM7K+k1WLj/TWcd2/svnMpdy6ewq66HPoOVK2alB9y+ubnZr8/V67NOuRWGayh4cs011/CNb3yDBx98kJMnT1JdXe0VpqlQKLjgggvYsGEDq1at4qWXXvJrOY83hOj6QKVS0dbWRmJiotzFNNhCOJLIpqWlyT2i5syZ42UxSN1XXf1kfX19WK1WIiMj6e/v9+lH8/x8X21WpPcKhYK6ujpmz56NxWKRx/xx44JMchIjOHoqrfjoSS09Axa/659pJiVG0tRj4JKiVG6an8V3Xzs8Zsfii0VTEtjb2Dtmnz87O85v+vfa2+djsTmwOxzMzXG6qT4oayMuQs2504YuciMV5/eVAOSLt99+m5tvvjnkx/t33nmHW265BaPRyLJly1Cr1RQWFnLo0CFuuukmZs6ciVqtZs2aNUybNo3Y2FgaGxvJy8vj8OHDPPnkk6xatYpHHnmEefPmcccdd4T0+WOFIkBh4Qlj5thsNsxmM0qlktraWpqamjj//PNlsWxtbUWv17s9PjU2NqJSqcjJyQGcgvvpp58yb948kpOTZbeCZ40GX9jtdvbu3UtxcbFbWJo/XNtf+8ufb29vR6VSodFo5DHPuGJfou1qWXcM2NlY3s1Le1pC/ZMKRoHVs8N4/oj7F2NalIrnrs4kKnzw/2pDyVf/WcHKwmR+eeV0uQeaL2pqaoiOjg66pfnKlSvZsGHDabdA/9GPfkR8fDyPPvqo17Lc3Fz27dtHSsrw4snHAL/fQMLS9cDhcNDS0kJqaqqbderPleDaJ62qqorY2Fh5XWnyLBiOHz9OSkpKUIILDBnnC85wtc7OThYuXOjTAnE4HHIYmy/RlmasHRYLl2dZmXF+NFtrdXzcfHZMVkwUPAUXoH3Axnf+3cKLN+YR53BgMBj4b10/AxY7xfEmysrK3KIPwN3d1d3dTVZWFgaDwe/TlCTa1dXVJCQknLbgOhwO1q9fz7Zt205rP2cDQnQ9aGpqIjo62mcFMV+P91Jrn97eXvR6PfHx8VgsFiwWi1fJRn8YDAZaW1v9pheHisPhoLKykoKCAr+PfK4FT4Ih5eRJirJ0/LGgQB6Twox69SY+PtbJbz9uHJHjF5w+WpONnMwM4iOdX8rP7islOVrDqgvmo1J6XxNSTG57eztKpZKUlBS3L2ODweD1JHXffffR39+PSqViwYIFxMbG8vzzzwedMuzK9u3bSU9PZ/r06T6XKxQKLrnkEhQKBXfffTd33XVXyJ8xXhCiewqFQoHBYODEiRMUFhZ6hbn4m0iTLIaKigpmz55NS0uLnI4YrKBVVlYyffr0oGeKA9HR0YFGoxmxyk42m81nRIUk2unh4dxyThxT0uK56/Uj/OXrsynKiGF7TRc//0/liByDIDi+vXQSkRoVlxSlyoKrM1n5tKqLGxdk+hRccMaPazQaOjo6mD59utxJZSh27drFOeecw759+9BoNOh0OrnwjSvBhI2tXbuWr3/9634/a+fOnWRlZdHe3s7KlSuZMWMG559/fsBjHI8I0T2FlMwwY8YM2QfqylATafX19aSnpxMdHS23qI6OjsZut/vM3HGlo6MDlUpFcnLyiJyHzWajrq6OefPmjcj+wBmrnJ2dHbA31qLcBCLUSrbXdJEVH85ftzeiUSn59dWFXFWSTn3nAA9sKKOmQz9ixxYKCr78kxS5yVFcNzeDMJfSoNsqOzHb7AGjFoxGI1arNSjBBdi6dSsrVqyQK+fFxsb6XC9Q2JjVauXtt99m//79fteRwsHS0tK47rrr2LNnjxDdsx2FQkFhYSFxcXHyxeeKv2I2JpOJ/v5+lixZgt1uJzY2lpaWFurr670ydwA5/My1In9OTg4nTpzwW7U/WIsZnJN7mf+/vTOPb6LO//8rR0t6pRc9aAstvehhW2gpLS4oAgXElQVdRNAvKuL1FUVdcVlRUL8KHrDr/gRFVxDkkEVWrULLpYAnrUCPLYU2vWna9MrRM/fn90eYIcckmbThnufj0UeTmU9mJpnkPe95f97v13vECKcaqWxRq9Xo6OhgFfoYJhQgZ3QgCs+245syGXw8Bdj+0FhkXJw9HxnohT6tHqHefOx6MBnDhwfjeHUXnt931i3H6oyhGtyEQAEkimtbZ+K1A1X49JdGPHVbDO5OD7tYENGGCH8RxkY51lloaWlxKdd1586dePnll10/xtdew7/+9S+EhJhkJu+55x4kJSXRE9IUBw8exPLly6HX67F48WKsWbMGfX19OHz4MONk2/UCZ3QvwuPxBtWcUqFQYPz48eDz+dBqtfD19UVycrJd75aKnel0OtTX1yM8PBy+vr7Q6/VQq9WME1uOjLa5cSaEoKWlBampqejt7aXX8fn8QVfr1NbWIjY2lnXo47b4YJyQdOGWCD98cF8awsSXjP9Xpa1oVWnwbKYX/AMCsfnHRmw7eQEeAh4W54zEY5NGwWgELigGUNqswrpDNYM65svFtW5wKZqVaqz69jw++bkRi7Ij8XONHEtuHeXwO0AIQVtbG+t5BYVCgaampkHfUT3//PN48cUXAQAPP/ywTWjhwoULuOeee1BRUQGtVotx48Zh9+7dEAqFWLRoEWbNmjWo/V4LcEaXASavlklBjErJ8vf3p/NunZX6UrEzrVaL/v5+ZGdnu2QQzY22dbpYc3MzxGIxZDKZxXLrWWp7eb3WzzUaDQYGBlxK07l33Aj4DDPFFEUelh66t6cAU0Z5Ypj/cNy5qQhdfTpMTxqO56bG0toREAAJoT7ILzfFAD0FfGgN9jt5sCEnJgBFDVcvn5aJtAi/QWkhu0KjfIC+cOl1WvT198PTLIfbHLlcDn9/f9Z3VV9++SXuv/9+t5Tebtu2zWZZc3MzJk+ejNjYWACgPdu//e1vQ97f1YYzulawLdfVaDSQSqXw9PR0SUGM2sf58+eRlJTk8peWMtqenp4Wy+VyOby8vJCenu7w9fb6Yul0Oou+WDqdDl1dXRCJRCguvtQI0zy1yNxYmz++bZQIes0A1IZLalQ8Hg+3jRLhhzIe3jt2qcTz6PlOHD1vX1ZyqAYXwDVncAFcNoPrN0yAHo3JYRgd6Il6hWlSd/vvMvxwvh0vZnvBT2j5mQqFQvT398PPzw9VVVUOL8h6vR4ikQj//ve/kZ+fP+jj3LhxIz7//HOMHz8eGzZsQGBgoMV6qVSKkSNH0s+joqJQVFQ06P1dS3BG9yJsu0dQnDt3DomJiZBIJDAYDHbb7zDR0tICPz8/utptqBiNRkgkEqcGF7BvtK3p6OgAj8fDLbfcYrMvewUZ1s0MqXXUXUN/fz9yQjzh7zkMfD4fAj4ffIHA9J/Pv5gNwcfHJ9vo/c0fF46RQd6mc0MI5P06fHZRgD3IWwh5//Xd1uiOxOEOtYxfmBaLv39fx2pbd6eF4VSTkja6XQMm4zohOgBpkWKcbe1BfFIiRg+/VGlGLubxlpeXIz4+3ubcajQai+e7d+/GoUOHIJfL6Vv8e+65B6+88orFsTjKWHjqqafw6quvgsfj4dVXX8Vf/vIXbN261WLc9Sxo4wzO6LoIIYQ2SCEhIaiurobBYGCdk6vVatHU1ITs7Gy3HVNzczOGDx8OLy8vt2zPaDSitrYWY8eOtVnH1mhb09HRgfb2dkxJTrYpf7YOlYR6BqG1W4spI4Uw6HthMKgubcgPEKSL8Gm5GhqtHiPFAtwS5oVCSS+8Pfjo15nFv3nA2CgxckcHwmeYEDuKmiHrttT/DfH1hGpA7xaPejA4E4/n83iYNzYcX5c6V4fLSw7BqjsT8Nr+ahysbKf77L0yOxHxIcxFNzweDx0dHYiMjLSbfWBORkYGhEIhbr/9dsydO5eujLSGrdDNY489hj/+8Y82y9mI3lyvcEbXDlRXCOuqNI1GA4lEQuesEkLQ09MDLy8v+hbM0RVZIpEgNjaWdaWaM7RaLaRSqdsKKwDTrV1ISAhjzuVgIITQGhBUCMZR+pmVUJXNtvxbVfi0vAR9emDd1BgEingolNSgX2fEglvEuC/FB9UdAyiWqnFa1osPfzTpE4R58zAjWggjeDjaaKrk6ui99lq8mIuyrz9ay/p1wT6eEIs8sOHeFEyKD8Ir354HAHx5pgV/m8lcdEBVYGZlZbHah06nw/fff493330XwKU5AldobW2lK9i+/vprm7spAMjOzoZEIkF9fT0iIyOxZ88eWrD8eoczumYwdY8w9+iEQiGqq6vprr4GgwHh4eFoaWmxEIE2vzUyj4cZDAb09PTA19cXLS0tduNmrtxG1dTUIDY21qW0MkdQE3Lu9MRlMhkCAgLc4onzeDyMGeEPsScPE2ICMD3dpCn85GQdfDwF9Cx9chJAaU61qNQ4Ud2FY9WdONaggM5AIBYJ0K2+drIRMkf64/aEYPzjhzrojQT/fWUK0t487tI2gn1M31Uej4fxoy4Vxuwoasa9Y0cgMcw2/1alUsHX15e14Tx69CimTZvmsqEFgBUrVuC7775DW1sbDAYDRo4cibi4OHz88ccATGG3pUuXorKykva6k5OTERkZiSVLliA1NdXlfV6LcEbXDGdGl7otTktLoyfPRo4caddrpRT5dTodtFotKioqEB0dTVeyDQwMMN5im+No0kqr1aK7uxtRUVEYGBhwWf6Rifr6eowaNcptnrjRaERjYyMyMzPdsj0A6Fap8O7UAORmXYphP3tHrN3xEf4iLMyOxMLsSPRp9PilTo7j1V04Xt0F5YCtdkFiqA+q2y9fAcfi1GHw9hRgc8mlPmZnLqjwwsRL6l9vF57DG7PjsLrA0tNNCvbA+S5m5bdgn0uGsPCsKS5euCwHTfIBxIcyhxeam5tZ9zQDgB07duCNN95gPd6cvLw8rFu3DkKhEH/9618BAO+88w69PiIiAgUFBYiJicGxY8euJ3Ebl+CMrhVU9oJ1Xq7RaERPTw9SU1PB4/FooRtHxsl8pr+trQ0jRoywmJFlcyz2Jq10Oh2ampoQGBiIpqYmu/KPjtLDqMfUf71eD4VCYbf+fTBQoQpXY8COqK2tRWbKGHgKXC+b9hkmxIzkUMxIDoXBSFDWrMLRc204XNGKlj7TBfdyGlwA+PysBjFBIgR6CaEYuHS+Htxbj/RwEcplauw63Ya/TOjDG5N84Ccw4PkTJo0PewYXMN22+3l5wsPDA/vLW5ER6YsIPw+MDBCBz3Ah1ul06O3tZV0u3tnZCZlMxmrClokZM2bQj3Nzc7Fv375Bbed6hzO6ZjB1j6Coq6ujb8PMc3LZMFhBG0dKYi0tLQgLC8OYMWPsvt7caFsbbCYRE0o4nUrNsSe0bi9VzDw9DDCVd7o7VKFQKODh4cG6VNURAj4PmaMC4KfpwP+kj8HWsh7sKpYi1M8T7T22sd4R/sNw6JlctHVrkff/foPfMCF6NIPLnmiQqxmXl8suLd9Q3Iv8J7OREOqLiNJf0aJy3AR02+kOLBkXgNrOAdR2qfFImjfKy8vtSnqaS5k6Oq9U7vnevXuxaNGiQb1fa7Zu3YoFCxYwrruRxG2Y4IyuHcyNbm9vLzo6OjB8+HDaYLHNyQVAK365S9BGr9ezauljbrSdxVOVSiXq6+stKoyYhNapx87SwwDTJJ9AIEBlZaXTQgy21XN1dXUOLzSuQuUjx8XF4YFsf+wqlkLI5+PuNJNOwXf/vZS+1qrSIP3NE3hqcjRigr3g5SFwa2dlJv60+XcUPJ2DKF8eWlSOxy6fkYoQv2H4rqEOfB7wSN44DPe1vMMwl/QsKSlBfHw8ANDn0fycUue1vLwc77//Prq7uxEWFoa9e/ciKCgI+fn5NueKjbjNW2+9BaFQiAceeIDxfdxI4jZMcEbXDCZPlxLCSUlJgUKhoIXJ2U5ctbe3QygUIijIsVq/K9TV1WHUqFGDmsxgghACiUSClJQUi+VsMg3sodVqcfr0aWRmZjIabutCDGqdudG2rp7T6/XQarVQKBTo7e0dsk4FYCo3jYqKAp/PR0ywNzbcm4rvymU4cq4Dar0RIiEfWaP80SAfgFRp8kI/+slWwlIk5OOzxWOxcOsZlz+rgqdzMHuT/cR/R+tenB5HZzh09ekw3NcTBWfbkDs60MbgApckPfv6+uDj40PrHzgiKysLWVlZeO+997Bv3z4MDAxApVIxXhydpYpt374d+/fvx/fff2/34nojidswwRldM5iMLlVa6+/vD5VKBbVaDR6Px8pr1ev1qK2tZZ2Ow4a+vj4olUq3xl3b2trg5+fHWkCdDQ0NDYiJiRmS8I55ybNOp8O5c+cQERFBN/90RaeC6daZz+ejpaUFGRkZ0Gg08PDwwJ2pobgzNRQavQHFDUr8WNOFHyVdtMEdJuRDo7fN6VXrjVi49QzSRvjgv62uxYTv2lSExydF45OfbY15hL8It0V7YU+5gvG15hOB/7unHK/cmYgLCjWemBzjcJ9SqdSlCbRdu3bh4YcfBo/Hg7e3N+tWPuYcPHgQ77zzDk6cOGH39X19fbRw1I0gbsME167HDPOWPTKZDN3d3ejo6EBOTg6EQlPTyLq6OhgMBosfNxUjs/5xd3V1wcvLC+Hh4TbrB5NhQAhBSUkJ4uLinLZnd+U9//7778jMzHTbZJdarUZZWRkmTJjgtioiuVyOlpYWxpxOe1jrVFgb6a6uLhiNRohEIqeKcB1qHkradDgj06CiTQ0Gu3vZ+GCqNwpl3iiodFxIYc7JlyZBLGK+QzEYDCguLkZubi6r86PVajF58mSUlJSwnsegFMIMBgOWLl2KlStXIj4+HhqNBsHBwTAajejr6wOPx4NYLIZYLMbx48dRV1eHefPmATA5LYsWLcKqVatYv+9rCK5dj6tQzSmTkpLoHFtfX1+MGzfOxsulcnTNf9y9vb0YGBhAYGAgOjs7bX709nJ5HU1WqVQqeHh4uK18GDDdXoeHh7s9uyA2NtZtBpcqrmDbnZbCUfWc0WiETCZDdna2XUNinns9SqdDRqweD+r1UPVrcLq5F28ebx/U+2EiKkCEZiXz5NozP/QD6LdZPislFA/lRtEhjcgAEaRKNSL8RXYNLmC6swkLC2N9fg4dOoSZM2eyNrgGgwFPP/00jhw5gqioKGRnZ2POnDmoqbmkGvfhhx+ivLwcmzdvxp49e/D1118DAGJjY1FWdm01IHU3nNE1w1x/obu7G4QQhISEOBW0oTooULfShBA0NDQgIyPDaWkllctrzyOjcnm1Wi06Ozvh4+NjIfwxlAwDrVZLGx53QV1s3JljKZfLIRKJ3Br+aGlpQWhoqENDYn1eKcIBjIkF1EI/rD9ai1fvTES3Wod/Hqsf9PE0K9VYMMYDX0t00LL0okeJ+eDpTRkN6+YkItzfC4/sKMML0+znLAOm0EJaWhrrY9u5cyfefvtt1uOLi4sRHx9PK4Tdf//9yM/Pt5gzyM/Px2uvvQYA+POf/4xly5axFpu63uGMrhWUEaREaQDQWQxsJ2mkUinEYjGrWnZzo+mo7Laurg5isRjR0dE2x2sd16T+a7VahxkGarUaQqEQZWVlrIy2eSzUHrW1tYiLi3O7l+vOaiSj0Yjm5uYhx9rvTgtDbWcf/pQRjmECIF3YCt9RKViwxfXJNAD4d5UO3p4CaLXsKuUCBFpUN7YAAFRtzThcooUnH/BR1uHkyXqbbs8eHh70Bb67uxv9/f0259r6vLW3t6Orq8ulz5+NQpj5GKFQCH9/f3R1dd2wBRHmcEbXDOoLV11djaioKCgUCpe7+mq1Wly4cMGt3uPAwADa29sZ83xdSQszp7e3F+fOnbObXWAtqm6+3jruSf1oqQISlUpF/6CZjLYrBrmrq2vQEzf2aG9vR1BQ0JCzP0L8huGtOaaQBxWmGR3pjw33puIv/zmL6UnDUdbc7ZK+Qz9LgwsAPoEh8BUJASgwPj0Fm8vLkJcSitsnpVrcQZmfvwsXLkAsFqO3t5dRk5nCaDRixYoV9Pl97rnnEBAQgAcffJBOM7MHG4WwG1lFzBmc0bVCqVSiv78f8fHxaG9vdzknt7q62q2CNoBJJMedjSvNt0ndQg82pmseGqmoqKBT2fR6Uxt3JqPNNp4tEAhQV1eH+Ph4t5U5E0LQ2NjIqKA2lG1KpVLac74zNRQn6xX48kwL/vVABg5VtqNS1oPKVvfm9Hb0aKC7qI5W3d4L5YAOd17sg2Z+B0VhNBpRU1ODsWPHOv0uGY1GfPfdd5g/fz7Wr18Po9EIpVLJKsTDRiGMGhMVFQW9Xg+VSuXWtMprGc7oWtHW1oaUlBTacABgbezkcjl0Oh1CQ0PddjxyuRyEELc1rgRM3qNAIHBLt2DKaPf29sLX1xejRo1i/VqmeLb5466uLhBC0N7eDqlUalPmPJh4dldXF8Risdt6yAEm2cqAgAALz3nlzHiUXFBh5TeV+OqJbIT4DsPz+ypwqLLDbfvdeKIBT06OBg/Ar3UKiEVCTIqzb7ja29sREhLC6vvM5/PR2NiIUaNG4Q9/+INLx8VGIWzOnDnYvn07Jk6ciH379mHq1Kmcp3uzMmbMGFr4Rq/Xo6SkxGF809wrq6qqQkZGhtu+PK6Ik7OFEIKamhq3b7O2ttalyRnAcTybEAKZTIZx48bZDZsMJp7d09MDkUiEkydP2hRfDDae3dTUZBPz9PIQYMO9qVjw6Sms/Poc1s1Nxg9VnViUHYm/zUzA0fMdeDn/PAZ0l8IJD06Iws7iZpc+w80XOy4fq+rEzJRQeArtG1SpVOpSBsjOnTvxyCOPsB5/4cIFLF68GDKZDAaDARMmTIBYLKYVwlavXo1hw4bh3XffRUxMDBobG7F3717Ex8djz549rPdzvcPl6VqhVqtBCKHjWzwez+4P2/yxUqmkW64z3T6zMdzW2gVNTU3Q6XSIcyQw6yJSqRT9/f1uLa6QyWRQKpVISkpy2zY7OjrQ0dFhUyU3FKxzfZn6zTE9dhTPBkzfGSoTwvpcF1SpsO5oI2KCvNAoH0DBshxEB5ni04QQ/FDdiWf+XUFv865bwnCgog2D4dMHM3BrLLOn29/fj3PnzrGePNRoNLj99ttRUlLCegK5tbUVra2tyMzMRE9PD7KysvDNN99YnMPjx49j/fr12L9/P6ttbt68GZs3bwZgkqGMiYlBdHQ0Tp06BR6PhyVLluD5559nta0rDJen6wrm7XeoNjKObkcHBgZQVlaG3NxcC0/IXNqR6QdtLjhjnVlACEF/fz8CAgJQXl7u0FizzSzQ6/VoampyqtngCkajEQ0NDW6VbiSEoL6+3mXP2RkNDQ17WmlOAAAVP0lEQVRITEyknw+2CwaFwWBAeXk5Ro4cCS8vL4vzSMWzM/11mBjpid+kAxgbIkBrdTlaL75eIBAg2MMDe+YNx/qiHpxq0bA2uOOixChp7rZYlhMTaGe0KUXOlQq0goICzJ4926Wy6hEjRtDi5H5+fkhOToZUKh3ShfPJJ5/Ek08+CZ1Oh6lTp2LKlCn4+eefUVFhulApldde/ztncEbXCuqW1ZkBMx9//vx5jBkzxmY802QGWyorKzF69GgEBQUxel8DAwPo6emxWcekJkUZ5J6eHnh7e6Ojo4PR6x7MJNXlkG7s6OiAn5+f29oPASYvSSAQuEWdjEKr1UKv19OGxh7vx+nx7pEaLM4ZiYSLurbW8ex/RulRWNmJN7+3DC8ME/CgMdjecEZ49KOcBwR78dHeb8TsBB80NtQzXogFAgHa29sxevRo1u9t165d+Mc//sF6vDUNDQ0oKSlBTk6OzbrffvsNGRkZiIiIwPr161mloy1fvhxTp07Fs88+i88//xzPPPMM7rrrLgu5yOsFzuhasWPHDlRUVCAwMBABAQHw9/dHYGAg/P39ERAQgICAAAQGBtJG5vDhw/D29rbpZjoUVCoVBgYG6KqhwRg0czUpSj2qs7MTERERFkUX1h44BVPXX6Yii6amJowdO9amtdFgoQpL3BlzBi5pQbiTpqYmVhOHfiIh/u9uy9ALUzx70R8CEeYN/N8PMrT36fH4pGjs/28bWlSWlWrBPh7ISIzGgfoatPebwh13p4+Ar+8wWjDfPJ5NPf79998t9s9koE+cOIHu7m5IpVIolUpUVVUhPDzcpbLz3t5e3HvvvXj//fdtqiczMzPR2NgIX19fFBQUYO7cuZBIJA63t23bNjQ2NmLjxo3g8/koKyvDoUOHsGnTJuzdu9emqeW1DhfTtaK6uhr19fVQKpVQKpVQKBRQKBRQqVSQy+VQqVRQKpXQarV0EUVSUhJ8fX1po2xuoJmMtp+fn10JQ4PBgJ9//hlZWVlu9coqKioQFhbGSlUKuOTxW4c/zA20XC6H0WiEp6enw/xdtpNVPB6PTsZ3teTXEb29vaiurnZrCESn0+HUqVOs9QvYQAhBUVERElIz0KMliAn2ht5oxKHKDmz9tcmujOQI/2E48uxERqFyACgpKUFCQoLF94kpnq3X67F//34cO3YMarUa0dHRUCqVyMvLw5IlS1i9B51Ohz/+8Y+YOXMmXnjhBafjY2JicOrUKbtFEadPn8ZDDz2En376iS6p9/T0hFgsRmlpKR5++GGUlpayOrYrDBfTZUtiYqJF3M8Rq1evRlRUFB544AHaOFOGWS6X054CZbxVKhUtS0hd7Ph8Pq1iFhAQQBv06dOn00ba3HgHBATQ8WW2P/bu7m5oNBqXqn0cCagDph9XR0cHJkyYwBj3c1TazCTpSOXv9vX1wdvbG6dOnWKdUWA9AWnN5fByKZUud6Y5yeVyiMViBPt5gUoQFPL5uOuWMMxODcXOYinWHbL1Cmenhtk1uGq1Gnq93uYCbi+evWTJEmzbtg0//PCDy6JKhBA8+uijSE5OtmtwZTIZfQdXXFwMo9HoMB1y48aNkMvluOOOOwAAgYGBUKlU9AV+3bp1Lh3jtQDn6Q6B3t5eeHt7D+q2mvrcDQYDbaSbm5vx6KOP4pVXXoFOp3PoZVPbEIlEFkZZLBZbeNZisRgbN27E6tWrERMT49DLdgWJRAIfHx+3tsVua2uDQqHAmDFjbCqlnGWP2NPhBUyTLSNHjhz0BKQ1RqMRRUVFDsVyBgMlKu6sfPzTXxrx9+/r6Of7HhuPlBHMr6mrq4NIJGJ9nk6dOoXNmzcPqvPuzz//jMmTJyMtLY3+PNeuXYumpiYApkmxjRs34qOPPoJQKISXlxf+/ve/49Zbb3V5X9cBnKd7ORjK7T9l9IRCIYKDgxEcHIzhw4dj+/btmDx5stPXU0a7v78fSqUScrkcCoWCfkx52WfOnEFTUxPeeecd2sumvATKy2YKiTjysuvr63Hs2DE89thjg37/TO+HEgly5mU7w/zWuaamBpGRkXR2wWAmIK3/d3d3QywW02mFQ62SA0wZMAaDgZVex9I/RKOzV4vPi5qRGOqD5HDm7yEhBG1tbS61idq5cyfrUAIFdTEXCATIysrCqVOnbI5j+fLliI+Ph7e3N3bs2OHWUM/1Bufp3uC89tprePzxxy08HSYv2zx+7SyWLZfL6Rl7R162tdF25GVXVVUBgFtb8Wg0GpSWlrqk62tPs8D8f3NzMwIDA2nj7uoEJFNoRCKRwM/PD+Hh4ayOU6s3YtW35zEvIxy32qlC6+rqQnt7O+v4uFqtxpQpU1zKzQWcx2ULCgrwwQcfoKCgAEVFRVi+fLmNAM4NCOfp3qxQ8nnmMHnZbGlpacHChQtx7NgxDAwM2HjZlLGurq6ml1F/PT09jF62n58ffvrpJ9x///0IDQ21mXwcbCy7sbER0dHRLnmhztL85HI5BgYG7IqpW/eVM/9PtShiys3u7++Hj48PmpqaWBvrtXcnOAxvSKVSl2LZ+/fvx5w5c1xueeSM/Px8LF68GDweD7m5uVAqlWhtbXWaanejwhldDpeIiIhAYWEh+Hw+fHx84OPj41LSPZOXvXfvXvT19WHcuHFQKpXo6OhAdXW1jZet0VzqhisSieyGRPz9/SEQCGgdWIVCAbFY7JZYdmNjo0OVrcH0lZNKpVCr1YiLi3N5AtJcGQy4VAEpEAigUCjg6+sLpVJp14Cbfya7du3Cpk2bXP5MnHXvZZJ6lEqlnNHl4GDLUGQWmbzsWbNmYenSpQgLC3P6ekexbMrLlkgkOHjwIEQiEVatWmXXy6YMtXl4hPKsmbzss2fPoqKiwqJj8lChFMoyMjIA2BdOZ7styhg3NTUhLCwMvr6+jLm75l72Z599hpMnT0KhUGDZsmUICAjA+PHj8eKLL7Lar7PuvTezjCMTnNHluOq4IiZO/VidedlKpRLvvPOOhfGy9rIpI20ey3bkZSsUCoSHh+Ozzz5z6GVbx7UdedlNTU3w9PR0i+qZedaGQqFAVlYWK487NzcX69evR1hYGO655x4oFMxNMO3hrHsvG6nHmwluIo2DgwXd3d2YPn06Tp486TCWTT12FsumjPLZs2eRk5OD2NhYm8nHoKAg+Pv70xVrbL1DhUIBqVTKuoknIQSTJk3Cjz/+yCp7whzr7r15eXlYvXo1Zs2aRY85cOAANm7cSE+kPfvssyguLnZpP9ch3EQaB8dQoLrVuiuWrVAoUFNTg+eeew6TJ0+mO09LJBLaoFvHstnkZQcEBGDHjh1YvHgx61h2UVER0tLSXDK4VVVVWLBgAbRaLerr66HVahESEoJnnnkGs2bNwubNm1FdXY0tW7Zg9OjRaG5uRnBwMCIjI/HZZ5+x3s+NCOfpcnBcJdra2lBXV4eJEyc6Hcsmlq1UKiGTyfDVV18hNzcXKpXKwsum2p1bh0SOHDmCN954A3l5eYN6HwaDAZGRkSgqKrLo4eeqjOMNBufpcnBca4SFhbGaPATYx7J/++03jB8/HkuXLqWX2YtlU8a7s7MTU6ZMGfT7+P777xEXF2fTNJWDGc7T5eDgGBJLlixBZmYmli1bZrH8+PHjuPfeexEVFeWSjOMNgl1PlzO6HBwcg0ar1SIiIgJnz5618dq7u7vB5/NpGcfly5c7lXG8gbBrdN3XXvYGYcmSJQgNDbWY+ZXL5cjLy0NCQgLy8vLsptRs374dCQkJSEhIwPbt26/UIXNwXDUKCwuRmZnJGCYRi8W0Psns2bOh0+nQ2dl5pQ/xmoMzulY8/PDDOHjwoMWyt99+G9OmTYNEIsG0adPw9ttv27xOLpfj9ddfR1FREYqLi/H666/bGOeqqiqMHTuW/hOLxXj//fctxhw/fhz+/v70mDfeeMP9b5KDw0188cUXWLhwIeM6mUxGx5PZyDjeNBBCHP3dlNTX15PU1FT6eWJiImlpaSGEENLS0kISExNtXrN7927y+OOP088ff/xxsnv3brv70Ov1JCwsjDQ0NFgsP3bsGLnrrruG+hY4OC47fX19JCgoiCiVSnrZRx99RD766CNCCCEffPABSUlJIenp6SQnJ4f88ssvV+tQrwZ27Srn6bKgra2NrhMfMWIE2tvbbcbYqy+3h7tmfGNiYpCWloaxY8cyNpwkhODZZ59FfHw80tPTcebMmSHtj+PmwlG4LSMjA5mZmRYdQ6hGkoCpOaVWq0V/fz+eeuqpG1U312U4o+smiIv15Xv27LF7W0Y17rvzzjtx9uxZp/s+duwYSktLbXRMAVPMTSKRQCKR4JNPPsFTTz3lcFsXLlzAHXfcgeTkZKSmpuKf//ynzRguBHLzcDnDbTcrnNFlQVhYGFpbTY2zW1tbERoaajPGlfpyrVaLb7/9FvPnz7dZRzXuKysrwzPPPIO5c+cO6djtyerZQygUYsOGDTh37hxOnjyJTZs2obKy0mbc5MmTUVpaitLSUqxevXpIx8hx7XLbbbchKMhSrzc/Px8PPfQQAOChhx7CN998Y/O6Q4cOIS8vD0FBQQgMDEReXp6N8b5Z4YwuC+bMmUNnI2zfvh1/+tOfbMbMnDkThw8fphPPDx8+jJkzZzJuz50zvpSsXlZWFj755BOb9a6GPUaMGEGr+vv5+SE5OdnheDYcPHgQY8aMQXx8PKNXpNFosGDBAsTHxyMnJwcNDQ1D2h/H5eVyhNtuJjija8XChQsxceJEVFVVISoqClu2bMHKlStx5MgRJCQk4MiRI1i5ciUAUz8pqvInKCgIr776KrKzs5GdnY3Vq1fbeAgU7pzx/eWXX3DmzBkUFhZi06ZN+PHHHy3Wuxr2MKehoQElJSXIycmxWcc2BGIwGPD000+jsLAQlZWV+OKLL2w85y1btiAwMBA1NTV4/vnn8de//pXV8QHAihUrkJSUhPT0dMybNw9KpZJxnLPYN4d7Gcr37obH0SzblZ7uuxm4nDO+a9asIe+9957FMussCvNMDEf09PSQzMxM8p///MdmnUqlIj09PYQQQg4cOEDi4+PtbufXX38lM2bMoJ+vXbuWrF271mLMjBkzyK+//koIIUSn05Hg4GBiNBqdHiMhhBw6dIjodDpCCCEvvfQSeemllxjHRUdHk46ODlbbXLNmDYmIiCAZGRkkIyODHDhwgHFcYWEhSUxMJHFxcWTdunWstn09ciWyeW5A7NpVzuhex/T29pLu7m768cSJE0lhYaHFmP3795NZs2YRo9FIfvvtN5Kdne10u1qtlsyYMYNs2LCB1XE4MmhffvklefTRR+nnn3/+OXn66actxqSmppILFy7Qz2NjY1kbSHO++uorsmjRIpeP0Rqmi5c1er2exMbGktraWqLRaEh6ejo5e/asy8d8PWBtdF988UX6IrNu3TqyYsUKm9d0dXWRmJgYIpfLiVwuJzExMaSrq+uKHfM1gF27ygneXMe0tbVh3rx5AAC9Xo9FixbRsnqAKX1n9uzZKCgooDuxOpPVI4Tg0UcfRXJyMl544QXGMTKZDGFhYeDxeE5DIITFbSabMWzYunUrFixYwLjOWUsZVykuLkZ8fDxiY2MBAPfffz/y8/ORkpLCOH7BggV0802lUomAgACUlpbajDPvrCsUChkzUq4kCxcuxPHjx9HZ2YmoqCi8/vrrWLlyJe677z5s2bIFo0aNwpdffgngUvv2Tz/91CLcBsBhuO2mw5FFvvIXB46rzU8//UQAkLS0NIvb68GGQNwRXpg2bRpJTU21+fvmm2/oMW+++SaZO3eu3bCEVColhBDS1tZG0tPTyYkTJ+we85o1a0h0dDRJS0sjjzzyCJHL5TZj2Hjw9njhhRfI66+/zrjOmUe+d+9ekpKSQng8Hvn9998t1q1du5bExcWRxMREcvDgQcbX19XVkQkTJpD4+Hhy3333EY1Gw+qYOVyGCy9wXB10Oh0ZPXo0qauro2/DKyoqLMZs3LiRPPHEE4QQQr744gsyf/58l/axbds2kpubS/r6+liNX7NmDYmPj7dryGUyGdHr9cRgMJCXX36ZPPLIIzbb2Lt3r43RXbZsmdN9G41GEhUVRaqrqxnXOzO6lZWV5Pz58+T222+3MLpnz54l6enpRK1Wk7q6OhIbG0v0er3N6+fPn0+++OILQgghTzzxBPnwww+dHjPHoOCMLsfV48CBAyQhIYHExsaSN998kxBCyKuvvkry8/MJIYQMDAyQP//5zyQuLo5kZ2eT2tpa1tsuLCwkycnJpL293e4YNrFve1jHMynYePBMnDhxgmRlZdldHxMTQ8aNG0cyMzPJxx9/bHectdG13r/53QOF0WgkwcHB9MSj9XvgcCtcTJfj6jF79mzMnj3bYpl5FZtIJKLjgq6ybNkyaDQauutBbm4uNm/ejJaWFixduhQFBQV2Y9/2aG1tpfNQv/76a8ZeY9nZ2ZBIJKivr0dkZCT27NkDLy8v7Nq1y2bsW2+9Red2O0oXBJx31rWHVCpFbm4u/ZwpL7arqwsBAQEQCoV2x3Bcfjijy3FdU1NTw7g8IiICBQUFAIDY2FiUlZWx3uZLL72E0tJS8Hg8xMTE4OOPPwYAC0MuFAqxceNGzJw5EwaDAUuWLMGqVascblev1+Orr77C6dOn7Y6JiIjA9OnTIZPJ0N7ejvvuuw/Dhw8HYGm8rSFXcMKSY2hwRpeDw4odO3YwLjc35ACzB++Io0ePIikpCVFRUYzrqc66R48etdtZ1x5sytCHDx8OpVIJvV4PoVB407dCv1pwFWkcHFcIJpGjlpYW2nC3tbVh0qRJyMjIwIQJE3DXXXexMriAqVR9z5490Gg0qK+vh0QiwYQJEyzG8Hg83HHHHdi3bx8A+yXtHJcXZ+16ODg4riF4PN48AB8ACAGgBFBKCJl5cd0qAEsA6AE8RwgpvLi8AMBSQkgLj8eLBbAHQBCAEgAPEkI0V/6d3LxwRpfjssHj8bIBbAEwAYAAQDGAJwG8BUAMU3jrKULIT1ftIDk4rjCc0eW4rPB4vDcBiAB4AWgGoAUgIoS8xePxBAC8CSE9V/MYOTiuJJzR5bis8Hg8TwC/A1ADuBXAHwBsBbATwDeEENtaWA6OGxhuIo3jchMEwBeAH0we7o8AbgMgBbCDx+MtvpoHx8FxpeE8XY7LCo/H+xamiZvRAEYAeA+AlBCi5/F4zwGIIYQ8dzWPkYPjSsLl6XJcNi56sXpCyO6L8dtfAUwBsILH4+kA9ALgPF2Om4r/D/mPH017fl73AAAAAElFTkSuQmCC# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/7:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
22/8:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions.сх of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
22/9:
# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case.
print (targets.shape)
22/10:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/11:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/12:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(112, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/13:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/14:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
print(targets)
22/15:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/16:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='2d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/17:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

#data:image/png;base64,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# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
22/18:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
22/19:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=(1,1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
22/20:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=(1,1))

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
22/21:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=(1,1))

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
22/22:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
23/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/3:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/4:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/5:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/6:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/7:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/8:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
    loss.shape
23/9:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
    print(loss.shape)
23/10:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    #print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
    print(loss.shape)
23/11:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
    print(i)
23/12:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/13:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

loss.shape
23/14:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

loss
23/15:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/16:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/17:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/18:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/19:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
23/20:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/21:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/22:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/23:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/24:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/25:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/26:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/27:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/28:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/29:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/30:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/31:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
23/32:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/33:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/34:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/35:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array[i,1]=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/36:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array(i,1)=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/37:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/38:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array(i,1)=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/39:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np
import pandas as pd
# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
23/40:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/41:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/42:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/43:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/44:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/45:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array(i,1)=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/46:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array(i,1)=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/47:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array [i,1]=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/48:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array[i,1]=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/49:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    loss_array([i,1])=loss
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/50:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss(i) = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/51:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/52:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/53:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/54:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array=arr.append(loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/55:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/56:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/57:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    #print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/58:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    #print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/59:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/60:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/61:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.

for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    #print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/62:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/63:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
23/64:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    #print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   





print (loss_array)
23/65:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/66:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/67:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    #print (loss_array)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   





print (loss_array)
23/68:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    #print (loss_array)
plt.plot(i,loss) 
plt.show()

    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
23/69:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/70:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/71:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    plt.plot(i,loss) 
    plt.show()
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
23/72:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
plt.plot(i,loss) 
plt.show()
23/73:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
plt.plot(i,loss) 
plt.show()
23/74:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/75:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/76:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/77:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/78:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/79:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/80:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/81:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/82:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/83:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/84:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/85:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/86:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/87:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (200):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/88:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/89:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/90:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
23/91:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/92:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.05
23/93:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   

#print (loss_array)
plt.plot(i_array,loss_array) 
plt.show()
23/94:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/95:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.01
23/96:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/97:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.03
23/98:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/99:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/100:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.03
23/101:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/102:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/103:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.03
23/104:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
   # print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/105:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/106:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.03
23/107:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/108:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/109:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.04
23/110:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/111:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/112:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.04
23/113:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/114:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/115:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/116:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 10000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/117:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/118:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/119:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/120:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.04
23/121:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/122:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/123:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/124:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 100000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/125:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/126:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/127:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/128:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
23/129:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/130:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/131:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/132:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/133:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.0001
23/134:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/135:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/136:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
23/137:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
23/138:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
23/139:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/140:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.0001
23/141:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/142:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/143:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/144:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
23/145:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.0001
23/146:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/147:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/148:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/149:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/150:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/151:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/152:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/153:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/154:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/155:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (10):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/156:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/157:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/158:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/159:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/160:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/161:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.01
23/162:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/163:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/164:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
23/165:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
23/166:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
23/167:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
24/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np
import pandas as pd
# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
24/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
24/3:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 2*xs - 3*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
24/4:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
24/5:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
24/6:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.01
24/7:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
24/8:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
24/9:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
24/10:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
targets = 80*xs - 35*zs + 5 + noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
24/11:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
24/12:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
24/13:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.01
24/14:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
24/15:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
24/16:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
24/17:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
24/18:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
24/19:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.01
24/20:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
loss_array=[]
i_array=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas)
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    
    print (loss)
    loss_array.append(loss)
    i_array.append(i)
    #print (loss_array)


    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology. 
   


plt.plot(i_array,loss_array) 
plt.show()
24/21:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
24/22:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
observations
27/1: import tensorflow as tf
31/1:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
34/1:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
34/2:
import numpy as np
#import matplotlib.pyplot as plt
import tensorflow as tf
34/3:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
34/4:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
34/5:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
35/1:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
35/2:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
35/3:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
35/4:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
35/5: training_data = np.load('TF_intro.npz')
35/6:
input_size = 2
output_size = 1

model = tf.keras.Sequential([
                            tf.keras.layers.Dense(output_size)
                            ])

model.compile(optimizer='sgd', loss='mean_squared_error')

model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=0)
38/1:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
38/2:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
38/3:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
tf.__version__
38/4:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)
38/5:
import numpy as np
#import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)
38/6:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)
38/7:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)
38/8:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
38/9:
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)
38/10:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
38/11:
observations = 1000

xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

generated_inputs = np.column_stack((xs,zs))

noise = np.random.uniform(-1, 1, (observations,1))

generated_targets = 2*xs - 3*zs + 5 + noise

np.savez('TF_intro_lalala', inputs=generated_inputs, targets=generated_targets)
38/12: training_data = np.load('TF_intro_lalala.npz')
38/13:
training_data = np.load('TF_intro_lalala.npz')
training_data
38/14:
training_data = np.load('TF_intro_lalala.npz')
print(training_data)
38/15:
input_size = 2
output_size = 1

model = tf.keras.Sequential([
                            tf.keras.layers.Dense(output_size)
                            ])

model.compile(optimizer='sgd', loss='mean_squared_error')

model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=0)
40/1:
# We must always import the relevant libraries for our problem at hand. NumPy and TensorFlow are required for this example.
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
40/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
generated_inputs = np.column_stack((xs,zs))

# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
generated_targets = 2*xs - 3*zs + 5 + noise

# save into an npz file called "TF_intro"
np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
40/3:
# Load the training data from the NPZ
training_data = np.load('TF_intro.npz')
41/1:
# We must always import the relevant libraries for our problem at hand. NumPy and TensorFlow are required for this example.
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
41/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
generated_inputs = np.column_stack((xs,zs))

# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
generated_targets = 2*xs - 3*zs + 5 + noise

# save into an npz file called "TF_intro"
np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
41/3:
# Load the training data from the NPZ
training_data = np.load('TF_intro.npz')
41/4:
# Declare a variable where we will store the input size of our model
# It should be equal to the number of variables you have
input_size = 2
# Declare the output size of the model
# It should be equal to the number of outputs you've got (for regressions that's usually 1)
output_size = 1

# Outline the model
# We lay out the model in 'Sequential'
# Note that there are no calculations involved - we are just describing our network
model = tf.keras.Sequential([
                            # Each 'layer' is listed here
                            # The method 'Dense' indicates, our mathematical operation to be (xw + b)
                            tf.keras.layers.Dense(output_size,
                                                 # there are extra arguments you can include to customize your model
                                                 # in our case we are just trying to create a solution that is 
                                                 # as close as possible to our NumPy model
                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),
                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)
                                                 )
                            ])

# We can also define a custom optimizer, where we can specify the learning rate
custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
# Note that sometimes you may also need a custom loss function 
# That's much harder to implement and won't be covered in this course though

# 'compile' is the place where you select and indicate the optimizers and the loss
model.compile(optimizer=custom_optimizer, loss='mean_squared_error')

# finally we fit the model, indicating the inputs and targets
# if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), 
# so the number of epochs is 'kind of' mandatory, too
# we can play around with verbose; we prefer verbose=2
model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=2)
41/5:
# Extracting the weights and biases is achieved quite easily
model.layers[0].get_weights()
41/6:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
weights = model.layers[0].get_weights()[0]
weights
41/7:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
bias = model.layers[0].get_weights()[1]
bias
41/8:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
model.predict_on_batch(training_data['inputs']).round(1)
41/9:
# We must always import the relevant libraries for our problem at hand. NumPy and TensorFlow are required for this example.
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
41/10:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
generated_inputs = np.column_stack((xs,zs))

# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
generated_targets = 2*xs - 3*zs + 5 + noise

# save into an npz file called "TF_intro"
np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
41/11:
# Load the training data from the NPZ
training_data = np.load('TF_intro.npz')
41/12:
# Declare a variable where we will store the input size of our model
# It should be equal to the number of variables you have
input_size = 2
# Declare the output size of the model
# It should be equal to the number of outputs you've got (for regressions that's usually 1)
output_size = 1

# Outline the model
# We lay out the model in 'Sequential'
# Note that there are no calculations involved - we are just describing our network
model = tf.keras.Sequential([
                            # Each 'layer' is listed here
                            # The method 'Dense' indicates, our mathematical operation to be (xw + b)
                            tf.keras.layers.Dense(output_size,
                                                 # there are extra arguments you can include to customize your model
                                                 # in our case we are just trying to create a solution that is 
                                                 # as close as possible to our NumPy model
                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),
                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)
                                                 )
                            ])

# We can also define a custom optimizer, where we can specify the learning rate
custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
# Note that sometimes you may also need a custom loss function 
# That's much harder to implement and won't be covered in this course though

# 'compile' is the place where you select and indicate the optimizers and the loss
model.compile(optimizer=custom_optimizer, loss='mean_squared_error')

# finally we fit the model, indicating the inputs and targets
# if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), 
# so the number of epochs is 'kind of' mandatory, too
# we can play around with verbose; we prefer verbose=2
model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=2)
41/13:
# Extracting the weights and biases is achieved quite easily
model.layers[0].get_weights()
41/14:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
weights = model.layers[0].get_weights()[0]
weights
41/15:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
bias = model.layers[0].get_weights()[1]
bias
41/16:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
model.predict_on_batch(training_data['inputs']).round(1)
41/17:
# If we display our targets (actual observed values), we can manually compare the outputs and the targets
training_data['targets'].round(1)
41/18:
# The model is optimized, so the outputs are calculated based on the last form of the model

# We have to np.squeeze the arrays in order to fit them to what the plot function expects.
# Doesn't change anything as we cut dimensions of size 1 - just a technicality.
plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()

# Voila - what you see should be exactly the same as in the previous notebook!
# You probably don't see the point of TensorFlow now - it took us the same number of lines of code
# to achieve this simple result. However, once we go deeper in the next chapter,
# TensorFlow will save us hundreds of lines of code.
41/19:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
model.predict_on_batch(training_data['inputs'])
41/20:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
41/21:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
model.predict_on_batch(training_data['inputs']).round(1)
training_data['inputs'].round(1)
41/22:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
41/23:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
ff
41/24:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
ff.round(1)
41/25:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
ff.round
41/26:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
ff.round(1)
41/27:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
x=round(ff,2)
41/28:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
x=round(ff)
41/29:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
training_data['inputs'].round(1)
41/30:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
ff
41/31:
# If we display our targets (actual observed values), we can manually compare the outputs and the targets
training_data['targets'].round(1)
41/32:
# The model is optimized, so the outputs are calculated based on the last form of the model

# We have to np.squeeze the arrays in order to fit them to what the plot function expects.
# Doesn't change anything as we cut dimensions of size 1 - just a technicality.
plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()

# Voila - what you see should be exactly the same as in the previous notebook!
# You probably don't see the point of TensorFlow now - it took us the same number of lines of code
# to achieve this simple result. However, once we go deeper in the next chapter,
# TensorFlow will save us hundreds of lines of code.
41/33:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 100000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations x 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two matrices (vectors) into one.
generated_inputs = np.column_stack((xs,zs))

# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
generated_targets = 2*xs - 3*zs + 5 + noise

# save into an npz file called "TF_intro"
np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
41/34:
# Load the training data from the NPZ
training_data = np.load('TF_intro.npz')
41/35:
# Declare a variable where we will store the input size of our model
# It should be equal to the number of variables you have
input_size = 2
# Declare the output size of the model
# It should be equal to the number of outputs you've got (for regressions that's usually 1)
output_size = 1

# Outline the model
# We lay out the model in 'Sequential'
# Note that there are no calculations involved - we are just describing our network
model = tf.keras.Sequential([
                            # Each 'layer' is listed here
                            # The method 'Dense' indicates, our mathematical operation to be (xw + b)
                            tf.keras.layers.Dense(output_size,
                                                 # there are extra arguments you can include to customize your model
                                                 # in our case we are just trying to create a solution that is 
                                                 # as close as possible to our NumPy model
                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),
                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)
                                                 )
                            ])

# We can also define a custom optimizer, where we can specify the learning rate
custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
# Note that sometimes you may also need a custom loss function 
# That's much harder to implement and won't be covered in this course though

# 'compile' is the place where you select and indicate the optimizers and the loss
model.compile(optimizer=custom_optimizer, loss='mean_squared_error')

# finally we fit the model, indicating the inputs and targets
# if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), 
# so the number of epochs is 'kind of' mandatory, too
# we can play around with verbose; we prefer verbose=2
model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=2)
41/36:
# Extracting the weights and biases is achieved quite easily
model.layers[0].get_weights()
41/37:
# Extracting the weights and biases is achieved quite easily
model.layers[0].get_weights()
41/38:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
weights = model.layers[0].get_weights()[0]
weights
41/39:
# We can save the weights and biases in separate variables for easier examination
# Note that there can be hundreds or thousands of them!
bias = model.layers[0].get_weights()[1]
bias
41/40:
# We can predict new values in order to actually make use of the model
# Sometimes it is useful to round the values to be able to read the output
# Usually we use this method on NEW DATA, rather than our original training data
ff=model.predict_on_batch(training_data['inputs'])
ff
41/41:
# If we display our targets (actual observed values), we can manually compare the outputs and the targets
training_data['targets'].round(1)
41/42:
# The model is optimized, so the outputs are calculated based on the last form of the model

# We have to np.squeeze the arrays in order to fit them to what the plot function expects.
# Doesn't change anything as we cut dimensions of size 1 - just a technicality.
plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()

# Voila - what you see should be exactly the same as in the previous notebook!
# You probably don't see the point of TensorFlow now - it took us the same number of lines of code
# to achieve this simple result. However, once we go deeper in the next chapter,
# TensorFlow will save us hundreds of lines of code.
42/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
42/2:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
43/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
44/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
44/2:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
44/3:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated
44/4:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated
44/5:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
45/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
45/2: mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
45/3:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
###########mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)
45/4:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
###########mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)
45/5: test_data
45/6: scaled_train_and_validation_data
45/7:
BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
47/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
47/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
47/3:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
47/4:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
47/5:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
validation_inputs
47/6:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
47/7:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 50
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
47/8:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
47/9:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)
47/10:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
47/11:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 50
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
47/12:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
47/13:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)
47/14:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
47/15:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 50
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
47/16:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
47/17:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)
47/18:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
47/19:
# determine the maximum number of epochs
NUM_EPOCHS = 6

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
47/20:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
47/21:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
47/22:
# determine the maximum number of epochs
NUM_EPOCHS = 6

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
47/23: test_loss, test_accuracy = model.evaluate(test_data)
47/24:
# We can apply some nice formatting if we want to
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
47/25:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
47/26:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
47/27:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
48/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
48/3:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
48/4:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
48/5:
# determine the maximum number of epochs
NUM_EPOCHS = 6

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
48/6: test_loss, test_accuracy = model.evaluate(test_data)
48/7:
# We can apply some nice formatting if we want to
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
48/8:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/9:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="5.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/10:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="4.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/11:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/12:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/13:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/14: im
48/15:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/16: im
48/17: img
48/18:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/19: im
48/20: img
48/21:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
48/22: im
49/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
49/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
49/3:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
49/4:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
49/5:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
49/6: test_loss, test_accuracy = model.evaluate(test_data)
49/7:
# We can apply some nice formatting if we want to
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
49/8:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/9: im
49/10:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/11: im
49/12:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/13: im
49/14:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="7.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/15: im
49/16:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="5.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/17: im
49/18:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="4.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/19: im
49/20:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="7.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/21: im
49/22:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="22.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/23: im
49/24:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="9.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/25: im
49/26:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[0] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/27:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/28: im
49/29:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="1.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/30: im
49/31:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[0] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/32:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[4] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/33:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[54] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/34:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[6518] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/35:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[111] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/36:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
49/37: im
49/38:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[74448] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/39:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[7448] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/40:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[748] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
49/41:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[7] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
52/1:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
52/2:
# Load the data from a .csv in the same folder
raw_data = pd.read_csv('Audiobooks_data.csv')

# Let's explore the top 5 rows of the df
raw_data.head()
52/3:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()
52/4:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
52/5:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')

# Let's explore the top 5 rows of the df
raw_data.head()
52/6:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
52/7:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:1:-1]
52/8:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:1:-1]
targets_data=raw_data[:-1]
52/9:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:1:-1]
targets_data=raw_data[:,-1]
52/10:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
52/11:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
52/12:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
raw_data
52/13:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
argets_data
52/14:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
targets_data
52/15:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['year'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
52/16:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Year'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
52/17:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Year'],axis=2)

# Let's check the descriptives without 'Model'
data.describe(include='all')
52/18:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Brand'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
52/19:
# For these several lessons, we will create the regression without 'Model'
# Certainly, when you work on the problem on your own, you could create a regression with 'Model'
data = raw_data.drop(['Model'],axis=1)

# Let's check the descriptives without 'Model'
data.describe(include='all')
52/20:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
52/21:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
targets_data
52/22:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
52/23:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
52/24:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
52/25:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
targets_data
52/26:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
52/27:
# For this practical example we will need the following libraries and modules
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
52/28:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
targets_data
52/29:
# Load the data from a .csv in the same folder
raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all=raw_data[:,1:-1]
targets_data=raw_data[:,-1]
52/30:
# Descriptive statistics are very useful for initial exploration of the variables
# By default, only descriptives for the numerical variables are shown
# To include the categorical ones, you should specify this with an argument
raw_data.describe(include='all')

# Note that categorical variables don't have some types of numerical descriptives
# and numerical variables don't have some types of categorical descriptives
50/1:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)

unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]
50/2:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
50/3:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)

unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]
50/4:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)

unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]
50/5:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]
50/6:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

raw_csv_data
50/7:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
num_one_targets
50/8:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.count()
50/9:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.count(0)
50/10:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.count
50/11:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all
50/12:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
50/13:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
num_one_targets
indices_to_remove
50/14:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
num_one_targets
50/15:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
num_one_targets
unscaled_inputs_equal_priors
50/16:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
50/17:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
50/18:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data.head()
50/19:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
50/20:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
unscaled_inputs_all.head()
50/21:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
data_la=pd.unscaled_inputs_all
50/22:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
unscaled_inputs_all
50/23:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
lolo=pd(unscaled_inputs_all)
50/24:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing
import pandas as pd

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')
data_la=pd.read_csv('Audiobooks_data.csv')
# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)
raw_csv_data
unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]

targets_all.shape
data_la.head()
50/25:
# Count how many targets are 1 (meaning that the customer did convert)
num_one_targets = int(np.sum(targets_all))

# Set a counter for targets that are 0 (meaning that the customer did not convert)
zero_targets_counter = 0

# We want to create a "balanced" dataset, so we will have to remove some input/target pairs.
# Declare a variable that will do that:
indices_to_remove = []

# Count the number of targets that are 0. 
# Once there are as many 0s as 1s, mark entries where the target is 0.
for i in range(targets_all.shape[0]):
    if targets_all[i] == 0:
        zero_targets_counter += 1
        if zero_targets_counter > num_one_targets:
            indices_to_remove.append(i)

# Create two new variables, one that will contain the inputs, and one that will contain the targets.
# We delete all indices that we marked "to remove" in the loop above.
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
num_one_targets
50/26:
# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities
# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.
# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. 
# The result will be interesting.
scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)
50/27:
# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities
# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.
# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. 
# The result will be interesting.
scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)
scaled_inputs
50/28:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
50/29:
# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities
# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.
# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. 
# The result will be interesting.
scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)
scaled_inputs.shape()
50/30:
# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities
# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.
# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. 
# The result will be interesting.
scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)
scaled_inputs.shape
50/31:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
shuffled_inputs.shape
50/32:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
shuffled_inputs.shape*0.8
50/33:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape*0.8
50/34:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
50/35:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
ff=dd*0.8
50/36:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
ff=int(dd)*0.8
50/37:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
ff=int(dd*0.8)
50/38:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
ff
50/39:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape
dd
50/40:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
50/41:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
dd
50/42:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
50/43:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
ff
50/44:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
int(ff)
50/45:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
int(ff)-dd
50/46:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
(int(ff)-dd)/2
50/47:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
int((int(ff)-dd)/2)
50/48:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
int((int(ff)-dd)/2)
shuffled_inputs
50/49:
# When the data was collected it was actually arranged by date
# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.
# Since we will be batching, we want the data to be as randomly spread out as possible
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

# Use the shuffled indices to shuffle the inputs and targets.
shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
dd=shuffled_inputs.shape[0]
ff=dd*0.8
int((int(ff)-dd)/2)
shuffled_inputs.shape
50/50:
# Count the total number of samples
samples_count = shuffled_inputs.shape[0]

# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.
# Naturally, the numbers are integers.
train_samples_count = int(0.8 * samples_count)
validation_samples_count = int(0.1 * samples_count)

# The 'test' dataset contains all remaining data.
test_samples_count = samples_count - train_samples_count - validation_samples_count

# Create variables that record the inputs and targets for training
# In our shuffled dataset, they are the first "train_samples_count" observations
train_inputs = shuffled_inputs[:train_samples_count]
train_targets = shuffled_targets[:train_samples_count]

# Create variables that record the inputs and targets for validation.
# They are the next "validation_samples_count" observations, folllowing the "train_samples_count" we already assigned
validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]
validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]

# Create variables that record the inputs and targets for test.
# They are everything that is remaining.
test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]
test_targets = shuffled_targets[train_samples_count+validation_samples_count:]

# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were 
# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, 
# you will get different values, as each time they are shuffled randomly.
# Normally you preprocess ONCE, so you need not rerun this code once it is done.
# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.

# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.
print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)
print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)
print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)
50/51:
# Save the three datasets in *.npz.
# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!

np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)
np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)
np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)
53/1:
import numpy as np

# We will use the sklearn preprocessing library, as it will be easier to standardize the data.
from sklearn import preprocessing

# Load the data
raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')

# The inputs are all columns in the csv, except for the first one [:,0]
# (which is just the arbitrary customer IDs that bear no useful information),
# and the last one [:,-1] (which is our targets)

unscaled_inputs_all = raw_csv_data[:,1:-1]

# The targets are in the last column. That's how datasets are conventionally organized.
targets_all = raw_csv_data[:,-1]
54/1:
# we must import the libraries once again since we haven't imported them in this file
import numpy as np
import tensorflow as tf
54/2:
# we must import the libraries once again since we haven't imported them in this file
import numpy as np
import tensorflow as tf
54/3:
# we must import the libraries once again since we haven't imported them in this file
import numpy as np
import tensorflow as tf
54/4:
# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets
npz = np.load('Audiobooks_data_train.npz')

# we extract the inputs using the keyword under which we saved them
# to ensure that they are all floats, let's also take care of that
train_inputs = npz['inputs'].astype(np.float)
# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)
train_targets = npz['targets'].astype(np.int)

# we load the validation data in the temporary variable
npz = np.load('Audiobooks_data_validation.npz')
# we can load the inputs and the targets in the same line
validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)

# we load the test data in the temporary variable
npz = np.load('Audiobooks_data_test.npz')
# we create 2 variables that will contain the test inputs and the test targets
test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)
55/1:
# we must import the libraries once again since we haven't imported them in this file
import numpy as np
import tensorflow as tf
55/2:
# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets
npz = np.load('Audiobooks_data_train.npz')

# we extract the inputs using the keyword under which we saved them
# to ensure that they are all floats, let's also take care of that
train_inputs = npz['inputs'].astype(np.float)
# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)
train_targets = npz['targets'].astype(np.int)

# we load the validation data in the temporary variable
npz = np.load('Audiobooks_data_validation.npz')
# we can load the inputs and the targets in the same line
validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)

# we load the test data in the temporary variable
npz = np.load('Audiobooks_data_test.npz')
# we create 2 variables that will contain the test inputs and the test targets
test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)
55/3: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/4:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 50
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/5: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/6: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
55/7:
# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets
npz = np.load('Audiobooks_data_train.npz')

# we extract the inputs using the keyword under which we saved them
# to ensure that they are all floats, let's also take care of that
train_inputs = npz['inputs'].astype(np.float)
# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)
train_targets = npz['targets'].astype(np.int)

# we load the validation data in the temporary variable
npz = np.load('Audiobooks_data_validation.npz')
# we can load the inputs and the targets in the same line
validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)

# we load the test data in the temporary variable
npz = np.load('Audiobooks_data_test.npz')
# we create 2 variables that will contain the test inputs and the test targets
test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)
55/8:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/9: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/10: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
55/11:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
     tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/12: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/13: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
55/14:
# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets
npz = np.load('Audiobooks_data_train.npz')

# we extract the inputs using the keyword under which we saved them
# to ensure that they are all floats, let's also take care of that
train_inputs = npz['inputs'].astype(np.float)
# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)
train_targets = npz['targets'].astype(np.int)

# we load the validation data in the temporary variable
npz = np.load('Audiobooks_data_validation.npz')
# we can load the inputs and the targets in the same line
validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)

# we load the test data in the temporary variable
npz = np.load('Audiobooks_data_test.npz')
# we create 2 variables that will contain the test inputs and the test targets
test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)
55/15:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
     tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/16: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/17: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
55/18:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
 
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/19: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/20: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
55/21:
# we must import the libraries once again since we haven't imported them in this file
import numpy as np
import tensorflow as tf
55/22:
# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets
npz = np.load('Audiobooks_data_train.npz')

# we extract the inputs using the keyword under which we saved them
# to ensure that they are all floats, let's also take care of that
train_inputs = npz['inputs'].astype(np.float)
# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)
train_targets = npz['targets'].astype(np.int)

# we load the validation data in the temporary variable
npz = np.load('Audiobooks_data_validation.npz')
# we can load the inputs and the targets in the same line
validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)

# we load the test data in the temporary variable
npz = np.load('Audiobooks_data_test.npz')
# we create 2 variables that will contain the test inputs and the test targets
test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)
55/23:
# Set the input and output sizes
input_size = 10
output_size = 2
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
 
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])


### Choose the optimizer and the loss function

# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

### Training
# That's where we train the model we have built.

# set the batch size
batch_size = 100

# set a maximum number of training epochs
max_epochs = 100

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

# fit the model
# note that this time the train, validation and test data are not iterable
model.fit(train_inputs, # train inputs
          train_targets, # train targets
          batch_size=batch_size, # batch size
          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)
          # callbacks are functions called by a task when a task is completed
          # task here is to check if val_loss is increasing
          callbacks=[early_stopping], # early stopping
          validation_data=(validation_inputs, validation_targets), # validation data
          verbose = 2 # making sure we get enough information about the training process
          )
55/24: test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
55/25: print('\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
57/1:
import pandas as pd
import numpy as np
import matplotlib as plt
sns
57/2:
import pandas as pd
import numpy as np
import matplotlib as plt
57/3: data_raw=pd.read_csv("Abs_data.csv")
57/4: dataraw(head)
57/5:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head()
57/6:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
57/7:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
57/8:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
57/9: raw_data.describe()
57/10: raw_data.describe(all)
57/11: data_raw.describe(all)
57/12: data_raw.describe()
57/13: data_raw = data_raw.drop("ID", axis=1)
57/14: data_raw
57/15:  data_raw.describe(include="all")
57/16:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data_raw.isnull().sum()
57/17:
# Let's simply drop all missing values
# This is not always recommended, however, when we remove less than 5% of the data, it is okay
data_no_mv = data_raw.dropna(axis=0)
57/18:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data_raw.isnull().sum("Reason for Absence")
57/19:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data_raw.isnull().sum()
57/20: sns.distplot(data_raw['Reason for Absence'])
57/21: sns.distplot(data_raw['Absenteeism Time in Hours'])
57/22: data_with_dummies = pd.get_dummies(data_raw, drop_first=True)
57/23: data_with_dummies
57/24: data_no_date = data_raw.drop(['Date'],axis=1)
57/25:
data_no_date = data_raw.drop(['Date'],axis=1)
data_no_date
57/26: data_with_dummies=pd.get_dummies(data_no_date, drop_first=True))
57/27: data_with_dummies=pd.get_dummies(data_no_date, drop_first=True)
57/28:
data_with_dummies=pd.get_dummies(data_no_date, drop_first=True)
data_with_dummies
57/29:
data_with_dummies=pd.get_dummies(data_raw, drop_first=True)
data_with_dummies
57/30:
data_with_dummies=pd.get_dummies(data_no_date, drop_first=True)
data_with_dummies
57/31:
data_with_dummies=pd.get_dummies(data_no_date, drop_first=false)
data_with_dummies
57/32:
data_with_dummies=pd.get_dummies(data_no_date, drop_first=False)
data_with_dummies
57/33:
data_with_dummies=pd.get_dummies(data_no_date, prefix=['Reason for Absence'], columns=['Reason for Absence'])
data_with_dummies
57/34:
data_with_dummies=pd.get_dummies(data_no_date, columns=['Reason for Absence'])
data_with_dummies
57/35:
data_with_dummies=pd.get_dummies(data_no_date, columns=['Reason for Absence'])
data_with_dummies
57/36:
data_with_dummies=pd.get_dummies(data_raw, columns=['Reason for Absence'])
data_with_dummies
57/37:
data_raw = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows = None
57/38:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
57/39:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
57/40:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows = None
57/41: data
57/42:  data.describe(include="all")
57/43:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
57/44: sns.distplot(data['Reason for Absence'])
57/45: sns.distplot(data§['Absenteeism Time in Hours'])
57/46: sns.distplot(data['Absenteeism Time in Hours'])
57/47: data["Reason for Absence"].max
57/48:
data_with_dummies=pd.get_dummies(data_raw, columns=['Reason for Absence'])
data_with_dummies
57/49: data["Reason for Absence"].max()
57/50:
data["Reason for Absence"].max()
data["Reason for Absence"].min()
57/51: data["Reason for Absence"].max(), data["Reason for Absence"].min()
57/52: pd.unique(data['Reason for Absence'])
57/53: sorted(data['Reason for Absence'].unique())
57/54: reason_columns = pd.get_dummies(data['Reason for Absence'])
57/55:
reason_columns = pd.get_dummies(data['Reason for Absence'])
reason_columns
57/56: reason_columns["check"]=reason_colmns.sum(axis=0)
57/57: reason_columns['check']=reason_colmns.sum(axis=0)
57/58: reason_columns['check']=reason_columns.sum(axis=0)
57/59:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/60:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/61:
pd.options.display.max_columns = 10
pd.options.display.max_rows=20
57/62:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/63:
pd.options.display.max_columns = 20
pd.options.display.max_rows=10
57/64:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/65:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
57/66:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/67:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/68:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/69:
reason_columns['check']=reason_columns.sum(axis=)
reason_columns
57/70:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/71:
reason_columns['check']=reason_columns.sum(axis=2)
reason_columns
57/72:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/73:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/74:
reason_columns['check']=reason_columns.sum()
reason_columns
57/75:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/76:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
57/77:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
57/78:
reason_columns['check']=reason_columns.sum(axis=0)
reason_columns
57/79:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns
57/80:
reason_columns['check']=reason_columns.sum(axis=1)
reason_columns.drop['check']
57/81: reason_columns
57/82: reason_columnsdrop("check", axis=1)
57/83: reason_columns.drop("check", axis=1)
57/84: data
57/85: data.drop('Reason for Absence', axis=1)
57/86: reason_columns.loc[:,1:14].max(axis=0)
57/87: reason_columns.loc[:,1:14].max(axis=1)
57/88: reason_columns.loc[:, 1:14].max(axis=1)
57/89: reason_columns.loc[:, 1:14]
57/90: reason_columns.loc[:, 1:14].max(axis=1)
66/1:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
66/2:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
66/3: data_raw.describe()
66/4:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows=None
66/5: data
66/6:  data.describe(include="all")
66/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
66/8: sns.distplot(data['Reason for Absence'])
66/9: sns.distplot(data['Absenteeism Time in Hours'])
66/10: data["Reason for Absence"].max(), data["Reason for Absence"].min()
66/11: pd.unique(data['Reason for Absence'])
66/12: sorted(data['Reason for Absence'].unique())
66/13:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
66/14:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
66/15: reason_columns.drop("check", axis=1)
66/16:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
66/17: reason_columns.loc[:, 1:14].max(axis=1)
66/18:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
66/19: data.drop('Reason for Absence', axis=1)
66/20: reason_columns.loc[:, 1:14].max(axis=1)
66/21:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
66/22: reason_type_1
66/23: pd.reason_type_1
66/24:
reason_type_1 = pd.reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
66/25:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
66/26: reason_type_4
66/27: data['Reason_type_1']=reason_type_1
66/28: data
66/29:
data['Reason_type_1']=reason_type_1
data['Reason_type_2']=reason_type_2
data['Reason_type_3']=reason_type_3
data['Reason_type_4']=reason_type_4
66/30: data
66/31: data_reason=data
66/32:
data_reason=data
data_reason
66/33: data_reason["Date"]
66/34: data_reason["Date"].type()
66/35: type(data_reason["Date"])
66/36: type(data_reason["Date"][0])
66/37: data_reason["Date"][0]
67/1: data.values
67/2: data.values()
67/3: data.columns.values()
67/4: data_reason.columns.values()
67/5: data_reason.columns.values
67/6: data_reason
67/7: data_reason
68/1:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
68/2:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
68/3: data_raw.describe()
68/4:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows=None
68/5: data
68/6:  data.describe(include="all")
68/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
68/8: sns.distplot(data['Reason for Absence'])
68/9: sns.distplot(data['Absenteeism Time in Hours'])
68/10: data["Reason for Absence"].max(), data["Reason for Absence"].min()
68/11: pd.unique(data['Reason for Absence'])
68/12: sorted(data['Reason for Absence'].unique())
68/13:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
68/14:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
68/15: data.drop('Reason for Absence', axis=1)
68/16: reason_columns.loc[:, 1:14].max(axis=1)
68/17:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
68/18: reason_type_4
68/19:
data['Reason_type_1']=reason_type_1
data['Reason_type_2']=reason_type_2
data['Reason_type_3']=reason_type_3
data['Reason_type_4']=reason_type_4
68/20:
data_reason=data
data_reason
68/21: type(data_reason["Date"][0])
68/22: data_reason["Date"][0]
68/23: data_reason
68/24: data_reason.columns.values
68/25: column_names=data_reason.columns.values
68/26: column_names
68/27:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4''Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
68/28: data_reason=data_reason[column_names_ordered]
68/29:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4','Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
68/30: data_reason=data_reason[column_names_ordered]
68/31: data_reason=data.reason.drop("Reason for Absence", axis=1)
68/32: data_reason=data_reason.drop("Reason for Absence", axis=1)
68/33: data_reason
68/34: data_reason["Date"][0]
68/35: month=data_reason["Date"][0][2:3]
68/36: month
68/37: month=data_reason["Date"][0][3:5]
68/38: month
68/39: int(month)
68/40: data_reason["month"]=data_reason["Date"][:,3:5]
68/41: data_reason["month"]=data_reason["Date"][:, 3:5]
68/42: data_reason["month"]=data_reason["Date"][:][3:5]
68/43: data_reason
68/44: month=data_reason["Date"][:][3:5]
68/45: int(month)
68/46: month=data_reason["Date"][:]
68/47: int(month)
68/48: month
68/49: month=data_reason["Date"]
68/50: month
68/51: data_reason["month"]=data_reason["Date"][:][3:5]
68/52: data_reason
68/53: data_reason_mod=data_reason.copy()
68/54: type(data_reason["Date"][0])
68/55: data_reason["Date"]=pd.to_datetime(data_reason["Date"])
68/56: data_reason["Date"]
68/57: data_reason_mod=data_reason.copy()
68/58: type(data_reason["Date"][0])
69/1:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
69/2:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
69/3: data_raw.describe()
69/4:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows=None
69/5: data
69/6:  data.describe(include="all")
69/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
69/8: sns.distplot(data['Reason for Absence'])
69/9: sns.distplot(data['Absenteeism Time in Hours'])
69/10: data["Reason for Absence"].max(), data["Reason for Absence"].min()
69/11: pd.unique(data['Reason for Absence'])
69/12: sorted(data['Reason for Absence'].unique())
69/13:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
69/14:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
69/15: data.drop('Reason for Absence', axis=1)
69/16: reason_columns.loc[:, 1:14].max(axis=1)
69/17:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
69/18: reason_type_4
69/19:
data['Reason_type_1']=reason_type_1
data['Reason_type_2']=reason_type_2
data['Reason_type_3']=reason_type_3
data['Reason_type_4']=reason_type_4
69/20:
data_reason=data
data_reason
69/21: type(data_reason["Date"][0])
69/22: data_reason["Date"][0]
69/23: column_names=data_reason.columns.values
69/24: column_names
69/25:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4','Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
69/26: data_reason=data_reason[column_names_ordered]
69/27: data_reason=data_reason.drop("Reason for Absence", axis=1)
69/28: data_reason
69/29: data_reason["Date"][0]
69/30: month=data_reason["Date"]
69/31: month
69/32: data_reason["month"]=data_reason["Date"][:][3:5]
69/33: data_reason
69/34: data_reason_mod=data_reason.copy()
69/35: type(data_reason["Date"][0])
69/36: data_reason["Date"]=pd.to_datetime(data_reason["Date"])
69/37: data_reason["Date"]
69/38: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"])
69/39: data_reason_mod["Date"]
69/40: data_reason_mod=data_reason.copy()
69/41: type(data_reason_mod["Date"][0])
70/1:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
70/2:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
70/3: data_raw.describe()
70/4:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows=None
70/5: data
70/6:  data.describe(include="all")
70/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
70/8: sns.distplot(data['Reason for Absence'])
70/9: sns.distplot(data['Absenteeism Time in Hours'])
70/10: data["Reason for Absence"].max(), data["Reason for Absence"].min()
70/11: pd.unique(data['Reason for Absence'])
70/12: sorted(data['Reason for Absence'].unique())
70/13:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
70/14:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
70/15: data.drop('Reason for Absence', axis=1)
70/16: reason_columns.loc[:, 1:14].max(axis=1)
70/17:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
70/18: reason_type_4
70/19:
data['Reason_type_1']=reason_type_1
data['Reason_type_2']=reason_type_2
data['Reason_type_3']=reason_type_3
data['Reason_type_4']=reason_type_4
70/20:
data_reason=data
data_reason
70/21: type(data_reason["Date"][0])
70/22: data_reason["Date"][0]
70/23: column_names=data_reason.columns.values
70/24: column_names
70/25:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4','Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
70/26: data_reason=data_reason[column_names_ordered]
70/27: data_reason=data_reason.drop("Reason for Absence", axis=1)
70/28: data_reason
70/29: data_reason["Date"][0]
70/30: month=data_reason["Date"]
70/31: month
70/32: data_reason["month"]=data_reason["Date"][:][3:5]
70/33: data_reason
70/34: data_reason_mod=data_reason.copy()
70/35: type(data_reason_mod["Date"][0])
70/36: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"])
70/37: data_reason_mod["Date"]
70/38: data_reason_mod=data_reason.copy()
70/39: type(data_reason_mod["Date"][0])
70/40: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"])
70/41: data_reason_mod["Date"]
70/42: type(data_reason_mod["Date"][0])
70/43: data_reason_mod["Date"].head(20)
70/44: data_reason_mod["Date"].head(50)
70/45: data_reason_mod["Date"]
70/46:
pd.options.display.max_columns = None
pd.options.display.max_rows=None
70/47: data_reason_mod["Date"]
70/48: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"], format="%d/%m/%Y")
70/49: data_reason_mod["Date"]
70/50: data_reason_mod=data_reason.copy()
70/51: type(data_reason_mod["Date"][0])
70/52: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"], format="%d/%m/%Y")
70/53: data_reason_mod["Date"]
70/54: type(data_reason_mod["Date"][0])
70/55:
pd.options.display.max_columns = None
pd.options.display.max_rows=None
70/56: data_reason_mod["Date"][0]
70/57: data_reason_mod["Date"][0][6:8]
70/58: data_reason_mod["Date"][0][:,6:8]
70/59: data_reason_mod["Date"][0, 6:8]
70/60: data_reason_mod["Date"][0,6:8]
70/61: data_reason_mod["Date"][0]
70/62: data_reason_mod["Date"].month
70/63: data_reason_mod["Date"][0].month
70/64: shape.data_reason_mod
70/65: shape(data_reason_mod)
70/66: data_reason_mod.shape
70/67: data_reason_mod.shape[0]
70/68:
single_months=[]

for i in range_max:
    single_months=data_reason_mod["Date"][i].month
70/69: range_max=data_reason_mod.shape[0]
70/70:
single_months=[]

for i in range_max:
    single_months=data_reason_mod["Date"][i].month
70/71:
single_months=[]

for i in range(range_max):
    single_months=data_reason_mod["Date"][i].month
70/72: single_months
70/73:
single_months=[]

for i in range(range_max):
    single_months=append.data_reason_mod["Date"][i].month
70/74:
single_months=[]

for i in range(range_max):
    single_months.append(data_reason_mod["Date"][i].month)
70/75: single_months
70/76: data_reason_mod["Month"]=single_months
70/77: data_reason_mod
70/78: data_reason_mod.drop("month", axis=1)
70/79:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4','Reason for Absence', 'Date', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
70/80: data_reason_mod=data_reason_mod[column_names_ordered]
70/81: column_names=data_reason.columns.values
70/82: column_names
70/83:
columns_ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', , 'month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours']
70/84:
columns_ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours']
70/85: data_months=data_reason[columns_ordered]
70/86: data_months
70/87: column_names=data_reason_mod.columns.values
70/88: column_names
70/89: data_reason_mod.drop("month", axis=1)
70/90: column_names=data_reason_mod.columns.values
70/91: column_names
70/92:
columns_ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours']
70/93: data_months=data_reason[columns_ordered]
70/94: data_months
70/95: column_names=data_reason_mod.columns.values
70/96: column_names
70/97: data_reason_mod=data_reason.copy()
71/1:
import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn import preprocessing
import seaborn as sns
sns.set()
71/2:
data_raw=pd.read_csv("Abs_data.csv")
data_raw.head(20)
71/3: data_raw.describe()
71/4:
data = data_raw.drop("ID", axis=1)
pd.options.display.max_columns = None
pd.options.display.max_rows=None
71/5: data
71/6:  data.describe(include="all")
71/7:
# data.isnull() # shows a df with the information whether a data point is null 
# Since True = the data point is missing, while False = the data point is not missing, we can sum them
# This will give us the total number of missing values feature-wise
data.isnull().sum()
71/8: sns.distplot(data['Reason for Absence'])
71/9: sns.distplot(data['Absenteeism Time in Hours'])
71/10: data["Reason for Absence"].max(), data["Reason for Absence"].min()
71/11: pd.unique(data['Reason for Absence'])
71/12: sorted(data['Reason for Absence'].unique())
71/13:
reason_columns = pd.get_dummies(data['Reason for Absence'], drop_first=True)
reason_columns
71/14:
pd.options.display.max_columns = 40
pd.options.display.max_rows=10
71/15: data.drop('Reason for Absence', axis=1)
71/16: reason_columns.loc[:, 1:14].max(axis=1)
71/17:
reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)
71/18: reason_type_4
71/19:
data['Reason_type_1']=reason_type_1
data['Reason_type_2']=reason_type_2
data['Reason_type_3']=reason_type_3
data['Reason_type_4']=reason_type_4
71/20:
data_reason=data
data_reason
71/21: type(data_reason["Date"][0])
71/22: data_reason["Date"][0]
71/23: column_names=data_reason.columns.values
71/24: column_names
71/25:
column_names_ordered=['Reason_type_1', 'Reason_type_2',
       'Reason_type_3', 'Reason_type_4','Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours']
71/26: data_reason=data_reason[column_names_ordered]
71/27: data_reason=data_reason.drop("Reason for Absence", axis=1)
71/28: data_reason
71/29: data_reason["Date"][0]
71/30: month=data_reason["Date"]
71/31: month
71/32: data_reason
71/33: data_reason_mod=data_reason.copy()
71/34: type(data_reason_mod["Date"][0])
71/35: data_reason_mod["Date"]=pd.to_datetime(data_reason_mod["Date"], format="%d/%m/%Y")
71/36: data_reason_mod["Date"]
71/37: type(data_reason_mod["Date"][0])
71/38:
pd.options.display.max_columns = None
pd.options.display.max_rows=None
71/39: data_reason_mod["Date"][0].month
71/40: range_max=data_reason_mod.shape[0]
71/41:
single_months=[]

for i in range(range_max):
    single_months.append(data_reason_mod["Date"][i].month)
71/42: single_months
71/43: data_reason_mod["Month"]=single_months
71/44: data_reason_mod
71/45: data_reason_mod.drop("month", axis=1)
71/46: column_names=data_reason_mod.columns.values
71/47: column_names
71/48:
columns_ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours']
71/49: data_months=data_reason[columns_ordered]
71/50:
columns_ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours']
71/51: data_months=data_reason[columns_ordered]
71/52: data_months=data_reason_mod[columns_ordered]
71/53: data_months
71/54: data_months["Date"].[0].weekday()
71/55: data_months["Date"][0].weekday()
71/56: data_months["Date"][2].weekday()
71/57: data_months["Date"][500].weekday()
71/58: data_months["Date"][:].weekday()
71/59: data_months["Date"][0].weekday()
71/60:
Days_single=[]

for i in range(range_max):
    Days_single.append(data_months["Date"][i].weekday())
71/61: Days_single
71/62: Days_single+1
71/63: yy=Days_single+1
71/64: yy=Days_single
71/65: Days_single
71/66: data_days=data_months.copy()
71/67: data_days["Days of the week"]=Days_single
71/68: data_days.columns
71/69: data_days.drop("Date", axis=1)
71/70: data_days.columns
71/71:
ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'Days of the week', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',
       'Pets', 'Absenteeism Time in Hours']
71/72: data_days=data_days[ordered]
71/73: data_days
71/74: data_days=data_days.drop("Date", axis=1)
71/75: data_days.columns
71/76:
ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Date', 'Days of the week', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',
       'Pets', 'Absenteeism Time in Hours']
71/77: data_days=data_days[ordered]
71/78: data_days
71/79:
ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
         'Days of the week', 'Month', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',
       'Pets', 'Absenteeism Time in Hours']
71/80: data_days=data_days[ordered]
71/81: sns.distplot(data['Days of the week'])
71/82: sns.distplot(data_days['Days of the week'])
71/83: sns.distplot(data_days['Transportation Expense'])
71/84: sns.distplot(data_days['Distance to Work'])
71/85: sns.distplot(data_days['Absenteeism Time in Hours'])
71/86: data_days["Education"]
71/87: sns.distplot(data_days['Education'])
71/88: data_days["Education"].unique()
71/89: Education=pd.get_dummies["Education"]
71/90: Education=pd.get_dummies(data_days["Education"])
71/91:
Education=pd.get_dummies(data_days["Education"])
Education
71/92: data_days["Education"].unique()
71/93: Education=data_days["Education"]
71/94:
Education_index=[]

for i in range(range_max):
    if Education[i]=1:
        Education_index[i].append(0)
        else:
            Education_index[i].append(1)
71/95:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
        else:
            Education_index[i].append(1)
71/96:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
        else
            Education_index[i].append(1)
71/97:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
        else if:
            Education_index[i].append(1)
71/98:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
        else:
            Education_index[i].append(1)
71/99:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
    else:
            Education_index[i].append(1)
71/100:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
        else:
            Education_index[i].append(1)
71/101:
Education_index=[]

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
    else:
        Education_index[i].append(1)
71/102:
Education=data_days["Education"]
Education
71/103:
Education=data_days["Education"]
Education[0]
71/104:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(no_edu)
    else:
        Education_index[i].append(edu)
71/105: i
71/106: range_max
71/107:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index[i]=0
    else:
        Education_index[i]=1
71/108:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(0)
    else:
        Education_index[i].append(1)
71/109:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index[i].append(data_days["Education"][i])
    else:
        Education_index[i].append(1)
71/110: i
71/111: Education_index
71/112:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index.append(0)
    else:
        Education_index.append(1)
71/113:
Education_index=[]
no_edu=0
edu=1

for i in range(range_max):
    if Education[i]==1:
        Education_index.append(0)
    else:
        Education_index.append(1)
71/114: Education_index
71/115:
Education_index=[]
no_edu=0
edu=1
x=0
y=0

for i in range(range_max):
    if Education[i]==1:
        Education_index.append(no_edu)
        x+=
    else:
        Education_index.append(edu)
        y+=
71/116:
Education_index=[]
no_edu=0
edu=1
x=0
y=0

for i in range(range_max):
    if Education[i]==1:
        Education_index.append(no_edu)
        x+=x
    else:
        Education_index.append(edu)
        y+=y
71/117: x,y
71/118:
Education_index=[]
no_edu=0
edu=1
x=0
y=0

for i in range(range_max):
    if Education[i]==1:
        Education_index.append(no_edu)
        x=x+1
    else:
        Education_index.append(edu)
        y=y+1
71/119: Education_index
71/120: x,y
71/121: data_Education=data_days.copy()
71/122: data_Education.drop("Education", axis=1)
71/123: data_Education["Edu_index"]=Education_index
71/124: data_Education.columns
71/125: data_Education=data_Education.drop("Education", axis=1)
71/126: data_Education["Edu_index"]=Education_index
71/127: data_Education.columns
71/128:
ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index',
       'Education', 'Children', 'Pets', 'Absenteeism Time in Hours',
       'Edu_index']
71/129:
ordered=['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index',
       'Edu_index', 'Children', 'Pets', 'Absenteeism Time in Hours']
71/130: data_Education=data_Education[ordered]
71/131: data_Education
71/132: df_preprocessed=data_Education.copy()
71/133: df_preprocessed.to_csv('Preprocessed_data.csv', index=False)
72/1:
import pandas as pd
import numpy as np
72/2: data_imported=pd.read_csv("Preprocessed_data.csv")
72/3: data_imported
72/4: targets=np.where(data_imported["Absenteeism Time in Hours"]>3, 1, 0)
72/5: targets
72/6: targets.shape
72/7: data_with_targets=data_imported.copy()
72/8: data_with_targets["Targets"]=targets
72/9: data_with_targets
72/10:
median=data_imported["Absenteeism Time in Hours"].median()
print(median)

targets=np.where(data_imported["Absenteeism Time in Hours"]>3, 1, 0)
72/11:
median=data_imported["Absenteeism Time in Hours"].median()
print(median)

targets=np.where(data_imported["Absenteeism Time in Hours"]>median, 1, 0)
72/12: targets.shape
72/13: data_with_targets=data_imported.copy()
72/14: data_with_targets["Targets"]=targets
72/15: data_with_targets
72/16: sum(data_with_targets["Targets"])
72/17: targets.shape[0]
72/18: range_maxtargets.shape[0]
72/19: range_max=targets.shape[0]
72/20: Excessive_count=sum(data_with_targets["Targets"])
72/21:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
72/22:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
print(Excessive_count, Moderate)
72/23:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
ratio=Excessive_count/Moderate
print(Excessive_count, Moderate, ratio)
72/24:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
ratio=Excessive_count/range_max
print(Excessive_count, Moderate, ratio)
72/25: data_targets_mod=data_with_targets.drop("Absenteeism Time in Hours", axis=1)
72/26: data_targets
72/27: data_targets_mod
72/28: inputs_sel=data_targets.iloc[:,0:-2]
72/29: inputs_sel=data_targets.iloc[:,0:8]
72/30: inputs_sel=data_targets_mod.iloc[:,0:-2]
72/31:
inputs_sel=data_targets_mod.iloc[:,0:-2]
inputs_sel
72/32:
inputs_sel=data_targets_mod.iloc[:,0:-1]
inputs_sel
72/33:
inputs_sel=data_targets_mod.iloc[:,:-1]
inputs_sel
72/34: from sklearn.preprocessing import StandardScalar
72/35: from sklearn.preprocessing import StandardScaler
72/36:
from sklearn.preprocessing import StandardScaler

Scaler=StandardScaler()
72/37: Scaler.fit(inputs_sel)
72/38: Scaled_inputs=Scaler.transform(inputs_sel)
72/39: Scaled_inputs
72/40: Scaled_inputs.shape
72/41: Scaled_inputs[2,6]
72/42: Scaled_inputs[:,6]
72/43: rows=Scaled_input.Shape
72/44: rows=Scaled_inputs.Shape
72/45: rows=Scaled_inputs.shape
72/46: rows=Scaled_inputs.shape[1]
72/47: rows=Scaled_inputs.shape[0]
72/48:
rows=Scaled_inputs.shape[0]
rows
72/49: from sklearn.preprocessing import train_test_split
72/50: from sklearn.model_selection import train_test_split
72/51: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets)
72/52: inputs_train, inputs_test, target_train, target_test
72/53: inputs_train.shape
72/54: inputs_train.shape, inputs_test.shape
72/55: inputs_train.shape, inputs_test.shape, target_train.shape
72/56: inputs_test.shape[0]/inputs_train.shape[0]
72/57: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.8)
72/58: inputs_train.shape, inputs_test.shape, target_train.shape
72/59: inputs_test.shape[0]/inputs_train.shape[0]
72/60: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.85)
72/61: inputs_train.shape, inputs_test.shape, target_train.shape
72/62: inputs_test.shape[0]/inputs_train.shape[0]
72/63: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.80)
72/64: inputs_train.shape, inputs_test.shape, target_train.shape
72/65: inputs_test.shape[0]/inputs_train.shape[0]
72/66: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.9)
72/67: inputs_train.shape, inputs_test.shape, target_train.shape
72/68: inputs_test.shape[0]/inputs_train.shape[0]
72/69: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.8)
72/70: inputs_train.shape, inputs_test.shape, target_train.shape
72/71: inputs_test.shape[0]/inputs_train.shape[0]
72/72: inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.8, shuffle=True)
72/73: inputs_train.shape, inputs_test.shape, target_train.shape
72/74: inputs_test.shape[0]/inputs_train.shape[0]
72/75:
inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, train_size=0.8, shuffle=True
                                                                     random_state=20)
72/76:
inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, 
                                                                      train_size=0.8, shuffle=True,random_state=20)
72/77: inputs_train.shape, inputs_test.shape, target_train.shape
72/78: inputs_test.shape[0]/inputs_train.shape[0]
72/79:  ##MODEL
72/80:
from sklear.linear_model import LogisticRegression
from sklearn import metrics
72/81:
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
72/82: Reg=LogisticRegression()
72/83:
Reg=LogisticRegression()
Reg.fit(inputs_train, target train)
72/84:
Reg=LogisticRegression()
Reg.fit(inputs_train, target train)
72/85: Reg.fit(inputs_train, target train)
72/86: Reg=LogisticRegression()
72/87: Reg.fit(inputs_train, target train)
72/88: Reg.fit(inputs_train, target_train)
72/89: Reg.score(inputs_train, target_train)
72/90: Reg.predict(inputs_train)
72/91: Output_train=Reg.predict(inputs_train)
72/92:
Output_train=Reg.predict(inputs_train)
targets
72/93: Output=Reg.predict(inputs_train)
72/94:
Output=Reg.predict(inputs_train)
Output==target
72/95:
Output=Reg.predict(inputs_train)
Output==targets
72/96:
Output=Reg.predict(inputs_train)
Output==target_train
72/97: np.sum(Output==target_train)
72/98: np.sum(Output==target_train)/range_max
72/99: np.sum(Output==target_train)
72/100: 560/439
72/101: 439/560
72/102: Reg.intercept_
72/103: Reg.coef_
72/104: Reg.coef_.shape
72/105: Reg.coef_
72/106: Scaled_inputs.columns
72/107: inputs_sel.columns
72/108: inputs_sel.columns.values
72/109: Table_input_values=pd.DataFrame(["Variables"]=inputs_sel.columns.values)
72/110: Table_input_values=pd.DataFrame(inputs_sel.columns.values)
72/111: Table_input_values
72/112: Table_input_values=pd.DataFrame(inputs_sel.columns.values="Variables")
72/113: Table_input_values=pd.DataFrame(inputs_sel.columns.values= "Variables")
72/114: Table_input_values=pd.DataFrame(inputs_sel.columns.values= ["Variables"])
72/115: Table_input_values=pd.DataFrame(inputs_sel.columns.values = ["Variables"])
72/116: Table_input_values=pd.DataFrame(inputs_sel.columns = ["Variables"])
72/117: Table_input_values
72/118: Table_input_values=pd.DataFrame(columns = ["Variables"])
72/119: Table_input_values
72/120: Table_input_values=pd.DataFrame(columns = ["Variables"], inputs_sel.columns.values)
72/121: Table_input_values=pd.DataFrame(columns = ["Variables"], inputs_sel.columns.values)
72/122: Table_input_values=pd.DataFrame(columns = ["Variables"], raws=inputs_sel.columns.values)
72/123: Table_input_values=pd.DataFrame(columns = ["Variables"], rows=inputs_sel.columns.values)
72/124: Table_input_values
72/125: Table_input_values=pd.DataFrame(columns = ["Variables"], data=inputs_sel.columns.values)
72/126: Table_input_values
72/127: Table_input_values=pd.DataFrame(columns = ["Variables name", "values"], data=inputs_sel.columns.values, Reg.coef_)
72/128: Table_input_values
72/129: Table_input_values=pd.DataFrame(columns = ["Variables name", "values"], data=inputs_sel.columns.values)
72/130: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
72/131: Table_input_values["values"]=Reg_coef_
72/132: coef=Reg.coef_
72/133: inputs_sel.columns.values
72/134: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
72/135: Table_input_values["values"]=coef
72/136:
coef=Reg.coef_
coef.shape
72/137: Table_input_values["values"]=T.coef
72/138: Table_input_values["values"]=coef.T
72/139: Table_input_values
72/140: Table_input_values.index
72/141: Table_input_values.index+1
72/142: Table_input_values.index=Table_input_values.index+1
72/143:
Table_input_values.index=Table_input_values.index+1
Table_input_values
72/144:
Table_input_values.index=Table_input_values.index+1
Table_input_values[0]
72/145:
Table_input_values.index=Table_input_values.index+1
Table_input_values[0,0]
72/146:
Table_input_values.index=Table_input_values.index+1
Table_input_values(0,0)
72/147: Table_values.index=Table_input_values.index+1
72/148:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
72/149:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
72/150: Summary_table
72/151: Summary_table.sort
72/152: Summary_table=Summary_table.sort_index
72/153: Summary_table
72/154: Summary_table=Summary_table.sort_index()
72/155: Summary_table
72/156: Summary_table=Summary_table.sort_index()
72/157:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
summary_table
72/158:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
Summary_table
72/159: Summary_table=Summary_table.sort_index()
72/160: Summary_table=Summary_table.sort_index()
72/161: Summary_table
72/162: Table_input_values
72/163: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
72/164: Table_input_values["values"]=coef.T
72/165: Table_input_values
72/166:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
Summary_table
72/167: Summary_table=Summary_table.sort_index()
72/168: Summary_table
72/169: Summary_table["Odds ratio"]=np.exp(Summary_table["values"])
72/170: Summary_table
72/171: Summary_table=Summary_table.sort_values("Odds ratio")
72/172:
Summary_table=Summary_table.sort_values("Odds ratio")
Summary_table
72/173:
Summary_table=Summary_table.sort_values("Odds ratio", ascending=True)
Summary_table
72/174:
Summary_table=Summary_table.sort_values("Odds ratio", ascending=False)
Summary_table
72/175:
Summary_table=Summary_table.sort_values("Odds ratio", descending=True)
Summary_table
72/176:
Summary_table=Summary_table.sort_values("Odds ratio", ascending=False)
Summary_table
73/1:
# import the relevant libraries
import pandas as pd
import numpy as np
73/2:
# load the preprocessed CSV data
data_preprocessed = pd.read_csv('Absenteeism_preprocessed.csv')
72/177:
#Scaler.fit(inputs_sel)

class CustomScaler(BaseEstimator,TransformerMixin): 
    
    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.mean_ = None
        self.var_ = None

    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.array(np.mean(X[self.columns]))
        self.var_ = np.array(np.var(X[self.columns]))
        return self

    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]
72/178:
import pandas as pd
import numpy as np

class CustomScaler(BaseEstimator,TransformerMixin): 
    
    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.mean_ = None
        self.var_ = None

    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.array(np.mean(X[self.columns]))
        self.var_ = np.array(np.var(X[self.columns]))
        return self

    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]
72/179:
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
class CustomScaler(BaseEstimator,TransformerMixin): 
    
    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.mean_ = None
        self.var_ = None

    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.array(np.mean(X[self.columns]))
        self.var_ = np.array(np.var(X[self.columns]))
        return self

    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]
72/180: data_imported=pd.read_csv("Preprocessed_data.csv")
72/181: data_imported
72/182:
median=data_imported["Absenteeism Time in Hours"].median()
print(median)

targets=np.where(data_imported["Absenteeism Time in Hours"]>median, 1, 0)
72/183: range_max=targets.shape[0]
72/184: data_with_targets=data_imported.copy()
72/185: data_with_targets["Targets"]=targets
72/186: data_with_targets
72/187:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
ratio=Excessive_count/range_max
print(Excessive_count, Moderate, ratio)
72/188: data_targets_mod=data_with_targets.drop("Absenteeism Time in Hours", axis=1)
72/189: data_targets_mod
72/190:
inputs_sel=data_targets_mod.iloc[:,:-1]
inputs_sel
72/191:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
72/192:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
columns_to_scale=['Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Children', 'Pets']
72/193:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
columns_to_scale=['Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Children', 'Pets']
Scaler.fit(columns_to_scale)
72/194:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
columns_to_scale=['Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Children', 'Pets']
Scaler.fit(inputs_sel)
72/195:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
columns_to_scale=['Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Children', 'Pets']
Scaler=CustomScaler(columns_to_scale)
Scaler.fit(inputs_sel)
72/196: Scaled_inputs=Scaler.transform(inputs_sel)
72/197: Scaled_inputs.shape
72/198: Scaled_inputs[:,6]
72/199: Scaled_inputs
72/200:
rows=Scaled_inputs.shape[0]
rows
Split_r=0.8
72/201: from sklearn.model_selection import train_test_split
72/202:
inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, 
                                                                      train_size=0.8, shuffle=True,random_state=20)
75/1:
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
class CustomScaler(BaseEstimator,TransformerMixin): 
    
    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.mean_ = None
        self.var_ = None

    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.array(np.mean(X[self.columns]))
        self.var_ = np.array(np.var(X[self.columns]))
        return self

    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]
75/2: data_imported=pd.read_csv("Preprocessed_data.csv")
75/3: data_imported
75/4:
median=data_imported["Absenteeism Time in Hours"].median()
print(median)

targets=np.where(data_imported["Absenteeism Time in Hours"]>median, 1, 0)
75/5: range_max=targets.shape[0]
75/6: data_with_targets=data_imported.copy()
75/7: data_with_targets["Targets"]=targets
75/8: data_with_targets
75/9:
Excessive_count=sum(data_with_targets["Targets"])
Moderate=range_max-Excessive_count
ratio=Excessive_count/range_max
print(Excessive_count, Moderate, ratio)
75/10: data_targets_mod=data_with_targets.drop("Absenteeism Time in Hours", axis=1)
75/11: data_targets_mod
75/12:
inputs_sel=data_targets_mod.iloc[:,:-1]
inputs_sel
75/13:
from sklearn.preprocessing import StandardScaler

#Scaler=StandardScaler()
75/14:
#Scaler.fit(inputs_sel)
inputs_sel.columns.values
columns_to_scale=['Days of the week', 'Month', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Children', 'Pets']
Scaler=CustomScaler(columns_to_scale)
Scaler.fit(inputs_sel)
75/15: Scaled_inputs=Scaler.transform(inputs_sel)
75/16: Scaled_inputs.shape
75/17: Scaled_inputs
75/18:
rows=Scaled_inputs.shape[0]
rows
Split_r=0.8
75/19: from sklearn.model_selection import train_test_split
75/20:
inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, 
                                                                      train_size=0.8, shuffle=True,random_state=20)
75/21: inputs_train.shape, inputs_test.shape, target_train.shape
75/22: inputs_test.shape[0]/inputs_train.shape[0]
75/23:
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
75/24: Reg=LogisticRegression()
75/25: Reg.fit(inputs_train, target_train)
75/26: Reg.score(inputs_train, target_train)
75/27:
Output=Reg.predict(inputs_train)
Output==target_train
75/28: np.sum(Output==target_train)
75/29: 439/560
75/30: Reg.intercept_
75/31:
coef=Reg.coef_
coef.shape
75/32: inputs_sel.columns.values
75/33: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
75/34: Table_input_values["values"]=coef.T
75/35: Table_input_values
75/36:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
Summary_table
75/37: Summary_table=Summary_table.sort_index()
75/38: Summary_table
75/39: Summary_table["Odds ratio"]=np.exp(Summary_table["values"])
75/40:
Summary_table=Summary_table.sort_values("Odds ratio", ascending=False)
Summary_table
75/41:
Scaled_inputs=Scaled_inputs.drop(['Day of the Week','Daily Work Load Average','Distance to Work'],axis=1)
Scaled_inputs
75/42:
Scaled_inputs=Scaled_inputs.drop(['Days of the Week','Daily Work Load Average','Distance to Work'],axis=1)
Scaled_inputs
75/43:
Scaled_inputs=Scaled_inputs.drop(['Days of the week','Daily Work Load Average','Distance to Work'],axis=1)
Scaled_inputs
75/44:
rows=Scaled_inputs.shape[0]
rows
Split_r=0.8
75/45: from sklearn.model_selection import train_test_split
75/46:
inputs_train, inputs_test, target_train, target_test=train_test_split(Scaled_inputs, targets, 
                                                                      train_size=0.8, shuffle=True,random_state=20)
75/47: inputs_train.shape, inputs_test.shape, target_train.shape
75/48: inputs_test.shape[0]/inputs_train.shape[0]
75/49:
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
75/50: Reg=LogisticRegression()
75/51: Reg.fit(inputs_train, target_train)
75/52: Reg.score(inputs_train, target_train)
75/53:
Output=Reg.predict(inputs_train)
Output==target_train
75/54: np.sum(Output==target_train)
75/55: 439/560
75/56: Reg.intercept_
75/57: 433/560
75/58: Reg.intercept_
75/59:
coef=Reg.coef_
coef.shape
75/60: inputs_sel.columns.values
75/61: Scaled_inputs.columns.values
75/62: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
75/63: Table_input_values["values"]=coef.T
75/64: Table_input_values
75/65:
Scaled_inputs.columns.values
Scaled_inputs.columns.shape
75/66: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
75/67:
Table_input_values=pd.DataFrame(columns = ["Variables name"], data=inputs_sel.columns.values)
Table_input_values.shape
75/68:
Scaled_inputs.columns.values
Scaled_inputs.columns.shape
DataFrame
75/69:
Table_input_values=pd.DataFrame(columns = ["Variables name"], data=Scaled_input.columns.values)
Table_input_values.shape
75/70: Table_input_values=pd.DataFrame(columns = ["Variables name"], data=Scaled_inputs.columns.values)
75/71: Table_input_values["values"]=coef.T
75/72: Table_input_values
75/73:
Summary_table=Table_input_values.copy()
Summary_table.index=Summary_table.index+1
Summary_table.loc[0]=["Intercept", Reg.intercept_[0]]
Summary_table
75/74: Summary_table=Summary_table.sort_index()
75/75: Summary_table
75/76: Summary_table["Odds ratio"]=np.exp(Summary_table["values"])
75/77:
Summary_table=Summary_table.sort_values("Odds ratio", ascending=False)
Summary_table
75/78: Reg.score(inputs_test, target_test)
75/79:
#Probability of getting 0 or 1
Prob=Reg.predict_proba(inputs_test)
75/80: Prob
75/81: Prob[:,1]
75/82: sns.regplot(Prob[;,1])
75/83: sns.regplot(Prob[:,1])
75/84:
import seaborn as sns
sns.regplot(Prob[:,1])
75/85:
import seaborn as sns
sns.regplot(inputs_test, Prob[:,1])
75/86:
Prob[:,1]
inputs_test
75/87:
import seaborn as sns
sns.regplot(inputs_test[:,2], Prob[:,1])
75/88:
Prob[:,1]
inputs_test[:,2]
75/89:
Prob[:,1]
inputs_test["Reason_type_3"]
75/90:
import seaborn as sns
sns.regplot(inputs_test["Reason_type_3"], Prob[:,1])
75/91:
Prob[:,1]
inputs_test["Reason_type_3"].head()
75/92:
Prob[:,1]
inputs_test["Reason_type_3"].head(50)
75/93:
import seaborn as sns
sns.distplot(Prob[:,1])
75/94: Prob[:,1]
75/95: Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns, data=Scaled_inputs.columns.values)
75/96:
Prob[:,1]
Scaled_inputs.columns
75/97:
Prob[:,1]
Scaled_inputs.columns
Scaled_inputs.columns.values
75/98: Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.T, data=Scaled_inputs.columns.values)
75/99: Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.T, data=Scaled_inputs.columns.values)
75/100: Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns, data=Scaled_inputs.columns.values)
75/101: Table_test_values=pd.DataFrame(columns = T.Scaled_inputs.columns, data=Scaled_inputs.columns.values)
75/102:
Table_test_values=pd.DataFrame(columns = ['Reason_type_1', 'Reason_type_2', 'Reason_type_3', 'Reason_type_4',
       'Month', 'Transportation Expense', 'Age', 'Body Mass Index',
       'Edu_index', 'Children', 'Pets'], data=Scaled_inputs.columns.values)
75/103: Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=Scaled_inputs)
75/104:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=Scaled_inputs)
Table_test_values
75/105:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=input_test)
Table_test_values
75/106:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
75/107:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=target_test
75/108:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=target_test
Table_test_values["Probability"]=Prob[:,1]
75/109:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=target_test
Table_test_values["Probability"]=Prob[:,1]
Table_test_values
75/110:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=target_test
Table_test_values["Probability"]=Prob[:,1]
Table_test_values.head(100)
75/111:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=target_test
Table_test_values["Probability"]=Prob[:,1]
Table_test_values.head(20)
75/112:
Prob[:,1]
Scaled_inputs.columns
Scaled_inputs.columns.values
Output_test=Reg.predict(inputs_test)
75/113:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=Output_test
Table_test_values["Probability"]=Prob[:,1]
Table_test_values.head(20)
75/114:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=Output_test
Table_test_values["Target REAL"]=target_test
Table_test_values["Probability"]=Prob[:,1]

Table_test_values.head(20)
75/115:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=Output_test
Table_test_values["Target REAL"]=target_test
Table_test_values["Probability"]=Prob[:,1]
pd.options.display.max_columns = None
pd.options.display.max_rows=None
Table_test_values.head(20)
75/116:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=Output_test
Table_test_values["Target REAL"]=target_test
Table_test_values["Probability"]=Prob[:,1]
pd.options.display.max_columns = None
pd.options.display.max_rows=None
Table_test_values.head
75/117:
Table_test_values=pd.DataFrame(columns = Scaled_inputs.columns.values, data=inputs_test)
Table_test_values
Table_test_values["Target"]=Output_test
Table_test_values["Target REAL"]=target_test
Table_test_values["Probability"]=Prob[:,1]
pd.options.display.max_columns = None
pd.options.display.max_rows=None
Table_test_values
75/118:
import seaborn as sns
sns.regplot(targets_test, Output_test, fit_reg=False)
75/119:
import seaborn as sns
sns.regplot(target_test, Output_test, fit_reg=False)
75/120: import pickle
75/121:
import pickle
With open("model_abs", "wb") as file
pickle.dump(Reg, file)
75/122:
import pickle
with open("model_abs", "wb") as file
pickle.dump(Reg, file)
75/123:
import pickle
with open('model_abs', 'wb') as file
pickle.dump(Reg, file)
75/124:
import pickle
with open('model_abs', 'wb') as file:
    pickle.dump(Reg, file)
75/125:
import pickle
with open('model_abs', 'wb') as file:
    pickle.dump(Reg, file)

with open('Scaler', 'wb') as file:
    pickle.dump(Scaler, file)
76/1: from absenteeism_module import *
76/2: from absenteism_module import *
76/3: from absenteeism_module import *
76/4: model=absenteeism_model('model', 'scaler')
76/5: model.load_and_clean_data('Absenteeism_new_data.csv')
76/6: model.predicted_outputs()
76/7: model.predicted_outputs().to_csv('Absenteeism_predictions.csv', index = False)
76/8: DF=pd.read_csv('Absenteeism_predictions.csv')
76/9: DF
76/10:
import seaborn as sns
sns.regplot(x=DF["Age", y=DF["Probability"], fit_reg=False)
76/11:
import seaborn as sns
sns.regplot(x=DF["Age"], y=DF["Probability"], fit_reg=False)
83/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
83/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
83/3:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

test_data
83/4: test_data
83/5: scaled_train_and_validation_data
83/6: mnist_info.splits['train']
83/7: mnist_datase
83/8: mnist_dataset
84/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
84/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples
84/3: mnist_dataset
84/4:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
84/5:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
84/6:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
84/7: test_loss, test_accuracy = model.evaluate(test_data)
84/8:
# We can apply some nice formatting if we want to
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
84/9:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
84/10: im
84/11:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[7] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/12:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/13: mnist_example
84/14: images
84/15: images.shape
84/16: images.shape()
84/17: iterator
84/18: img
84/19: img.shape
84/20: img.shape[0]
84/21:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
model.predict_classes(img)[0]
84/22: img
84/23: im
84/24: img
84/25:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/26: img
84/27: mnist_example
84/28:
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/29:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/30:
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/31:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/32:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/33:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))

plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))

img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/34:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
plt.imshow(im.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))

img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/35:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/36:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
84/37:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
pred
84/38:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
im
84/39:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/40: img
84/41:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
#model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
im
84/42:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = tf.expand_dims(img, axis=0)      
#img = scale(img, '')[0]
model.predict_classes(img)[0]

 
# predict class of img
#img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
im
84/43:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/44:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/45:
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/46:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/47:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = tf.expand_dims(im, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/48:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/49:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(80, 80), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/50:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
84/51:
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/52:
# plot img
plt.imshow(im.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/53:
gg
img = image.img_to_array(im)
plt.imshow(im.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/54:
img = image.img_to_array(im)
plt.imshow(im.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/55:
img = image.img_to_array(im)
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/56:
img = image.img_to_array(im)
img
#plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/57:
img = image.img_to_array(im)
img.shape
#plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
84/58:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
84/59: img.shape
84/60: img
84/61: images[8]
84/62: images[8].shape
84/63: images[8]
84/64:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
94/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
94/2: train_data
94/3:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
94/4:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data, test_data
94/5: train_data.shape
94/6: type(train_data)
94/7: scaled_train_and_validation_data
94/8: mnist_test.map(scale)
94/9: V
94/10: mnist_train
94/11: image
94/12: mnist_dataset
94/13:
input_size = 784
output_size = 10
# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 100
    
# define how the model will look like
model = tf.keras.Sequential([
    
    # the first layer (the input layer)
    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3
    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images
    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) 
    # or (28x28x1,) = (784,) vector
    # this allows us to actually create a feed forward neural network
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer
    
    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)
    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer
    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer
    # the final layer is no different, we just make sure to activate it with softmax
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer
])
94/14:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
94/15:
# determine the maximum number of epochs
NUM_EPOCHS = 5

# we fit the model, specifying the
# training data
# the total number of epochs
# and the validation data we just created ourselves in the format: (inputs,targets)
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose =2)

#validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2
94/16:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data[1], test_data
94/17:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data[1]
94/18:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
94/19:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data(1)
94/20:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
94/21:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
94/22:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs, validation_targets
94/23:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
validation_targets
94/24: test_loss, test_accuracy = model.evaluate(test_data)
94/25:
# We can apply some nice formatting if we want to
print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))
94/26:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]

im
94/27:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0],im
94/28:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
model.predict_classes(img)[0]
im
94/29:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="2.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
print(model.predict_classes(img)[0])
im
94/30:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
print(model.predict_classes(img)[0])
im
94/31:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
print(model.predict_classes(img)[0])
print(img)
im
94/32:
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
 
image_path="3.png"
im = image.load_img(image_path, target_size=(28, 28), color_mode = "grayscale")
img = image.img_to_array(im)
img = tf.expand_dims(img, axis=0)      
img = scale(img, '')[0]
print(model.predict_classes(img)[0])
print(img.shape)
im
94/33:
# we define the optimizer we'd like to use, 
# the loss function, 
# and the metrics we are interested in obtaining at each iteration
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

train_data
94/34:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
94/35: img
94/36: img.shape
94/37: img
94/38: type.img
94/39: type(img)
94/40: type(img[1])
94/41:
type(img[1])
img[1]
94/42:
#iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
94/43:
iterator = mnist_test.__iter__()
images=[]
for mnist_example in iterator: 
    images.append(mnist_example)
    
mnist_example = images[8] # first image
img, label = mnist_example[0], mnist_example[1]
 
# plot img
plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
 
# predict class of img
img = tf.expand_dims(img, axis=0)      
img = scale(img, label)
pred = model.predict_classes(img)[0]
print('actual class:', label.numpy())
print('predicted class:', pred)
96/1: import pandas as pd
96/2:
import pandas as pd
from openpyx1.workbook import Workbook
96/3:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
96/4:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
os.listdir()
96/5:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
os.listdir()
96/6:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
arr=os.listdir()
96/7:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
#arr=os.listdir()
96/8:
import pandas as pd
#from openpyx1.workbook import Workbook
from os import listdir
#arr=os.listdir()
listdir()
96/9: data_excel=pd.read_excel("exchange_rates[1].xlsx")
96/10: data_excel
96/11: data_excel.head(200)
96/12: data_excel.head(150)
96/13: data_excel.head(4)
96/14: data_excel
96/15: data_excel=pd.read_excel("NFL.xlsx")
96/16: data_excel
96/17:
data_excel=pd.read_excel("NFL.xlsx")
data_excel=data_excel.drop(["Unnamed: 5"], axis=1)
96/18: data_excel
96/19: data_excel["Dallas Cowboys"]=data_excel["Dallas"]
96/20: data_excel["Dallas Cowboys"]
96/21: data_excel.columns
96/22:
data_excel.columns=['Dallas', 'NY', 'Chicago', 'Atlanta',
       'Detroit']
96/23: data_excel
96/24: data_excel.to_excel("Prueba_gente.xlsx")
96/25:
import pandas as pd
from openpyxl.workbook import Workbook
from os import listdir
#arr=os.listdir()
listdir()
96/26:
data_excel=pd.read_excel("NFL.xlsx")
data_excel=data_excel.drop(["Unnamed: 5"], axis=1)
96/27: data_excel.columns
96/28:
data_excel.columns=['Dallas', 'NY', 'Chicago', 'Atlanta',
       'Detroit']
96/29: data_excel.to_excel("Prueba_gente.xlsx")
96/30: data_excel
96/31: data_excel
96/32: data_excel.columns
96/33: data_excel["Last"]
96/34: data_excel["NY"]
96/35: data_excel["NY"][:3]
96/36: data_excel[:3]
96/37: data_excel.iloc
96/38: data_excel.iloc[1]
96/39: data_excel.iloc[2]
96/40: data_excel.iloc[3]
96/41: data_excel.iloc[31]
96/42: data_excel.iloc[8]
96/43: data_excel.iloc[1]
96/44: data_excel.iloc[1,:]
96/45: data_excel.iloc[1,5]
96/46: data_excel.iloc[1,2]
96/47: data_excel
96/48: data_excel.iloc[1]
96/49: data_excel.iloc[1,2]
96/50: data_excel.iloc[:,2]
96/51: data_excel.iloc[1,2]
96/52: data_excel.iloc[0,2]
96/53: data_excel.to_excel("Prueba_gente2.xlsx", index=None)
96/54: data_excel.to_excel("Prueba_gente2.xlsx", index=4)
96/55: data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
96/56:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel
96/57:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
96/58:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
data_excel
96/59:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
data_excel.iloc[1]
96/60:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
data_excel.iloc[:,1]
96/61:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
data_excel.iloc[:,1].mean
96/62:
data_excel["Premium"]=data_excel["Detroit"].apply(lambda x:0.3 if 10<x<30 else .2 )
data_excel["Taxes"]=data_excel["Detroit"]*data_excel["Premium"]
print(data_excel.iloc[:,1].mean)
96/63: from openpyxl import load_workbook
96/64: wb=Workbook()
96/65:
wb=Workbook()
ws=wb.active
ws1=wb.create_sheet("lala")
96/66:
wb=Workbook()
ws=wb.active
ws1=wb.create_sheet("lala")
ws2=wb.create_sheet("lulu", index=0)
96/67:
wb=Workbook()
ws=wb.active
ws1=wb.create_sheet("lala")
ws2=wb.create_sheet("lulu", index=0)
wb.sheet_names
96/68:
wb=Workbook()
ws=wb.active
ws1=wb.create_sheet("lala")
ws2=wb.create_sheet("lulu", index=0)
wb.sheetnames
96/69:
wb=Workbook()
ws=wb.active
ws1=wb.create_sheet("lala")
ws2=wb.create_sheet("lulu", index=0)
ws.title="wwww"
wb.sheetnames
97/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
97/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
97/3:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))

# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*x + gamma*x**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
97/4:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*x + gamma*x**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
97/5:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
97/6:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
97/7:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
97/8:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
97/9:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
97/10:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
97/11:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
97/12:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*zs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
97/13:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
97/14:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
97/15:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
97/16:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
98/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
98/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
98/3:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*zs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
98/4:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
98/5:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
98/6:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
98/7:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
98/8:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
98/9:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
99/1:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
99/2:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs,zs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
99/3:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/4:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/5:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
99/6:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
99/7:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/8:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
99/9:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
99/10:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 1000

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
# zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
99/11:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/12:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/13:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/14:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.plot(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/15:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.plot(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/16:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.plot(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/17:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatterplot(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/18:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/19:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
99/20:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
99/21:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/22: outputs,shape
99/23: outputs.shape
99/24:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
99/25:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (biases)
99/26:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (biases)
99/27:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)
99/28:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
99/29: outputs.shape
100/1:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)


def create_data():
    x = PolynomialFeatures(degree=6).fit_transform(np.linspace(-2,2,100).reshape(100,-1))
    x[:,1:] = MinMaxScaler(feature_range=(-2,2),copy=False).fit_transform(x[:,1:])
    l = lambda x_i: np.cos(0.8*np.pi*x_i)
    data = l(x[:,1])
    noise = np.random.normal(0,0.1,size=np.shape(data))
    y = data+noise
    y= y.reshape(100,1)
    # Normalize Data
    return {'x':x,'y':y}

def plot_function(x,y,w,Error,w_s):
    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(40,10))
    ax[0].plot(x[:,1],[np.cos(0.8*np.pi*x_i) for x_i in x[:,1]],c='lightgreen',linewidth=3,zorder=0)
    ax[0].scatter(x[:,1],y)
    ax[0].plot(x[:,1],np.dot(x,w))
    ax[0].set_title('Function')
    ax[1].scatter(range(iterations),Error)
    ax[1].set_title('Error')



    plt.show()
100/2:
# initialize variables
data = create_data()
x = data['x']
y = data['y']
w = np.random.normal(size=(np.shape(x)[1],1))
eta = 0.1
iterations = 10000
batch = 10



def stochastic_gradient_descent(x,y,w,eta):
    derivative = -(y-np.dot(w.T,x))*x.reshape(np.shape(w))
    return eta*derivative


def batch_gradient_descent(x,y,w,eta):
    derivative = np.sum([-(y[d]-np.dot(w.T.copy(),x[d,:]))*x[d,:].reshape(np.shape(w)) for d in range(len(x))],axis=0)
    return eta*(1/len(x))*derivative


def mini_batch_gradient_descent(x,y,w,eta,batch):
    gradient_sum = np.zeros(shape=np.shape(w))
    for b in range(batch):
        choice = np.random.choice(list(range(len(x))))
        gradient_sum += -(y[choice]-np.dot(w.T,x[choice,:]))*x[choice,:].reshape(np.shape(w))
        return eta*(1/batch)*gradient_sum

# Update w
w_s = []
Error = []
for i in range(iterations):
    # Calculate error
    error = (1/2)*np.sum([(y[i]-np.dot(w.T,x[i,:]))**2 for i in range(len(x))])
    Error.append(error)
    # Stochastic Gradient Descent
    """
    for d in range(len(x)):
        w-= stochastic_gradient_descent(x[d,:],y[d],w,eta)
        w_s.append(w.copy())
    """
    # Minibatch Gradient Descent
    """
    w-= mini_batch_gradient_descent(x,y,w,eta,batch)
    """

    # Batch Gradient Descent



# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
100/3:
# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
100/4:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)


def create_data():
    x = PolynomialFeatures(degree=2).fit_transform(np.linspace(-2,2,100).reshape(100,-1))
    x[:,1:] = MinMaxScaler(feature_range=(-2,2),copy=False).fit_transform(x[:,1:])
    l = lambda x_i: np.cos(0.8*np.pi*x_i)
    data = l(x[:,1])
    noise = np.random.normal(0,0.1,size=np.shape(data))
    y = data+noise
    y= y.reshape(100,1)
    # Normalize Data
    return {'x':x,'y':y}

def plot_function(x,y,w,Error,w_s):
    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(40,10))
    ax[0].plot(x[:,1],[np.cos(0.8*np.pi*x_i) for x_i in x[:,1]],c='lightgreen',linewidth=3,zorder=0)
    ax[0].scatter(x[:,1],y)
    ax[0].plot(x[:,1],np.dot(x,w))
    ax[0].set_title('Function')
    ax[1].scatter(range(iterations),Error)
    ax[1].set_title('Error')



    plt.show()
100/5:
# initialize variables
data = create_data()
x = data['x']
y = data['y']
w = np.random.normal(size=(np.shape(x)[1],1))
eta = 0.1
iterations = 10000
batch = 10



def stochastic_gradient_descent(x,y,w,eta):
    derivative = -(y-np.dot(w.T,x))*x.reshape(np.shape(w))
    return eta*derivative


def batch_gradient_descent(x,y,w,eta):
    derivative = np.sum([-(y[d]-np.dot(w.T.copy(),x[d,:]))*x[d,:].reshape(np.shape(w)) for d in range(len(x))],axis=0)
    return eta*(1/len(x))*derivative


def mini_batch_gradient_descent(x,y,w,eta,batch):
    gradient_sum = np.zeros(shape=np.shape(w))
    for b in range(batch):
        choice = np.random.choice(list(range(len(x))))
        gradient_sum += -(y[choice]-np.dot(w.T,x[choice,:]))*x[choice,:].reshape(np.shape(w))
        return eta*(1/batch)*gradient_sum

# Update w
w_s = []
Error = []
for i in range(iterations):
    # Calculate error
    error = (1/2)*np.sum([(y[i]-np.dot(w.T,x[i,:]))**2 for i in range(len(x))])
    Error.append(error)
    # Stochastic Gradient Descent
    """
    for d in range(len(x)):
        w-= stochastic_gradient_descent(x[d,:],y[d],w,eta)
        w_s.append(w.copy())
    """
    # Minibatch Gradient Descent
    """
    w-= mini_batch_gradient_descent(x,y,w,eta,batch)
    """

    # Batch Gradient Descent



# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
100/6:
# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
100/7:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)


def create_data():
    x = PolynomialFeatures(degree=2).fit_transform(np.linspace(-2,2,100).reshape(100,-1))
    x[:,1:] = MinMaxScaler(feature_range=(-2,2),copy=False).fit_transform(x[:,1:])
    l = lambda x_i: np.(0.8*x_i**2+0.5*x_i+3)
    data = l(x[:,1])
    noise = np.random.normal(0,0.1,size=np.shape(data))
    y = data+noise
    y= y.reshape(100,1)
    # Normalize Data
    return {'x':x,'y':y}

def plot_function(x,y,w,Error,w_s):
    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(40,10))
    ax[0].plot(x[:,1],[np.cos(0.8*np.pi*x_i) for x_i in x[:,1]],c='lightgreen',linewidth=3,zorder=0)
    ax[0].scatter(x[:,1],y)
    ax[0].plot(x[:,1],np.dot(x,w))
    ax[0].set_title('Function')
    ax[1].scatter(range(iterations),Error)
    ax[1].set_title('Error')



    plt.show()
99/30:
# First, we should declare a variable containing the size of the training set we want to generate.
observations = 100

# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.
# We have picked x and z, since it is easier to differentiate them.
# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).
# The size of xs and zs is observations by 1. In this case: 1000 x 1.
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
# zs = np.random.uniform(-10, 10, (observations,1))

# Combine the two dimensions of the input into one input matrix. 
# This is the X matrix from the linear model y = x*w + b.
# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.
inputs = np.column_stack((xs))

# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. 
# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.
print (inputs.shape)
99/31:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/32:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
100/8:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)


def create_data():
    x = PolynomialFeatures(degree=2).fit_transform(np.linspace(-2,2,100).reshape(100,-1))
    x[:,1:] = MinMaxScaler(feature_range=(-2,2),copy=False).fit_transform(x[:,1:])
    l = (0.8*x_i**2+0.5*x_i+3)
    data = l(x[:,1])
    noise = np.random.normal(0,0.1,size=np.shape(data))
    y = data+noise
    y= y.reshape(100,1)
    # Normalize Data
    return {'x':x,'y':y}

def plot_function(x,y,w,Error,w_s):
    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(40,10))
    ax[0].plot(x[:,1],[np.cos(0.8*np.pi*x_i) for x_i in x[:,1]],c='lightgreen',linewidth=3,zorder=0)
    ax[0].scatter(x[:,1],y)
    ax[0].plot(x[:,1],np.dot(x,w))
    ax[0].set_title('Function')
    ax[1].scatter(range(iterations),Error)
    ax[1].set_title('Error')



    plt.show()
100/9:
# initialize variables
data = create_data()
x = data['x']
y = data['y']
w = np.random.normal(size=(np.shape(x)[1],1))
eta = 0.1
iterations = 10000
batch = 10



def stochastic_gradient_descent(x,y,w,eta):
    derivative = -(y-np.dot(w.T,x))*x.reshape(np.shape(w))
    return eta*derivative


def batch_gradient_descent(x,y,w,eta):
    derivative = np.sum([-(y[d]-np.dot(w.T.copy(),x[d,:]))*x[d,:].reshape(np.shape(w)) for d in range(len(x))],axis=0)
    return eta*(1/len(x))*derivative


def mini_batch_gradient_descent(x,y,w,eta,batch):
    gradient_sum = np.zeros(shape=np.shape(w))
    for b in range(batch):
        choice = np.random.choice(list(range(len(x))))
        gradient_sum += -(y[choice]-np.dot(w.T,x[choice,:]))*x[choice,:].reshape(np.shape(w))
        return eta*(1/batch)*gradient_sum

# Update w
w_s = []
Error = []
for i in range(iterations):
    # Calculate error
    error = (1/2)*np.sum([(y[i]-np.dot(w.T,x[i,:]))**2 for i in range(len(x))])
    Error.append(error)
    # Stochastic Gradient Descent
    """
    for d in range(len(x)):
        w-= stochastic_gradient_descent(x[d,:],y[d],w,eta)
        w_s.append(w.copy())
    """
    # Minibatch Gradient Descent
    """
    w-= mini_batch_gradient_descent(x,y,w,eta,batch)
    """

    # Batch Gradient Descent
100/10:
# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
99/33:
# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.
import numpy as np

# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  
import matplotlib.pyplot as plt
99/34:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/35:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-1, 1, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/36:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/37:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-5, 5, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/38:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/39:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=2
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/40:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/41:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)
99/42:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
99/43:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.02
99/44: outputs.shape
99/45:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights_1)+np.dot(inputs,weights_2) + biases
99/46:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/47:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights_1)+np.dot(inputs,weights_2) + biases
99/48:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
99/49:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
99/50:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    output.shapes
99/51:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs.shapes
99/52:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs
99/53:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/54:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/55:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs
    ax.scatter(xs, outputs)
99/56:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/57:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs
    ax.scatter(xs, outputs)
99/58:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/59:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
99/60:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
99/61:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/62:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/63:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/64:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs
    ax.scatter(xs, outputs)
99/65:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/66:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.01

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/67:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/68:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/69:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.001

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/70:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/71:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/72:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
99/73:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/74:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/75:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 2

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/76:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/77:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/78:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 2

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/79:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/80:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs
99/81:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    outputs, targets
99/82:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    print(outputs, targets)
99/83:
   
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    print(outputs)
99/84: print(targets)
99/85:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 2

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/86:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/87:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights_1 = weights_1 - learning_rate * np.dot(inputs,deltas_scaled)
    weights_2 = weights_2 - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/88:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 2

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/89:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/90:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights_1 = weights_1 - learning_rate * np.dot(inputs,deltas_scaled)
    weights_2 = weights_2 - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/91:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights_1 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))
weights_2 = np.random.uniform(low=-init_range, high=init_range, size=(1, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights_1)
print (weights_2)
print (biases)

biases.shape
99/92:
# Set some small learning rate (denoted eta in the lecture). 
# 0.02 is going to work quite well for our example. Once again, you can play around with it.
# It is HIGHLY recommended that you play around with it.
learning_rate = 0.1
99/93:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights_1 = weights_1 - learning_rate * np.dot(inputs,deltas_scaled)
    weights_2 = weights_2 - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/94:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
99/95:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
100/11: #
100/12:
f=np.polyfit(x, y, 2)
p=np.polyld(f)
100/13:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)
100/14:
f=np.polyfit(x, y, 2)
p=np.polyld(f)
100/15:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from numpy import polyfit
100/16:
f=np.polyfit(x, y, 2)
p=np.polyld(f)
100/17:
f=np.polyfit(x, y, 3)
p=np.polyld(f)
99/96:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
99/97:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
p=np.polyld(f)
99/98:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
p=np.poly1d(f)
99/99:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
p=np.poly1d(f)
99/100:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
f
99/101:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
f=np.polyfit(xs, targets, 2)
99/102:
# We want to "make up" a function, use the ML methodology, and see if the algorithm has learned it.
# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>
noise = np.random.uniform(-10, 10, (observations,1))
alpha=5
beta=5
gamma=0.8
# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.
# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.
#targets = 13*xs + 7*zs - 12 + noise
targets= alpha + beta*xs + gamma*xs**2+noise

# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.
print (targets.shape)
#f=np.polyfit(xs, targets, 2)
99/103:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
99/104:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights_1 = weights_1 - learning_rate * np.dot(inputs,deltas_scaled)
    #weights_2 = weights_2 - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/105:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs.T,weights_1)+np.dot(inputs.T,weights_2) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(deltas ** 2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    #weights_1 = weights_1 - learning_rate * np.dot(inputs,deltas_scaled)
    weights_2 = weights_2 - learning_rate * np.dot(inputs,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
99/106:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights, biases)

# Note that they may be convergING. So more iterations are needed.
99/107:
# We print the weights and the biases, so we can see if they have converged to what we wanted.
# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.
print (weights_1, weights_2, biases)

# Note that they may be convergING. So more iterations are needed.
99/108:
# We print the outputs and the targets in order to see if they have a linear relationship.
# Again, that's not needed. Moreover, in later lectures, that would not even be possible.
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
101/1:
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
101/2:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
101/3:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
101/4:
import numpy as np
import matplotlib.pyplot as plt
101/5:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/6:
# transforming the data to include another axis
x = x[:, np.newaxis]
y = y[:, np.newaxis]
101/7: x
101/8: y
101/9:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(x)

model = LinearRegression()
model.fit(x_poly, y)
y_poly_pred = model.predict(x_poly)
101/10:
mse = np.sqrt(mean_squared_error(y,y_poly_pred))
r2 = r2_score(y,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(x, y, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color='m')
plt.show()
101/11:
rmse = np.sqrt(mean_squared_error(y,y_poly_pred))
r2 = r2_score(y,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(x, y, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color='m')
plt.show()
101/12:
import operator
rmse = np.sqrt(mean_squared_error(y,y_poly_pred))
r2 = r2_score(y,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(x, y, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color='m')
plt.show()
101/13:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) 
#+ np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
101/14:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
101/15:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y)
plt.show()
101/16:
np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)
plt.scatter(x,y, s=10)
plt.show()
99/109:
# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.
# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.
#targets = targets.reshape(observations,)

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111)

# Choose the axes.
ax.scatter(xs, targets, s=10)

# Set labels
ax.set_xlabel('xs')
#ax.set_ylabel('zs')
#ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
#ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
targets = targets.reshape(observations,1)
101/17:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.normal(-3, 3, 20)
plt.scatter(xs,target, s=10)
plt.show()
101/18:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.normal(-3, 3, 20)
plt.scatter(xs,target, s=10)
plt.show()
101/19:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.uniform(-10, 10, observations)
plt.scatter(xs,target, s=10)
plt.show()
101/20:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.uniform(-10, 10, observations)
print(target)
plt.scatter(xs,target, s=10)
plt.show()
101/21:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.uniform(-10, 10, observations)
target.shape
plt.scatter(xs,target, s=10)
plt.show()
101/22:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.uniform(-10, 10, observations)
print(target.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/23:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) + np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/24:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
target = alpha+beta*xs - gamma * (xs ** 2) 
#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/25:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/26:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/27:
# transforming the data to include another axis
x = x[:, np.newaxis]
y = y[:, np.newaxis]
101/28:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(x)

model = LinearRegression()
model.fit(x_poly, y)
y_poly_pred = model.predict(x_poly)
101/29:
import operator
rmse = np.sqrt(mean_squared_error(y,y_poly_pred))
r2 = r2_score(y,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(x, y, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color='m')
plt.show()
101/30:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='m')
plt.show()
101/31:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
101/32:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
xs
101/33:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
xs[0]
101/34:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
101/35:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/36:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/37:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
101/38:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/39:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
xs
101/40:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/41:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/42:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
xs, xs.shape
101/43:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/44:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
101/45:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
101/46:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/47:
# transforming the data to include another axis
xs = xs[:, np.newaxis]
target = target[:, np.newaxis]
xs, xs.shape
101/48:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
101/49:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/50:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/51:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/52:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
101/53:
import numpy as np
import matplotlib.pyplot as plt
101/54:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
101/55:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/56:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/57:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
101/58:
import numpy as np
import matplotlib.pyplot as plt
101/59:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
101/60:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
101/61:
polynomial_features= PolynomialFeatures(degree=3)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
101/62:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
102/1:
import numpy as np
import matplotlib.pyplot as plt
102/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
102/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
102/4:
polynomial_features= PolynomialFeatures(degree=1)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
102/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
103/1:
import numpy as np
import matplotlib.pyplot as plt
103/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
103/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
103/4:
polynomial_features= PolynomialFeatures(degree=4)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
103/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
104/1:
import numpy as np
import matplotlib.pyplot as plt
104/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
104/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
104/4:
polynomial_features= PolynomialFeatures(degree=10)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
104/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
105/1:
import numpy as np
import matplotlib.pyplot as plt
105/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
105/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
105/4:
polynomial_features= PolynomialFeatures(degree=100)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
105/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
106/1:
import numpy as np
import matplotlib.pyplot as plt
106/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
106/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
106/4:
polynomial_features= PolynomialFeatures(degree=1000)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
107/1:
import numpy as np
import matplotlib.pyplot as plt
107/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
107/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
107/4:
polynomial_features= PolynomialFeatures(degree=200)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
107/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
108/1:
import numpy as np
import matplotlib.pyplot as plt
108/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
108/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
108/4:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
108/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
109/1:
import numpy as np
import matplotlib.pyplot as plt
109/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
109/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
109/4:
polynomial_features= PolynomialFeatures(degree=20)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
109/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/1:
import numpy as np
import matplotlib.pyplot as plt
110/2:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs - gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/4:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/6:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/7:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/8:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/9:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/10:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/11:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/12:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/13:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/14:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/15:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/16:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/17:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/18: zs=xs**2
110/19: zs=xs*xs
110/20: zs=xs*xs
110/21: xs
110/22:
np.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/23:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/24:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/25:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/26: #de cuadratica a multilinear  alpha + betax + gammax2= alpha + beta*x + gamma*z
110/27: xs
110/28: type(xs)
110/29: xs.shape
110/30: xs[0]
110/31:
p.random.seed(0)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/32:

alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/33:
np.random.seed(45)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/34:
np.random.seed(45)
alpha=5
beta=5
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/35: inputs = np.column_stack((xs,zs))
110/36:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
110/37:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
print (targets.shape)
110/38:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
print (target.shape)
110/39:
from mpl_toolkits.mplot3d import Axes3D
targets = target.reshape(observations,)
110/40:
from mpl_toolkits.mplot3d import Axes3D
targets = target.reshape(observations,)
targets.shape
110/41:
from mpl_toolkits.mplot3d import Axes3D
Target_mod = target.reshape(observations,)
Target_mod.shape
110/42:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/43:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/44:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(112, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/45:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/46:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=150)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/47:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=200)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/48:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=90)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/49:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=180)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/50:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=0)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/51:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=1)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/52:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=3)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/53:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/54:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)
110/55:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.02
110/56:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/57:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/58:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/59:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.01
110/60:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/61:
np.random.seed(45)
alpha=5
beta=5
gamma=0.8
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/62:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3Dprint (target.shape)
110/63:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
110/64:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/65:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/66:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/67:
np.random.seed(45)
alpha=5
beta=5
gamma=0.8
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/68:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
110/69:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/70:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/71:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.01
110/72:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/73:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/74:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/75:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/76:
np.random.seed(45)
alpha=5
beta=3
gamma=4
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/77:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/78:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/79:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/80:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.01
110/81:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/82:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/83:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/84:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/85:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/86:
np.random.seed(45)
alpha=5
beta=3
gamma=4
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/87:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/88:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/89:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/90:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/91:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/92: print (weights, biases)
110/93:
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/94: print (weights, biases)
110/95:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/96:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/97:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/98:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/99: print (weights, biases)
110/100:
np.random.seed(45)
alpha=1
beta=3
gamma=4
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/101:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/102:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/103:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/104:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/105:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/106: print (weights, biases)
110/107:
plt.plot(outputs,targets)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/108:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/109:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/110:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/111:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.00001
110/112:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/113: print (weights, biases)
110/114:
np.random.seed(45)
alpha=1
beta=3
gamma=0.8
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/115:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/116:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/117:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/118:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.00001
110/119:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/120:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.00001
110/121:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/122: print (weights, biases)
110/123:
print (weights, biases)
print(beta, gamma, alfa)
110/124:
print (weights, biases)
print(beta, gamma, alpha)
110/125:
print (weights, biases)

print(beta, gamma, alpha)
110/126:
print (weights, biases)

print( alpha, beta, gamma)
110/127:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/128:
print (weights, biases)

print( alpha, beta, gamma)
110/129:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/130:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/131:
print (weights, biases)

print( alpha, beta, gamma)
110/132:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/133:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/134:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/135:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/136:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/137:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/138:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/139:
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/140:
print (weights, biases)

print( alpha, beta, gamma)
110/141:
for i in range (100000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/142:
print (weights, biases)

print( alpha, beta, gamma)
110/143:
print (weights, biases)

print( alpha, beta, gamma)
110/144:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/145:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/146:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/147:
print (weights, biases)

print( alpha, beta, gamma)
110/148:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/149:
np.random.seed(45)
alpha=1
beta=3
gamma=0.8
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/150:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/151:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/152:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/153:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/154:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/155:
print (weights, biases)

print( alpha, beta, gamma)
100/18:
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
np.random.seed(seed=42)


def create_data():
    x = PolynomialFeatures(degree=6).fit_transform(np.linspace(-2,2,100).reshape(100,-1))
    x[:,1:] = MinMaxScaler(feature_range=(-2,2),copy=False).fit_transform(x[:,1:])
    l = lambda x_i: np.cos(0.8*np.pi*x_i)
    data = l(x[:,1])
    noise = np.random.normal(0,0.1,size=np.shape(data))
    y = data+noise
    y= y.reshape(100,1)
    # Normalize Data
    return {'x':x,'y':y}

def plot_function(x,y,w,Error,w_s):
    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(40,10))
    ax[0].plot(x[:,1],[np.cos(0.8*np.pi*x_i) for x_i in x[:,1]],c='lightgreen',linewidth=3,zorder=0)
    ax[0].scatter(x[:,1],y)
    ax[0].plot(x[:,1],np.dot(x,w))
    ax[0].set_title('Function')
    ax[1].scatter(range(iterations),Error)
    ax[1].set_title('Error')



    plt.show()



# initialize variables
data = create_data()
x = data['x']
y = data['y']
w = np.random.normal(size=(np.shape(x)[1],1))
eta = 0.1
iterations = 10000
batch = 10



def stochastic_gradient_descent(x,y,w,eta):
    derivative = -(y-np.dot(w.T,x))*x.reshape(np.shape(w))
    return eta*derivative


def batch_gradient_descent(x,y,w,eta):
    derivative = np.sum([-(y[d]-np.dot(w.T.copy(),x[d,:]))*x[d,:].reshape(np.shape(w)) for d in range(len(x))],axis=0)
    return eta*(1/len(x))*derivative


def mini_batch_gradient_descent(x,y,w,eta,batch):
    gradient_sum = np.zeros(shape=np.shape(w))
    for b in range(batch):
        choice = np.random.choice(list(range(len(x))))
        gradient_sum += -(y[choice]-np.dot(w.T,x[choice,:]))*x[choice,:].reshape(np.shape(w))
        return eta*(1/batch)*gradient_sum

# Update w
w_s = []
Error = []
for i in range(iterations):
    # Calculate error
    error = (1/2)*np.sum([(y[i]-np.dot(w.T,x[i,:]))**2 for i in range(len(x))])
    Error.append(error)
    # Stochastic Gradient Descent
    """
    for d in range(len(x)):
        w-= stochastic_gradient_descent(x[d,:],y[d],w,eta)
        w_s.append(w.copy())
    """
    # Minibatch Gradient Descent
    """
    w-= mini_batch_gradient_descent(x,y,w,eta,batch)
    """

    # Batch Gradient Descent

    w -= batch_gradient_descent(x,y,w,eta)




# Show predicted weights
print(w_s)

# Plot the predicted function and the Error
plot_function(x,y,w,Error,w_s)
110/156:
np.random.seed(45)
alpha=1
beta=3
gamma=0.8
observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/157:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/158:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/159:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/160:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/161:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/162:
print (weights, biases)

print( alpha, beta, gamma)
110/163:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/164:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/165:
np.random.seed(0)
alpha=1
beta=-3
gamma=0.8
observations=20
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/166:
np.random.seed(42)
alpha=1
beta=-3
gamma=0.8
observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
110/167:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
110/168:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
110/169:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/170: #de cuadratica a multilinear  alpha + betax + gammax2= alpha + beta*x + gamma*z
110/171:
np.random.seed(42)

observations=200
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/172:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/173:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/174:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/175:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.plot(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/176:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/177:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-30)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/178:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-130)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/179:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/180:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/181:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/182:
print (weights, biases)

print( alpha, beta, gamma)
110/183:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate*2 * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/184:
print (weights, biases)

print( alpha, beta, gamma)
110/185:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/186:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate*2 * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/187:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0002
110/188:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate*2 * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/189:
print (weights, biases)

print( alpha, beta, gamma)
110/190:
plt.plot(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/191:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/192:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.5

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0002
110/193:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate*2 * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/194:
print (weights, biases)

print( alpha, beta, gamma)
110/195:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/196:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0002
110/197:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate*2 * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/198:
print (weights, biases)

print( alpha, beta, gamma)
110/199:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/200:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/201:
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/202:
print (weights, biases)

print( alpha, beta, gamma)
110/203:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/204:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/205:
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/206:
Loss=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/207:
print (weights, biases)

print( alpha, beta, gamma)

plt.scatter(Loss,i)
110/208:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/209:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/210:
print (weights, biases)

print( alpha, beta, gamma)

plt.scatter(Loss,Cuenta)
110/211:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/212:
print (weights, biases)

print( alpha, beta, gamma)

plt.scatter(Loss,Cuenta, s=5)
110/213:
print (weights, biases)

print( alpha, beta, gamma)

plt.scatter(Loss,Cuenta, s=5)
plt.xlabel('Loss')
plt.ylabel('iterations')
110/214:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Loss,Cuenta, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
110/215:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Loss,Cuenta, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/216:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
111/1:
# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.
# The proper number of iterations is something we will talk about later on, but generally
# a lower learning rate would need more iterations, while a higher learning rate would need less iterations
# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - targets
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)**2) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/217:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/218:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/219:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/220:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/221:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/222:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/223:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/224:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/225:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/226:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/227:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/228:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/229:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/230:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/231:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/232:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0006
110/233:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/234:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[0]
110/235:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0007
110/236:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/237:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/238:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0007
110/239:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/240:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/241:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0007
110/242:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/243:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/244:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/245:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/246:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/247:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/248:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0009
110/249:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/250:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/251:
Loss=[]
Cuenta=[]
for i in range (100):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/252:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/253:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.8

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/254:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/255:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/256:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/257:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/258:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/259:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/260:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/261:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/262:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/263:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, outputs = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
110/264:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, outputs = zip(*sorted_zip)
plt.plot(xs, outputs, color='r')
plt.show()
110/265:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

plt.plot(xs, outputs, color='r')
plt.show()
110/266:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/267:
plt.scatter(inputs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/268:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/269:
plt.scatter(xs, targets, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/270:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/271:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
xs
110/272:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
xs, targets
110/273:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
xs, target
110/274:
plt.scatter(xs, output, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/275:
plt.scatter(xs, outputs, s=10)
# sort the values of x before line plot

#plt.plot(xs, outputs, color='r')
plt.show()
110/276:
plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, outputs = zip(*sorted_zip)
plt.plot(xs,outputs, color='r')
plt.show()
110/277:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/278:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/279:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/280:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/281:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/282:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/283:
Loss=[]
Cuenta=[]
for i in range (100000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/284:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/285:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/286:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/287:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0009
110/288:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/289:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.01
110/290:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/291:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.01
110/292:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/293:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.001
110/294:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/295:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0007
110/296:
Loss=[]
Cuenta=[]
for i in range (1000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/297:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/298:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0007
110/299:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/300:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/301:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/302:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/303:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/304:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/305:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/306:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/307:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/308:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/309:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/310:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/311:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0005
110/312:
Loss=[]
Cuenta=[]
for i in range (100000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/313:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/314:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.9

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/315:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/316:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/317:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/318:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/319:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/320:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/321:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/322:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/323:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
inputs
110/324:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/325:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/326:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/327:
Loss=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/328:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/329:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/330:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/331:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/332:
Loss=[]
Loss2=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    loss2 = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Loss2.append(loss2)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/333:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-10, high=10, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/334:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
#inputs
110/335:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/336:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/337:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/338:
Loss=[]
Loss2=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    loss2 = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Loss2.append(loss2)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/339:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/340:
plt.scatter(Cuenta,Loss2, s=5)
plt.ylabel('Loss MSE')
plt.xlabel('iterations')
Loss2[-1]
110/341:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/342:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-5, high=5, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/343:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
#inputs
110/344:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/345:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/346:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/347:
Loss=[]
Loss2=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    loss2 = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Loss2.append(loss2)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/348:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/349:
plt.scatter(Cuenta,Loss2, s=5)
plt.ylabel('Loss MSE')
plt.xlabel('iterations')
Loss2[-1]
110/350:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/351:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/352:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
110/353:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
#inputs
110/354:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
110/355:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
110/356:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
110/357:
Loss=[]
Loss2=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    loss2 = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Loss2.append(loss2)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
110/358:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
110/359:
plt.scatter(Cuenta,Loss2, s=5)
plt.ylabel('Loss MSE')
plt.xlabel('iterations')
Loss2[-1]
110/360:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
110/361:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
110/362: plt.hist(target,10)
110/363:
plt.hist(target,10)
plt.show()
110/364:
plt.hist(target,100)
plt.show()
110/365:
plt.hist(target,1)
plt.show()
110/366:
%matplotlib
plt.hist(target,1)
plt.show()
110/367: help(plt)
112/1:
import numpy as np
import matplotlib.pyplot as plt
112/2:
np.random.seed(42)
alpha=1
beta=-3
gamma=0.8
observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * (xs ** 2)+noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
print(xs)
112/3:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
112/4:
polynomial_features= PolynomialFeatures(degree=2)
x_poly = polynomial_features.fit_transform(xs)

model = LinearRegression()
model.fit(x_poly, target)
y_poly_pred = model.predict(x_poly)
112/5:
import operator
rmse = np.sqrt(mean_squared_error(target,y_poly_pred))
r2 = r2_score(target,y_poly_pred)
print(rmse)
print(r2)

plt.scatter(xs, target, s=10)
# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(xs,y_poly_pred), key=sort_axis)
xs, y_poly_pred = zip(*sorted_zip)
plt.plot(xs, y_poly_pred, color='r')
plt.show()
112/6: #de cuadratica a multilinear  alpha + betax + gammax2= alpha + beta*x + gamma*z
112/7:
np.random.seed(42)

observations=100
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs=xs**2
#x = alpha - 3 * np.random.normal(0, 1, 20)
noise = np.random.uniform(low=-20, high=20, size=(observations,1))
target = alpha+beta*xs + gamma * zs +noise

#+ np.random.uniform(-10, 10, observations)
print(xs.shape)
plt.scatter(xs,target, s=10)
plt.show()
112/8:
inputs = np.column_stack((xs,zs))
print (inputs.shape)
from mpl_toolkits.mplot3d import Axes3D
print (target.shape)
#inputs
112/9:
from mpl_toolkits.mplot3d import Axes3D
target = target.reshape(observations,)
target.shape
112/10:

# Plotting according to the conventional matplotlib.pyplot syntax

# Declare the figure
fig = plt.figure()

# A method allowing us to create the 3D plot
ax = fig.add_subplot(111, projection='3d')

# Choose the axes.
ax.scatter(xs, zs, target)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100
# to azim = 0 ; azim = 200, or whatever. Check and see what happens.
ax.view_init(azim=-100)

# So far we were just describing the plot. This method actually shows the plot. 
plt.show()

# We reshape the targets back to the shape that they were in before plotting.
# This reshaping is a side-effect of the 3D plot. Sorry for that.
target = target.reshape(observations,1)
112/11:
# We will initialize the weights and biases randomly in some small initial range.
# init_range is the variable that will measure that.
# You can play around with the initial range, but we don't really encourage you to do so.
# High initial ranges may prevent the machine learning algorithm from learning.
init_range = 0.1

# Weights are of size k x m, where k is the number of input variables and m is the number of output variables
# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)
weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))

# Biases are of size 1 since there is only 1 output. The bias is a scalar.
biases = np.random.uniform(low=-init_range, high=init_range, size=1)

#Print the weights to get a sense of how they were initialized.
print (weights)
print (biases)

learning_rate = 0.0001
112/12:
Loss=[]
Loss2=[]
Cuenta=[]
for i in range (10000):
    
    # This is the linear model: y = xw + b equation
    outputs = np.dot(inputs,weights) + biases
    # The deltas are the differences between the outputs and the targets
    # Note that deltas here is a vector 1000 x 1
    deltas = outputs - target
        
    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.
    # Moreover, we further divide it by the number of observations.
    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,
    # as any function holding the basic property of being lower for better results, and higher for worse results
    # can be a loss function.
    loss = np.sum(np.absolute(deltas)) / 2 / observations
    loss2 = np.sum(np.absolute(deltas**2)) / 2 / observations
    
    # We print the loss function value at each step so we can observe whether it is decreasing as desired.
    print (loss)
    Loss.append(loss)
    Loss2.append(loss2)
    Cuenta.append(i)
    
    # Another small trick is to scale the deltas the same way as the loss function
    # In this way our learning rate is independent of the number of samples (observations).
    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate
    # that can remain the same if we change the number of training samples (observations).
    # You can try solving the problem without rescaling to see how that works for you.
    deltas_scaled = deltas / observations
    
    # Finally, we must apply the gradient descent update rules from the relevant lecture.
    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1
    # We must transpose the inputs so that we get an allowed operation.
    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)
    biases = biases - learning_rate * np.sum(deltas_scaled)
    
    # The weights are updated in a linear algebraic way (a matrix minus another matrix)
    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.
    # The two lines are both consistent with the gradient descent methodology.
112/13:
print (weights, biases)

print( alpha, beta, gamma)
plt.scatter(Cuenta,Loss, s=5)
plt.ylabel('Loss')
plt.xlabel('iterations')
Loss[-1]
112/14:
plt.scatter(Cuenta,Loss2, s=5)
plt.ylabel('Loss MSE')
plt.xlabel('iterations')
Loss2[-1]
112/15:
plt.scatter(outputs,target)
plt.xlabel('outputs')
plt.ylabel('targets')
plt.show()
112/16:

# sort the values of x before line plot
sort_axis = operator.itemgetter(0)
sorted_zip0 = sorted(zip(xs,target), key=sort_axis)
sorted_zip = sorted(zip(xs,outputs), key=sort_axis)
xs, target = zip(*sorted_zip0)
xs, outputs = zip(*sorted_zip)
plt.scatter(xs, target, s=10)
plt.plot(xs,outputs, color='r')
plt.show()
112/17:
%matplotlib
plt.hist(target,1)
plt.show()
112/18: help(plt)
112/19: help(plt)
113/1:
import numpy as np
import tensorflow as tf

# TensorFLow includes a data provider for MNIST that we'll use.
# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using
# pip install tensorflow-datasets 
# or
# conda install tensorflow-datasets

import tensorflow_datasets as tfds

# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer
113/2:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
validation_targets
113/3:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
validation_targets[0]
113/4:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
validation_targets[0][0]
113/5:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)

# determine the batch size
BATCH_SIZE = 100

# we can also take advantage of the occasion to batch the train data
# this would be very helpful when we train, as we would be able to iterate over the different batches
train_data = train_data.batch(BATCH_SIZE)

validation_data = validation_data.batch(num_validation_samples)

# batch the test data
test_data = test_data.batch(num_test_samples)


# takes next batch (it is the only batch)
# because as_supervized=True, we've got a 2-tuple structure
validation_inputs, validation_targets = next(iter(validation_data))
num_test_samples

train_data
validation_inputs
validation_targets[0]
113/6: validation_targets[0]
113/7: validation_targets[1]
113/8: validation_inputs
113/9: validation_inputs[0]
113/10: validation_inputs[0], validation_targets[0]
113/11: print(validation_inputs[0], validation_targets[0])
113/12: print(validation_inputs[0,1], validation_targets[0])
113/13:
#print(validation_inputs[0,1], validation_targets[0])
train_data
113/14:
#print(validation_inputs[0,1], validation_targets[0])
train_data[1]
113/15:
#print(validation_inputs[0,1], validation_targets[0])
train_data[0]
113/16:
#print(validation_inputs[0,1], validation_targets[0])
train_data
113/17:
# remember the comment from above
# these datasets will be stored in C:\Users\*USERNAME*\tensorflow_datasets\...
# the first time you download a dataset, it is stored in the respective folder 
# every other time, it is automatically loading the copy on your computer 

# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) 
# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument
# there are other arguments we can specify, which we can find useful
# mnist_dataset = tfds.load(name='mnist', as_supervised=True)
mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
# with_info=True will also provide us with a tuple containing information about the version, features, number of samples
# we will use this information a bit below and we will store it in mnist_info

# as_supervised=True will load the dataset in a 2-tuple structure (input, target) 
# alternatively, as_supervised=False, would return a dictionary
# obviously we prefer to have our inputs and targets separated 

# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']

# by default, TF has training and testing datasets, but no validation sets
# thus we must split it on our own

# we start by defining the number of validation samples as a % of the train samples
# this is also where we make use of mnist_info (we don't have to count the observations)
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
# let's cast this number to an integer, as a float may cause an error along the way
num_validation_samples = tf.cast(num_validation_samples, tf.int64)

# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)
num_test_samples = mnist_info.splits['test'].num_examples
# once more, we'd prefer an integer (rather than the default float)
num_test_samples = tf.cast(num_test_samples, tf.int64)


# normally, we would like to scale our data in some way to make the result more numerically stable
# in this case we will simply prefer to have inputs between 0 and 1
# let's define a function called: scale, that will take an MNIST image and its label
def scale(image, label):
    # we make sure the value is a float
    image = tf.cast(image, tf.float32)
    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)
    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 
    image /= 255.

    return image, label


# the method .map() allows us to apply a custom transformation to a given dataset
# we have already decided that we will get the validation data from mnist_train, so 
scaled_train_and_validation_data = mnist_train.map(scale)

# finally, we scale and batch the test data
# we scale it so it has the same magnitude as the train and validation
# there is no need to shuffle it, because we won't be training on the test data
# there would be a single batch, equal to the size of the test data
test_data = mnist_test.map(scale)


# let's also shuffle the data

BUFFER_SIZE = 10000
# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets
# then we can't shuffle the whole dataset in one go because we can't fit it all in memory
# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them
# if BUFFER_SIZE=1 => no shuffling will actually happen
# if BUFFER_SIZE >= num samples => shuffling is uniform
# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling

# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation
# our validation data would be equal to 10% of the training set, which we've already calculated
# we use the .take() method to take that many samples
# finally, we create a batch with a batch size equal to the total number of validation samples
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)

# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)
113/18:
#print(validation_inputs[0,1], validation_targets[0])
train_data
113/19:
#print(validation_inputs[0,1], validation_targets[0])
train_data[1]
113/20:
#print(validation_inputs[0,1], validation_targets[0])
train_data
113/21:
#print(validation_inputs[0,1], validation_targets[0])
train_data.shape()
113/22:
#print(validation_inputs[0,1], validation_targets[0])
help(batch)
113/23:
#print(validation_inputs[0,1], validation_targets[0])
help(shape)
116/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
116/2:
!conda install -c conda-forge wordcloud==1.4.1 --yes

# import package and its set of stopwords
from wordcloud import WordCloud, STOPWORDS

print ('Wordcloud is installed and imported!')
117/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
117/2: git clone https://github.com/amueller/word_cloud.git
117/3: from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
117/4:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
% matplotlib inline
117/5: df = pd.read_csv("data/winemag-data-130k-v2.csv", index_col=0)
117/6: df = pd.read_txt("prueba.txt")
117/7: df = open("dprueba.txt", "r")
117/8: df = open("prueba.txt", "r")
117/9: df
117/10:
# Start with one review:
text = df.description[0]

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
117/11:
# Start with one review:
#text = df.description[0]

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
117/12:
# Start with one review:
text = df.description[0]

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
117/13:
def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):
    h = int(360.0 * 45.0 / 255.0)
    s = int(100.0 * 255.0 / 255.0)
    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)

    return "hsl({}, {}%, {}%)".format(h, s, l)
117/14:
file_content=open ("prueba.txt").read()

wordcloud = WordCloud(font_path = r'C:\Windows\Fonts\Verdana.ttf',
                            stopwords = STOPWORDS,
                            background_color = 'white',
                            width = 1200,
                            height = 1000,
                            color_func = random_color_func
                            ).generate(file_content)

plt.imshow(wordcloud)
plt.axis('off')
plt.show()
117/15: $ wordcloud_cli --text prueba.txt --imagefile wordcloud.png
117/16:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):
   maskArray = npy.array(Image.open("cloud.png"))
   cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
   cloud.generate(string)
   cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/17:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):
    maskArray = npy.array(Image.open("cloud.png"))
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/18:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):
    maskArray = npy.array(Image.open("cloud.png"))
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/19:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):maskArray = npy.array(Image.open("cloud.png"))
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/20:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):maskArray = npy.array(Image.open("cloud.png")) 
cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
cloud.generate(string)
cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/21:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):maskArray = npy.array(Image.open("mega.jpeg")) 
cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
cloud.generate(string)
cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/22:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):maskArray = npy.array(Image.open("mega.jpg")) 
cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
cloud.generate(string)
cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/23:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):
    maskArray = npy.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/24:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): maskArray = npy.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/25:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): maskArray = npy.array(Image.open("mega.jpg")) cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/26:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): maskArray = npy.array(Image.open("mega.jpg")) cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/27:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): maskArray = npy.array(Image.open("mega.jpg")) cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) cloud.generate(string) cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/28:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): maskArray = npy.array(Image.open("mega.jpg")) cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) cloud.generate(string)cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/29:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string): 
    maskArray = npy.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/30: dataset = open("prueba.txt", "r").read()
117/31: dataset = open("prueba.txt", "r").read()
117/32: df = open("prueba.txt", "r")
117/33: df
117/34: dataset = open("prueba.txt", "r").read()
117/35: df[0]
117/36: df
117/37: dataset = open("prueba.txt", "r").read()
117/38: dataset = open("prueba.txt", "r").read()
117/39:
dataset = open("prueba.txt", "r").read()
defcreate_word_cloud(string):
117/40:
defcreate_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/41:
def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/42: dataset = open("prueba.txt", "r").read()
117/43: dataset = open("prueba.txt", "r")
117/44:
def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/45: dataset = open("prueba.txt", "r").read()
117/46:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/47:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/48:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "black", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/49:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "black", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/50:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 400, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/51:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 20, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
117/52:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
118/2:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
% matplotlib inline
118/3: df = open("prueba.txt", "r")
118/4: df
118/5:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS, "step")) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/6:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD). "step") 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/7:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD), "step") 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/8:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/9:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/10:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
118/11:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
118/12: df = open("prueba.txt", "r")
118/13: df
118/14:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD)) 
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/15:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/16:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
118/17:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
118/18:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORD))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/19:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/20: STOPWORDS
118/21: STOPWORDS=STOPWORDS.append("step")
118/22:
STOPWORDS
newStopWords = ['step','big','well']
STOPWORDS.extend(newStopWords)
118/23:
STOPWORDS
newStopWords = ['step','big','well']
118/24:
STOPWORDS.shape
newStopWords = ['step','big','well']
118/25:

newStopWords = ['step','big','well']
STOPWORDS
118/26:

newStopWords = ['step','big','well']
type(STOPWORDS)
118/27:

newStopWords = ['step','big','well']
STOPWORDS.add('step','big','well')
118/28:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
118/29:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS
118/30:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add(['big', 'well'])
118/31:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add('big')
118/32:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add('big')
STOPWORDS.add('well')
118/33:
dataset = open("prueba2.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/34:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/35:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add('big')
STOPWORDS.add('well')
STOPWORDS.add('segment')
118/36:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/37:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add('big')
STOPWORDS.add('well')
STOPWORDS.add('segment')
STOPWORDS.add('mainly')
STOPWORDS.add('way')
118/38:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/39:

newStopWords = ['step','big','well']
STOPWORDS.add('step')
STOPWORDS.add('big')
STOPWORDS.add('well')
STOPWORDS.add('segment')
STOPWORDS.add('mainly')
STOPWORDS.add('way')
STOPWORDS.add('market')
118/40:
dataset = open("prueba.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("mega.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/41:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
118/42:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
118/43: df = open("prueba.txt", "r")
118/44: df
118/45: #
118/46:
dataset = open("prueba23.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("swiss.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/47:
dataset = open("prueba23.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("swiss.jpeg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
118/48:
dataset = open("prueba23.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("swiss.jpeg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
120/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
120/2:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
120/3: df = open("OM.txt", "r")
120/4: df
120/5:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
120/6:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
121/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
from os import path
121/2:
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
#% matplotlib inline
121/3: df = open("OM.txt", "r")
121/4: df
121/5: #
121/6:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpeg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/7:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpeg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/8: df
121/9: #
121/10:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpeg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/11:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpeg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/12:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "black", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/13:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/14:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/15:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/16:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/17:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/18:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/19:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/20:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/21:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/22:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/23:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/24:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/25:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/26:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/27: STOPWORDS
121/28: STOPWORDS.append('ddd')
121/29: STOPWORDS
121/30: STOPWORDS.type
121/31: type(STOPWORDS)
121/32: newStopWords = ['Toyota','lala']
121/33: STOPWORDS.extend(newStopWords)
121/34: STOPWORDS
121/35: STOPWORDS.extend(newStopWords)
121/36: STOPWORDS.add(newStopWords)
121/37: ["https", "co", "RT"] + list(STOPWORDS)
121/38:
newStopWords = ['Toyota','lala']
STOPWORDS=STOPWORDS+newStopWords
121/39:
newStopWords = ['Toyota','lala']
STOPWORDS=STOPWORDS + ['Toyota','lala']
121/40:
newStopWords = ['Toyota','lala']
STOPWORDS2=STOPWORDS + ['Toyota','lala']
121/41:
newStopWords = ['Toyota','lala']
STOPWORDS2= list(STOPWORDS) + newStopWords
121/42: #
121/43:
newStopWords = ['Toyota','lala']
STOPWORDS= list(STOPWORDS) + newStopWords
121/44: #
121/45:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/46:
newStopWords = ['Toyota','processes']
STOPWORDS= list(STOPWORDS) + newStopWords
121/47: #
121/48:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 250, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/49:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 200, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/50:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/51:
newStopWords = ['Toyota','processes', 'to', 'related']
STOPWORDS= list(STOPWORDS) + newStopWords
121/52: #
121/53:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 150, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/54:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 149, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/55:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 130, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/56:
newStopWords = ['Toyota','processes', 'to', 'related', 'another']
STOPWORDS= list(STOPWORDS) + newStopWords
121/57: #
121/58:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 130, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/59:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 100, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/60:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work']
STOPWORDS= list(STOPWORDS) + newStopWords
121/61: #
121/62:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 100, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/63:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 50, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/64: type(STOPWORDS)
121/65:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work', 'look']
STOPWORDS= list(STOPWORDS) + newStopWords
121/66: #
121/67:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 50, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/68:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 45, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/69: type(STOPWORDS)
121/70:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work', 'look','may']
STOPWORDS= list(STOPWORDS) + newStopWords
121/71: #
121/72:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 45, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/73:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work', 'look','may','fully']
STOPWORDS= list(STOPWORDS) + newStopWords
121/74: #
121/75:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 45, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/76:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work', 'look','may','fully','way', 'beyond']
STOPWORDS= list(STOPWORDS) + newStopWords
121/77: #
121/78:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 45, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/79:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 46, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/80:
newStopWords = ['Toyota','processes', 'to', 'related', 'another', 'highly', 'work', 'look','may','fully','way', 'beyond', 'due']
STOPWORDS= list(STOPWORDS) + newStopWords
121/81:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 46, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
121/82:
dataset = open("OM.txt", "r").read()

def create_word_cloud(string):     
    maskArray = np.array(Image.open("factory.jpg")) 
    cloud = WordCloud(background_color = "white", max_words = 46, mask = maskArray, stopwords = set(STOPWORDS))
    cloud.generate(string)
    cloud.to_file("wordCloud.png")
dataset = dataset.lower()
create_word_cloud(dataset)
123/1: $pip install -U spacy
123/2: $ pip install -U spacy
123/3:
import sys
!{sys.executable} -m pip install spacy
!{sys.executable} -m spacy download en
124/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
124/2:
doc="""According to a media report, Mumbai has the highest density of cars in India. Pune is in second place. 

The density of private cars in Mumbai has gone up by 18% in 2 years. There are 510 cars per km of road as opposed to 430 cars per km in 2016. This is almost five times that of Delhi (108 cars per km). Despite having fewer cars than Delhi, Mumbai is more congested due to low road space. Mumbai has 2,000 km of roads compared to the national capital, which has 28,000 km of roadways.

There are 10.2 lakh private cars in Mumbai. That is 28% of the total number of vehicles in the city, which stands at 36 lakh. According to RTO officials, the western suburbs have the highest number of registered cars (5 lakh). There are 3.3 lakh private cars in the island city and 1.7 lakh in the eastern suburbs.

Pune has 359 cars per km and Kolkata is the third most congested city with 319 cars per km. Chennai comes in fourth with 297 cars per km followed by Bangalore with 149 cars per km."""
124/3:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/4:
doc="""According to a media report, Mumbai has the highest density of cars in India. Pune is in second place. 

The density of private cars in Mumbai has gone up by 18% in 2 years. There are 510 cars per km of road as opposed to 430 cars per km in 2016. This is almost five times that of Delhi (108 cars per km). Despite having fewer cars than Delhi, Mumbai is more congested due to low road space. Mumbai has 2,000 km of roads compared to the national capital, which has 28,000 km of roadways.

There are 10.2 lakh private cars in Mumbai. That is 28% of the total number of vehicles in the city, which stands at 36 lakh. According to RTO officials, the western suburbs have the highest number of registered cars (5 lakh). There are 3.3 lakh private cars in the island city and 1.7 lakh in the eastern suburbs.

Pune has 359 cars per km and Kolkata is the third most congested city with 319 cars per km. Chennai comes in fourth with 297 cars per km followed by Bangalore with 149 cars per km."""
124/5:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/6:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/7:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
124/8:
doc="""According to a media report, Mumbai has the highest density of cars in India. Pune is in second place. 

The density of private cars in Mumbai has gone up by 18% in 2 years. There are 510 cars per km of road as opposed to 430 cars per km in 2016. This is almost five times that of Delhi (108 cars per km). Despite having fewer cars than Delhi, Mumbai is more congested due to low road space. Mumbai has 2,000 km of roads compared to the national capital, which has 28,000 km of roadways.

There are 10.2 lakh private cars in Mumbai. That is 28% of the total number of vehicles in the city, which stands at 36 lakh. According to RTO officials, the western suburbs have the highest number of registered cars (5 lakh). There are 3.3 lakh private cars in the island city and 1.7 lakh in the eastern suburbs.

Pune has 359 cars per km and Kolkata is the third most congested city with 319 cars per km. Chennai comes in fourth with 297 cars per km followed by Bangalore with 149 cars per km."""
124/9:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/10:
import nltk
nltk.download()
124/11:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/12:
import nltk
nltk.download()
124/13:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/14:
import nltk
nltk.download()
124/15:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/16:
import nltk
nltk.download()
124/17:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/18:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/19:
doc="""According to a media report, Caracas has the highest density of cars in India. Pune is in second place. 

The density of private cars in Mumbai has gone up by 18% in 2 years. There are 510 cars per km of road as opposed to 430 cars per km in 2016. This is almost five times that of Delhi (108 cars per km). Despite having fewer cars than Delhi, Mumbai is more congested due to low road space. Mumbai has 2,000 km of roads compared to the national capital, which has 28,000 km of roadways.

There are 10.2 lakh private cars in Mumbai. That is 28% of the total number of vehicles in the city, which stands at 36 lakh. According to RTO officials, the western suburbs have the highest number of registered cars (5 lakh). There are 3.3 lakh private cars in the island city and 1.7 lakh in the eastern suburbs.

Pune has 359 cars per km and Kolkata is the third most congested city with 319 cars per km. Chennai comes in fourth with 297 cars per km followed by Bangalore with 149 cars per km."""
124/20:
import nltk
nltk.download()
124/21:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/22:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/23:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
124/24:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/25: city
124/26:
doc="""• The road from Paris to Patagonia, oddly
enough, once ran straight through the living
room of the iconic modernist designer Eileen
Gray. It was in the then 93 year old’s salon that
Bruce Chatwin, a young British journalist for the
Sunday Times Magazine, chanced upon a map of
that stretch of land at the southern tip of South
America, “the uttermost part of the earth,” as
he would come to describe it, which she had
painted. “I’ve always wanted to go there,” he
told her. “So have I,” she said, “go there for me.”
Two years later, in November 1974, Chatwin did
just that, flying to Lima and reaching Patagonia
a month after that. Upon arrival, he quit his
magazine job and spent the next six months
traveling up and down that desolate and sublime
landscape of bogs in Tierra del Fuego and the
glaciers beyond El Calafate, taking buses, hitchhiking and drifting, often walking for miles at
a time from one destination to the next. The
trip resulted in the instant classic In Patagonia,
his 1977 collection, so vivid and perceptive it
established his reputation among the greats of
travel writing virtually overnight.
Chatwin’s book, structured in 97 discreet
entries ranging in length from a single paragraph
to several pages, proceeds with minimal narrative momentum. In place of story and plot, he
provides intricate and evocative patterns: images
and motifs that appear and reappear and combine—like the terrain itself—into a sprawling
tapestry far greater than the sum of its individual parts. These episodes are usually tied to
a locale that Chatwin visits, which he has either
learned of from a native or passes through on the
way to somewhere else. Other times, his interest
has been piqued by a novel sight or ceremony.
However they’re sparked, the same incidents are
frequently revisited.
Chatwin himself compared his writing technique in In Patagonia to the photographic process
that seeks to frame large quantities of information, capturing and preserving only moments
of consequence. In keeping with his writeras-camera formulation, he consciously restricts
his own presence within the text. Instead, the
emphasis falls on the various people he encounters
and the residues left by the myths that saturate
this alien land with deeper human significance.
That would include the maneuverings of Butch
Cassidy and the Sundance Kid, legendary outlaws
who hid out in Patagonian no-man’s land after
robbing banks and terrorizing the Southwestern
United States. At other times, Chatwin searches
far and wide for unicorn fossils. There are also
colorful portraits of English sheep ranchers, the
descendants of Falkland Island kelpers, Scottish
shepherds and exiled Welsh nationalists who, in
1865, fled their failed independence movement.
It turns out his British countrymen’s presence in
Patagonia is extensive.
We are told of the mining concern with its
orders posted in English and Gaelic, of the homes
where cucumber sandwiches accompany afternoon tea, of The Magellan Times, the Bank of
London and South America and the British
Club. We meet a rancher from Scotland who
wears kilts and plays the bagpipes, an English
lady who grows strawberries in Tierra del Fuego,
and another across the Chilean border who, when
Allende seized power, was evicted from a farm
her family had owned for generations. Recalling
the fall of that regime, she tells Chatwin: “There
was a bit of shooting in the morning, and by
afternoon they had all the Marxists rounded up.
It was beautifully done.”
Drawing inspiration from Chatwin’s singular
but by no means exhaustive tour, the New
Zealand-born photographer Derek Henderson
—another subject of her majesty—set out with
his Mamiya 67 and 4 x 5 field camera, using
rich color negatives to retrace and recreate the
essential patterns from Chatwin’s masterpiece,
resulting in this lush series of images for Holiday.
“I started driving from Bahia Blanca and ended
up in Tierra Del Fuego,” Henderson recalls. “We
drove as far south as you can go in a vehicle in
the world—to the end of the earth, so to speak.”
And with these postcards from the end of the
world—eternal images of rock, ice, water, grass
and sunlight, as well as all variety of testament
to the resilience of human civilization and life—
Henderson has fastidiously captured the spirit of
In Patagonia, its conflicting sense of adventure
and boredom on the road, the lonely rhythms
of walking and riding buses and even hitching
rides on trucks. In turn, he has visited many of
the same places Chatwin saw first. But he has
also made the region—in its rugged expanse, in
its pristine blue skies, red sunsets and dawns and
straight, unceasing roads—something new and
entirely his own. ."""
124/27:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/28:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/29:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/30: city
124/31:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/32:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
124/33:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/34: city
124/35:
doc="""Hey fellow travellers,

I’m a young female from Czech Republic.

I came to New Zealand with Working Holiday Visa, thinking I’ll travel a little and then find a hostel job. I manage to do the first part and travel around the North island of NZ. While I was living in my van I was constantly dropping my CV’s around the country, but didn’t have any luck with finding any hostel position.

Another factor was that hostels in NZ are not a really big thing ’cause everyone travels in their vans.

Anyway, I decided to settle in one of the towns I really liked and admitted I might have to lower my expectations and find another job.

I lived in a hostel for while tried to impress few more mangers but no luck again. At that time things got really bad in Italy and people started to talk about Covid much more.

I printed approx. 20 CV’s and dropped them personally in almost every cafe, bar, restaurant and diner in area I moved into. Luckily I found a job in a Burger diner just 2 weeks before the lock-down. I got a job, moved to a house and everything seem to be on a stable path at that moment. After a week working first cases of Covid showed up in NZ and that was the start.

I just finished my second week of working when the PM of NZ announced that we will move to Alert 4, meaning everything apart essential businesses have to shut down. So after 2-month long hunt for a job and only 2 weeks working I was unemployed again. Luckily I had a house to stay in and amazing people around me.

NZ’s government also released a wage subsidy for those effected and that was another lucky moment that allowed me to stay in this amazing country. But the money I received barely covered my rent. SO job hunt again!

Before I came to NZ I said that I really don’t want to work in any kind of farm or orchard. Where do you think I work now? Yep the universe gave me a lesson again and here I am working as a kiwi picker.

Let me tell you it’s a hard job, I don’t like it at all, but what I realized is that I should be grateful for all of it. There’s so many people out there that have no chance of any kind of income that would do anything to accept any kind of job.

I complain, I cry but I also consider myself lucky at this stage."""
124/36:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/37:
doc="""Hey fellow travellers,

I’m a young female from Czech Republic.

I came to New Zealand with Working Holiday Visa, thinking I’ll travel a little and then find a hostel job. I manage to do the first part and travel around the North island of NZ. While I was living in my van I was constantly dropping my CV’s around the country, but didn’t have any luck with finding any hostel position.

Another factor was that hostels in NZ are not a really big thing ’cause everyone travels in their vans.

Anyway, I decided to settle in one of the towns I really liked and admitted I might have to lower my expectations and find another job.

I lived in a hostel for while tried to impress few more mangers but no luck again. At that time things got really bad in Italy and people started to talk about Covid much more.

I printed approx. 20 CV’s and dropped them personally in almost every cafe, bar, restaurant and diner in area I moved into. Luckily I found a job in a Burger diner just 2 weeks before the lock-down. I got a job, moved to a house and everything seem to be on a stable path at that moment. After a week working first cases of Covid showed up in NZ and that was the start.

I just finished my second week of working when the PM of NZ announced that we will move to Alert 4, meaning everything apart essential businesses have to shut down. So after 2-month long hunt for a job and only 2 weeks working I was unemployed again. Luckily I had a house to stay in and amazing people around me.

NZ’s government also released a wage subsidy for those effected and that was another lucky moment that allowed me to stay in this amazing country. But the money I received barely covered my rent. SO job hunt again!

Before I came to NZ I said that I really don’t want to work in any kind of farm or orchard. Where do you think I work now? Yep the universe gave me a lesson again and here I am working as a kiwi picker.

Let me tell you it’s a hard job, I don’t like it at all, but what I realized is that I should be grateful for all of it. There’s so many people out there that have no chance of any kind of income that would do anything to accept any kind of job.

I complain, I cry but I also consider myself lucky at this stage."""
124/38:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/39:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/40:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
124/41:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/42:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/43: city
124/44: doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
124/45:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
124/46:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
124/47:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
124/48:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
124/49: city
124/50:
import nltk
nltk.download()
#a bajar punkt - tagger y
124/51: import spacy
124/52:
import spacy
import spacy.displacy as displacy
124/53:
import nltk
nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
125/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
125/2:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
125/3: doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
125/4:
import nltk
nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
125/5:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
125/6:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)ù
name=ne_chunk(pos)
125/7:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
125/8:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
125/9:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
125/10:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
125/11: city
125/12:
import spacy
import spacy.displacy as displacy
125/13:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy("en_core_web_sm")
125/14:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
125/15: print(nlp)
125/16:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("es_core_web_sm")
125/17: print(nlp)
125/18:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("es_core_news_sm")
125/19: print(nlp)
125/20:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
125/21: print(nlp)
125/22: doc2=nlp(doc)
125/23: doc2
125/24: doc.ents
125/25: doc.ent
125/26: doc2.ent
125/27: doc2.ents
125/28:
for ent in doc2.ents:
    print(ent.text)
125/29:
for ent in doc2.ents:
    print(ent.text, ent.start)
125/30:
for ent in doc2.ents:
    print(ent.text, ent.start_char)
125/31:
for ent in doc2.ents:
    print(ent.text, ent.start)
125/32:
for ent in doc2.ents:
    print(ent.text, ent.start, ent.end)
125/33:
for ent in doc2.ents:
    print(ent.text, ent.start, ent.end_char)
125/34:
for ent in doc2.ents:
    print(ent.text, ent.start_char, ent.end_char)
125/35:
for ent in doc2.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label)
125/36:
for ent in doc2.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
125/37:
for ent in doc2.ents:
    print(ent.text, ent.label_)
125/38: displacy.serve(doc2)
125/39: displacy.serve(doc2, style="ent")
125/40:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_md")
126/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
126/2: doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
126/3:
import nltk
nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
126/4:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
126/5:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
126/6:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
126/7:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
126/8: city
126/9:
import spacy
import spacy.displacy as displacy
126/10:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_md")
126/11:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
126/12:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_lg")
126/13:
import spacy
import spacy.displacy as displacy
import en_core_web_md
126/14:
import spacy
import spacy.displacy as displacy
126/15:
python -m venv .env
source .env/bin/activate
pip install -U spacy
python -m spacy download en_core_web_lg
126/16:

pip install -U spacy
python -m spacy download en_core_web_lg
126/17:
$ pip install -U spacy
python -m spacy download en_core_web_lg
126/18:
import spacy
import spacy.displacy as displacy
126/19:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_lg")
126/20:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
126/21:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_md")
126/22: conda install -c conda-forge spacy-model-en_core_web_lg
126/23:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_lg")
126/24:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_lg")
126/25: conda install -c conda-forge/label/cf202003 spacy-model-en_core_web_l
127/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
127/2: doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
127/3:
import nltk
#nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
127/4:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
127/5:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
127/6:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
127/7:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
127/8: city
127/9:
import spacy
import spacy.displacy as displacy
127/10: #conda install -c conda-forge spacy-model-en_core_web_lg
127/11: #conda install -c conda-forge/label/cf202003 spacy-model-en_core_web_l
127/12:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_lg")
127/13: print(nlp)
127/14:
doc2=nlp(doc)
#Create doc object
127/15: doc2
127/16: doc2.ents
127/17:
for ent in doc2.ents:
    print(ent.text, ent.label_)
127/18: displacy.serve(doc2)
127/19: displacy.serve(doc2, style="ent")
127/20:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
127/21:
import spacy
import spacy.displacy as displacy
127/22: #conda install -c conda-forge spacy-model-en_core_web_lg
127/23: #conda install -c conda-forge/label/cf202003 spacy-model-en_core_web_l
127/24:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
127/25: print(nlp)
127/26:
doc2=nlp(doc)
#Create doc object
127/27: doc2
127/28: doc2.ents
127/29:
for ent in doc2.ents:
    print(ent.text, ent.label_)
127/30: displacy.serve(doc2)
127/31: displacy.serve(doc2, style="ent")
127/32: nlp=spacy.load("en_core_web_lg")
127/33: nlp=spacy.load("en_core_web_lg")
127/34: doc3=nlp(doc)
127/35:
for ent in doc2.ents:
    print(ent.text, ent.label_)
127/36:
for ent in doc3.ents:
    print(ent.text, ent.label_)
127/37: displacy.serve(doc3, style="ent")
127/38:
#EJEMPLO TEXTO
doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
127/39:
for ent in doc3.ents:
    print(ent.text, ent.label_GPE)
127/40:
for ent in doc3.ents:
    print(ent.text, ent.label_)
127/41: ent.label_
127/42: doc.ents.label_
127/43:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_="GPE":
        lugares.append(ent.text)
127/44:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE":
        lugares.append(ent.text)
127/45: lugares
127/46:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" and  ent.label_=="LOC":
        lugares.append(ent.text)
127/47: lugares
127/48:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
127/49: lugares
127/50:
pip install geopandas
pip install geopy
128/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
128/2:
#EJEMPLO TEXTO
doc="""Germignaga is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
128/3:

import nltk
#nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
128/4:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
128/5:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
128/6:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
128/7:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
128/8: city
128/9:
import spacy
import spacy.displacy as displacy #Text Visualization
128/10: #conda install -c conda-forge spacy-model-en_core_web_lg
128/11: #conda install -c conda-forge/label/cf202003 spacy-model-en_core_web_l
128/12:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
128/13: print(nlp)
128/14:
doc2=nlp(doc)
#Create doc object
128/15: doc2
128/16: doc2.ents
128/17:
for ent in doc2.ents:
    print(ent.text, ent.label_)
128/18:
displacy.serve(doc2)
#sentence and its dependencies
128/19: displacy.serve(doc2, style="ent")
128/20: nlp=spacy.load("en_core_web_lg")
128/21: doc3=nlp(doc)
128/22:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
128/23: lugares
128/24: displacy.serve(doc3, style="ent")
128/25: ent.label_
128/26:
#EJEMPLO TEXTO
doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
128/27:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
128/28:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
128/29:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
128/30:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
128/31:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
128/32: city
128/33:
doc2=nlp(doc)
#Create doc object
128/34: doc2
128/35: doc2.ents
128/36:
for ent in doc2.ents:
    print(ent.text, ent.label_)
128/37:
displacy.serve(doc2)
#sentence and its dependencies
128/38: displacy.serve(doc2, style="ent")
128/39: doc3=nlp(doc)
128/40: doc3=nlp(doc)
128/41:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
128/42: lugares
128/43: displacy.serve(doc3, style="ent")
128/44: ent.label_
128/45: lugares
128/46:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
129/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
129/2:
#EJEMPLO TEXTO
doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
129/3:
import spacy
import spacy.displacy as displacy #Text Visualization
129/4:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
129/5: print(nlp)
129/6:
doc2=nlp(doc)
#Create doc object
129/7: doc2
129/8: doc2.ents
129/9:
for ent in doc2.ents:
    print(ent.text, ent.label_)
129/10:
displacy.serve(doc2)
#sentence and its dependencies
129/11: nlp=spacy.load("en_core_web_lg")
129/12: nlp=spacy.load("en_core_web_lg")
129/13: doc3=nlp(doc)
129/14:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
129/15: lugares
129/16: displacy.serve(doc3, style="ent")
129/17: ent.label_
129/18: lugares
129/19:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
130/1:
import Numpy as np
import Pandas as pd
130/2:
import numpy as np
import Pandas as pd
130/3:
import numpy as np
import pandas as pd
130/4: !conda install -c conda-forge folium=0.5.0 --yes
130/5: import folium
130/6:
# define the world map
world_map = folium.Map()

# display world map
world_map
130/7:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[56.130, -106.35], zoom_start=4)

# display world map
world_map
130/8:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[56.130, 10], zoom_start=4)

# display world map
world_map
130/9:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -48], zoom_start=4)

# display world map
world_map
130/10:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -48], zoom_start=8)

# display world map
world_map
130/11:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -50], zoom_start=8)

# display world map
world_map
130/12:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8)

# display world map
world_map
130/13:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Stamen Toner')

# display world map
world_map
130/14:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Stamen Terrain')

# display world map
world_map
130/15:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Mapbox Bright')

# display world map
world_map
130/16:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Mapbox Control Room')

# display world map
world_map
130/17:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
130/18:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Mapbox Bright')

# display world map
world_map
130/19:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='MapboxBright')

# display world map
world_map
130/20:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Stamen Terrain')

# display world map
world_map
130/21:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='Stamen Terrain')

# display world map
world_map
130/22:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
131/1:
import numpy as np
import pandas as pd
132/1:
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
132/2:
#EJEMPLO TEXTO
doc="""Venezuela is a country on the northern coast of South America with diverse natural attractions. Along its Caribbean coast are tropical resort islands including Isla de Margarita and the Los Roques archipelago. To the northwest are the Andes Mountains and the colonial town of Mérida, a base for visiting Sierra Nevada National Park. Caracas, the capital, is to the north"""
132/3:

import nltk
#nltk.download()
#a bajar punkt - average perceptron tagger - ACE Entity chunker
132/4:
words=word_tokenize(doc)
pos=nltk.pos_tag(words)
name=ne_chunk(pos)
132/5:
loc=[]
for na in str(name).split('\n'):
    if '/NNP'in na:
        loc.append(na)
        print(na)
132/6:
import re 
i=0
town=[]
while i<len(loc): 
    pattern=r'\w+/NNP'
    s=re.findall(pattern,loc[i])
    print(s)
    if s not in town:
        town.append(s)
    i=i+1
132/7:
city=[]
for l in town:
    z=str(l)
    w=z.split('/')
    az=w[0]
    rr=az[2:]
    city.append(rr)
132/8: city
132/9:
import spacy
import spacy.displacy as displacy #Text Visualization
132/10: #conda install -c conda-forge spacy-model-en_core_web_lg
132/11: #conda install -c conda-forge/label/cf202003 spacy-model-en_core_web_l
132/12:
#Calling the nlp object on a string of text will return a processed Doc

nlp=spacy.load("en_core_web_sm")
132/13: print(nlp)
132/14:
doc2=nlp(doc)
#Create doc object
132/15: doc2
132/16: doc2.ents
132/17:
for ent in doc2.ents:
    print(ent.text, ent.label_)
132/18:
displacy.serve(doc2)
#sentence and its dependencies
132/19: nlp=spacy.load("en_core_web_lg")
132/20: nlp=spacy.load("en_core_web_lg")
132/21: doc3=nlp(doc)
132/22:
lugares=[]
for ent in doc3.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
132/23: lugares
132/24: lugares
132/25:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
132/26: np.save(lugares)
132/27: savetxt('lugares.csv', lugares)
132/28:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

from numpy import savetxt
132/29: savetxt('lugares.csv', lugares)
132/30:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from numpy import asarray
from numpy import savetxt
132/31: savetxt('lugares.csv', lugares)
132/32: savetxt('lugares.csv', lugares, delimiter=',')
132/33:
asarray(lugares)
savetxt('lugares.csv', lugares, delimiter=',')
132/34:
lugares=asarray(lugares)
savetxt('lugares.csv', lugares, delimiter=',')
132/35:
lugares=asarray(lugares)
np.savetxt('lugares.csv', lugares, delimiter=',')
132/36:
lugares=np.asarray(lugares)
np.savetxt('lugares.csv', lugares, delimiter=',')
132/37: np.savetxt('lugares.csv', lugares, delimiter=',')
132/38: np.save('lugares.csv', lugares)
131/2:
import numpy as np
import pandas as pd
131/3: import folium
131/4:
# define the world map
world_map = folium.Map()

# display world map
world_map
131/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
131/6: lugares=np.load("lugares.csv")
131/7: lugares=np.load('lugares.csv')
131/8: lugares2=np.load('lugares.csv')
131/9: lugares2=pd.read_csv('lugares.csv')
131/10: lugares2=np.load('lugares.csv')
131/11: import googlemaps as gp
131/12: import googlemaps
131/13: lugares2=pd.csv_read('lugares.csv')
131/14:
import numpy as np
import pandas as pd
131/15: lugares2=pd.csv_read('lugares.csv')
132/39: np.save('lugares.csv', lugares)
132/40: np.save('lugares.csv', lugares)
132/41: np.save('lugares', lugares)
131/16: lugares2=np.load('lugares.npy')
131/17: lugares2
131/18: $ pip install -U googlemaps
131/19: pip install -U googlemaps
131/20: import googlemaps
131/21: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
131/22:
lugares2["LAT"]=none
lugares2["LON"]=none
131/23:
lugares2["LAT"]=None
lugares2["LON"]=None
131/24: len(lugares2)
134/1:
import numpy as np
import pandas as pd
134/2: #!conda install -c conda-forge folium=0.5.0 --yes
134/3: import folium
134/4:
# define the world map
world_map = folium.Map()

# display world map
world_map
134/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
134/6: lugares2=np.load('lugares.npy')
134/7: lugares2
134/8: pip install -U googlemaps
134/9: import googlemaps
134/10: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
134/11: for i in range(len(lugares2)):
134/12: import googlemaps
134/13: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
134/14: len(lugares2)
134/15:
for i in range(len(lugares2)):
    geocode_result=g_key.geocode(lugares2[i])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares2.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares2.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/16: lugares2
134/17: lugares3=pd.DataFrame(lugares2)
134/18: lugares3
134/19:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3[i])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/20:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/21: range(0, len(lugares3), 1)
134/22: lugares3[0,0]
134/23: lugares3[1,0]
134/24: lugares3.iat[0,0]
134/25:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/26:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/27:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares2.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares2.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/28: lugares3
134/29:
for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/30: lugares3
134/31:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/32: range(0, len(lugares3), 1)
134/33: lugares3
134/34: print(geocode_result[0])
134/35:
world_map = folium.Map(location=[11.8567, -66.7665], zoom_start=10, tiles='OpenStreetMap')

# display world map
world_map
134/36:
world_map = folium.Map(location=[11.8567, -66.7665], zoom_start=10, tiles='OpenStreetMap')

# display world map
world_map
134/37:
world_map = folium.Map(location=[11.8567, -66.7665], zoom_start=10, tiles='OpenStreetMap')

# display world map
world_map
134/38: lugares3[1,0]
134/39: lugares3[1,1]
134/40: lugares3(1,1)
134/41: lugares3
134/42: lugares3["LAT"]
134/43: lugares3["LAT"][0]
134/44:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )
134/45:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )


Area.add(Dots)
134/46:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )


Area.add(Dots)
134/47:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )


Area.add_child(Dots)
134/48:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )


Area.add_child(Dots)
134/49: lugares3["0"]
134/50: lugares3[0]
134/51: lugares3[0][0]
134/52: lugares3[0]
134/53:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/54:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
134/55:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
136/1:
from epub_conversion.utils import open_book, convert_epub_to_lines

book = open_book("twilight.epub")

lines = convert_epub_to_lines(book)
136/2: !pip install ebooklib
136/3: !pip install BeautifulSoup4
136/4:
import ebooklib
from ebooklib import epub
epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
136/5:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
136/6: from bs4 import BeautifulSoup
136/7:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
136/8:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
136/9:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
136/10:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
136/11:
Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text(Book)
136/12:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text("C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
)
136/13:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text('C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')
136/14:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')
136/15: out
136/16:
import spacy
import spacy.displacy as displacy #Text Visualization
136/17: nlp=spacy.load("en_core_web_lg")
136/18: Book=nlp(out)
136/19: out.type
136/20: len(out)
136/21: type(out)
136/22: out[0]
136/23: out[1]
136/24: out[7]
136/25: out[6]
136/26: out[5]
136/27: out[4]
136/28: out[4][0]
136/29: out[4][1]
136/30: out[4][10]
136/31:
lugares=[]
for ent in out.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
136/32:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
print(listToString(out))
136/33:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
136/34: Book2=nlp(Book)
136/35:
lugares=[]
for ent in out.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
136/36:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC":
        lugares.append(ent.text)
136/37: lugares
136/38:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
136/39: lugares
136/40: displacy.serve(Book2, style="ent")
136/41:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
136/42: np.save('lugares_Alchimist', lugares)
134/56: lugares2=np.load('lugares_Alchimist.npy')
134/57: lugares2
134/58: lugares3=pd.DataFrame(lugares2)
134/59: lugares3
134/60:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
134/61: range(0, len(lugares3), 1)
134/62: lugares3
134/63: print(geocode_result[0])
134/64:
world_map = folium.Map(location=[11.8567, -66.7665], zoom_start=10, tiles='OpenStreetMap')

# display world map
world_map
134/65:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/66:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/67: lugares3["LAT"][0]
134/68: lugares3[0]
134/69:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
134/70:
world_map = folium.Map(location=[26.8206, 30.8025], zoom_start=10, tiles='OpenStreetMap')

# display world map
world_map
134/71:
world_map = folium.Map(location=[26.8206, 30.8025], zoom_start=2, tiles='OpenStreetMap')

# display world map
world_map
134/72:
world_map = folium.Map(location=[26.8206, 30.8025], zoom_start=4, tiles='OpenStreetMap')

# display world map
world_map
134/73:
world_map = folium.Map(location=[26.8206, 30.8025], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
134/74:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=1
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/75:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/76:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
134/77: import folium
134/78:
# define the world map
world_map = folium.Map()

# display world map
world_map
134/79:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
134/80: lugares2=np.load('lugares_Alchimist.npy')
134/81: lugares2
134/82: #pip install -U googlemaps
134/83: lugares3
134/84: print(geocode_result[0])
134/85:
world_map = folium.Map(location=[26.8206, 30.8025], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
134/86:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/87: lugares3["LAT"][0]
134/88: lugares3[0]
134/89:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
134/90:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/91:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/92:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/93:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
134/94:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
134/95: lat
134/96: lng
134/97: labels
137/1:
import numpy as np
import pandas as pd
137/2: #!conda install -c conda-forge folium=0.5.0 --yes
137/3: import folium
137/4:
# define the world map
world_map = folium.Map()

# display world map
world_map
137/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
137/6: lugares2=np.load('lugares_Alchimist.npy')
137/7: lugares2
137/8: #pip install -U googlemaps
137/9: import googlemaps
137/10: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
137/11: lugares3=pd.DataFrame(lugares2)
137/12: lugares3
137/13:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
137/14: range(0, len(lugares3), 1)
137/15: lugares3
137/16: print(geocode_result[0])
137/17:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
137/18:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"],lugares3["LON"]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"])
longitudes = list(lugares3["LON"])
labels = list(lugares3[0])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/19: labels
137/20: lugares3[0]
137/21:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugares3[0]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
137/22: lugares3["LAT"][:,10]
137/23: lugares3["LAT"][: 10]
137/24: lugares3["LAT"][: 100]
137/25: lugares3["LAT"][: 20]
137/26:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=4)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: 20],lugares3["LON"][: 20]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: 20])
longitudes = list(lugares3["LON"][: 20])
labels = list(lugares3[0][: 20])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/27:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()

# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: 20],lugares3["LON"][: 20]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: 20])
longitudes = list(lugares3["LON"][: 20])
labels = list(lugares3[0][: 20])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/28:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=50
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/29:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=30
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/30:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
137/31:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0][: ff], lugares3["LON"][0][: ff]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
137/32:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0][: ff], lugares3["LON"][0][: ff]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
137/33:
from folium import plugins
ff=30
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0][: ff], lugares3["LON"][0][: ff]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
137/34:
from folium import plugins
ff=30
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
137/35:
from folium import plugins
ff=30
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
139/1: !pip install pypdf2
141/1: import pypdf2
141/2: import pyPDF2
141/3: import PyPDF2 as pdf
141/4: pdf_imported=open("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf", 'rb')
141/5:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

pdf_reader=pdf(pdf_imported)
141/6:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

pdf_reader=pdf.PdfFileReader(pdf_imported)
141/7:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

pdf_reader=pdf.PdfFileReader(pdf_imported)
print(pdf_reader.NumPages)
141/8:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

pdf_reader=pdf.PdfFileReader(pdf_imported)
print(pdf_reader.numPages)
141/9:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

pdf_reader=pdf.PdfFileReader(pdf_imported)
paginas=pdf_reader.numPages
print(paginas)
141/10: print(pdf_reader)
141/11: print(pdf_reader.getPage(0))
141/12: page=pdf_reader.getPage(0)
141/13: page=pdf_reader.getPage(0)
141/14: text=page.extractText()
141/15:
text=page.extractText()
text
141/16: page=pdf_reader.getPage(5)
141/17:
text=page.extractText()
text
141/18: page=pdf_reader.getPage(50)
141/19:
text=page.extractText()
text
141/20:
#!pip install pypdf2
!pip install tika
141/21:
import PyPDF2 as pdf
from tika import parser
141/22:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

#pdf_reader=pdf.PdfFileReader(pdf_imported)
#paginas=pdf_reader.numPages
#print(paginas)
File="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf"
raw = parser.from_file(File)
print(raw['content'])
141/23:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

#pdf_reader=pdf.PdfFileReader(pdf_imported)
#paginas=pdf_reader.numPages
#print(paginas)
File="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf"
raw = parser.from_file(File)
print(raw['content'])
141/24:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import io
import os
#from tika import parser

def extract_text(file):
    parsed = parser.from_file(file)
    parsed_text = parsed['content']
    parsed_text = parsed_text.lower()
    return parsed_text

File="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf"
#raw = parser.from_file(File)
#print(raw['content'])
#file_name_with_extension = input("Enter File Name:")
text = extract_text(File)
print(text)
#out=epub2text('Coelho, Paulo - The Alchemist (0) - libgen.lc.epub')

#pdf_reader=pdf.PdfFileReader(pdf_imported)
#paginas=pdf_reader.numPages
#print(paginas)
141/25:
#blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.

tika.initVM()
141/26:
import PyPDF2 as pdf
from tika import parser
141/27:
#blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.

tika.initVM()
141/28:
#blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
141/29:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"

from jnius import autoclass
#from tika import parser
141/30:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"
!pip install jnius
from jnius import autoclass
#from tika import parser
141/31:
#pdf_imported=open("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf", 'rb')
pip install cython
pip install git+git://github.com/kivy/pyjnius.git
141/32:
#pdf_imported=open("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf", 'rb')
!pip install cython
!pip install git+git://github.com/kivy/pyjnius.git
141/33:
#pdf_imported=open("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf", 'rb')
!pip install cython
!pip install git
!pip install git+git://github.com/kivy/pyjnius.git
141/34:
#pdf_imported=open("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.pdf", 'rb')
!pip install cython
!pip install git
#!pip install git+git://github.com/kivy/pyjnius.git
141/35:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"
!pip install jnius
from jnius import autoclass
#from tika import parser
141/36:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"
!pip install pyjnius
from jnius import autoclass
#from tika import parser
141/37:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"
!pip install pyjnius
from pyjnius import autoclass
#from tika import parser
141/38:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Extract-locations-form-a-text-using-NLP--master\\tika-app-1.24.1.jar"
!pip install pyjnius
from jnius import autoclass
#from tika import parser
141/39:
#Book="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"
import os
os.environ['CLASSPATH'] = "C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Extract-locations-form-a-text-using-NLP--master\tika-app-1.24.1.jar"
!pip install pyjnius
from jnius import autoclass
#from tika import parser
139/2:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
139/3:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
139/4: from bs4 import BeautifulSoup
139/5:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
139/6:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
139/7:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
139/8:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
139/9:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
139/10: out
139/11:
import spacy
import spacy.displacy as displacy #Text Visualization
139/12: nlp=spacy.load("en_core_web_lg")
139/13: doc3=nlp(doc)
139/14: #doc3=nlp(doc)
139/15: type(out)
139/16: out[4][10]
139/17:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
139/18: Book2=nlp(Book)
139/19:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
139/20: lugares
139/21: displacy.serve(Book2, style="ent")
139/22:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
139/23: np.save('lugares_Alchimist', lugares)
142/1:
import numpy as np
import pandas as pd
142/2: #!conda install -c conda-forge folium=0.5.0 --yes
142/3: import folium
142/4:
# define the world map
world_map = folium.Map()

# display world map
world_map
142/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
139/24: np.save('lugares_Diarios', lugares)
142/6: lugares2=np.load('lugares_Diarios.npy')
142/7: lugares2
142/8: #pip install -U googlemaps
142/9: import googlemaps
142/10: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
142/11: lugares3=pd.DataFrame(lugares2)
142/12: lugares3
142/13:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
142/14: range(0, len(lugares3), 1)
142/15: lugares3
142/16: print(geocode_result[0])
142/17:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
142/18:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if lugares3["LAT"] is not None:
        Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/19: lugares3["LAT"][: ff]
142/20: lugares3["LAT"][0]
142/21: lugares3["LAT"][1]
142/22:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if lugares3["LAT"][i] is not None:
        Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )
        i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/23: lugares3["LAT"][i]
142/24:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if lugares3["LAT"][i] is not None:
        Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )
        i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/25: lugares3["LAT"][i]
142/26: lugares3["LAT"][i]
142/27: i
142/28: lugares3["LAT"][7]
142/29: lugares3["LAT"][6]
142/30:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if lugares3["LAT"][i] is not None:
        Dots.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='red',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6

        )
    )
        i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/31: type(ugares3["LAT"][6])
142/32: type(lugares3["LAT"][6])
142/33: type(lugares3["LAT"][i])
142/34:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if lugares3["LAT"][i] is not None:
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/35: if (lugares3["LAT"][i]) is not None
142/36: if (lugares3["LAT"][i]) is not None:
142/37: type(None)
142/38: if type(lugares3["LAT"][i])==type(None):
142/39:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])==type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/40:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/41:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=40
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/42:
from folium import plugins
ff=30
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
142/43: lugares3["LAT"][: 20]
142/44: lugares3["LAT"][: 100]
142/45:
pd.set_option('display.max_rows', None)
lugares3
142/46:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares)
142/47:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
142/48:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/49:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
143/1:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text(Book)
143/2: #!pip install ebooklib
143/3: #!pip install BeautifulSoup4
143/4: #!pip install pypdf2
143/5:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
143/6: from bs4 import BeautifulSoup
143/7:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
143/8:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
143/9:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
143/10:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
143/11:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text(Book)
143/12: out
143/13:
import spacy
import spacy.displacy as displacy #Text Visualization
143/14: nlp=spacy.load("en_core_web_lg")
143/15: #doc3=nlp(doc)
143/16: type(out)
143/17: out[4][10]
143/18:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
143/19: Book2=nlp(Book)
143/20:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
143/21: lugares
143/22: np.save('lugares_Alchemist', lugares)
143/23:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
143/24: np.save('lugares_Alchemist', lugares)
142/50:
#lugares2=np.load('lugares_Diarios.npy')
lugares2=np.load('lugares_Alchemist.npy')
142/51: lugares2
142/52: #pip install -U googlemaps
142/53: lugares3
142/54: lugares3=pd.DataFrame(lugares2)
142/55: lugares3
142/56:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
142/57: range(0, len(lugares3), 1)
142/58:
pd.set_option('display.max_rows', None)
lugares3
142/59: print(geocode_result[0])
142/60:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
142/61:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/62: type(None)
142/63:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
142/64:
lugares2=np.load('lugares_Diarios.npy')
#lugares2=np.load('lugares_Alchemist.npy')
142/65: lugares2
142/66: #pip install -U googlemaps
142/67: lugares3=pd.DataFrame(lugares2)
142/68: lugares3
142/69:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
142/70: range(0, len(lugares3), 1)
142/71:
pd.set_option('display.max_rows', None)
lugares3
142/72: print(geocode_result[0])
142/73:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
142/74:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
142/75: type(None)
142/76:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
147/1: #!pip install ebooklib
147/2: #!pip install BeautifulSoup4
147/3: #!pip install pypdf2
147/4:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
147/5: from bs4 import BeautifulSoup
147/6:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
147/7:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
147/8:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
147/9:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
147/10:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text(Book)
147/11: out
147/12:
import spacy
import spacy.displacy as displacy #Text Visualization
147/13: nlp=spacy.load("en_core_web_lg")
147/14: #doc3=nlp(doc)
147/15: type(out)
147/16: out[4][10]
147/17:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
147/18: Book2=nlp(Book)
147/19:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
147/20: lugares
147/21: displacy.serve(Book2, style="ent")
147/22:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
148/1:
import bs4 as bs
import urllib.request
import re
148/2:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/3:
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
148/4:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
148/5:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
148/6:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/7:
import bs4 as bs
import urllib.request
import re
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
148/8:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/9:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
formatted_article_text
148/10:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
article_text
148/11:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
scraped_data
148/12: parsed_article
148/13: paragraphs
148/14: article_text
148/15:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
148/16:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
148/17:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/18: sentence_scores
148/19:
import heapq
summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/20:
import heapq
summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/21:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/World_War_II')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/22: article_text
148/23:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
148/24:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
148/25:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/26:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
148/27:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
148/28:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/29: sentence_scores
148/30:
import heapq
summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/31:
import heapq
summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/32:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/33:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/The_Motorcycle_Diaries_(book)')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/34: article_text
148/35:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
148/36:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
148/37:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/38:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
148/39:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
148/40:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/41: sentence_scores
148/42:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/43:
scraped_data = urllib.request.urlopen('https://www.repubblica.it')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/44: article_text
148/45:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
148/46:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
148/47:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/48:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
148/49:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
148/50:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/51: sentence_scores
148/52:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/53:
scraped_data = urllib.request.urlopen('https://www.repubblica.it/vaticano/2020/10/21/news/papa_unioni_civili_gay-271344386/')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/54: article_text
148/55:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
148/56: article_text
148/57:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
148/58:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
148/59:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
148/60:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
148/61:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
148/62:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/63: sentence_scores
148/64:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
148/65:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 45:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
148/66: sentence_scores
148/67:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
151/1:
# -*- coding: utf-8 -*-

# This is a simple python script that requests the name of a book/author 
# and prints out the top 30 relevant quotes as found on Goodreads (www.goodreads.com)

# This script uses BeautifulSoup-4 to perform the final html parsing to retrieve the quotes and
# a python wrapper for the Goodreads API to retrieve the relevant bookID from the given string raw_input

# Link to the python wrapper: https://github.com/sefakilic/goodreads 

from goodreads import client
import re
import urllib
from bs4 import BeautifulSoup
152/1: #!pip install ebooklib
152/2: #!pip install BeautifulSoup4
152/3: #!pip install pypdf2
152/4:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
152/5: from bs4 import BeautifulSoup
152/6:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
152/7:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
152/8:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
152/9:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
152/10:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

out=epub2text(Book)
152/11: out
152/12:
import spacy
import spacy.displacy as displacy #Text Visualization
152/13: nlp=spacy.load("en_core_web_lg")
152/14: #doc3=nlp(doc)
152/15: type(out)
152/16: out[4][10]
152/17:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
152/18: Book2=nlp(Book)
152/19:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
152/20: lugares
152/21: displacy.serve(Book2, style="ent")
152/22:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
152/23: np.save('lugares_Alchemist', lugares)
152/24: displacy.render(Book2, style="ent")
152/25:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
152/26: np.save('lugares_Alchemist', lugares)
153/1:
import numpy as np
import pandas as pd
153/2: #!conda install -c conda-forge folium=0.5.0 --yes
153/3: import folium
153/4:
# define the world map
world_map = folium.Map()

# display world map
world_map
153/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
world_map
153/6:
lugares2=np.load('lugares_Diarios.npy')
#lugares2=np.load('lugares_Alchemist.npy')
153/7: lugares2
153/8: #pip install -U googlemaps
153/9: import googlemaps
153/10: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
153/11: lugares3=pd.DataFrame(lugares2)
153/12: lugares3
153/13:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
153/14: range(0, len(lugares3), 1)
153/15:
pd.set_option('display.max_rows', None)
lugares3
153/16:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
153/17:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
153/18: lugares3["LAT"][: 100]
154/1:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/olp')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
154/2: article_text
154/3:
import bs4 as bs
import urllib.request
import re
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
154/4:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/olp')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
154/5: article_text
154/6:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
154/7:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
154/8:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
154/9:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
154/10:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
154/11:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 45:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
154/12: sentence_scores
154/13:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
154/14:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/venezuela')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
154/15: article_text
154/16:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
154/17:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
154/18:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
154/19:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
154/20:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
154/21:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 45:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
154/22: sentence_scores
154/23:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
155/1: import tkinter as tk
155/2:
import tkinter as tk
from tkinter import filedialog, Text
import os
155/3: root.mainloop()
155/4: root.mainloop()
155/5:
root= tk.Tk()
root.mainloop()
155/6:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#2626")
root.mainloop()
155/7:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
root.mainloop()
155/8:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()
root.mainloop()
155/9:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")

root.mainloop()
155/10:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=1, relheight=1)
root.mainloop()
155/11:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9)
root.mainloop()
155/12:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)
root.mainloop()
155/13:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()


root.mainloop()
155/14:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(frame, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()


root.mainloop()
155/15:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(canvas, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()


root.mainloop()
155/16:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(frame, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()


root.mainloop()
155/17:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()


root.mainloop()
155/18:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42")
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/19:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
155/20:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42" command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/21:
root= tk.Tk()
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/22:
root= tk.Tk()
apps=[]
155/23:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
155/24:

canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/25:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()
155/26:

canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/27:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()
155/28:

canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/29:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/30:
root= tk.Tk()
apps=[]
155/31:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/32:
root= tk.Tk()
apps=[]
155/33:
def addApp():
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()
155/34:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/35:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()
155/36:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/37:
root= tk.Tk()
apps=[]
155/38:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()
155/39:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42")
runApps.pack()

root.mainloop()
155/40:
root= tk.Tk()
apps=[]
155/41:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/42:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()
155/43:
root= tk.Tk()
apps=[]
155/44:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/45:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps
    f.write(app + ',')
155/46:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
    f.write(app + ',')
155/47:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/48:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/49:
root= tk.Tk()
apps=[]
155/50:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/51:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/52:
root= tk.Tk()
apps=[]
155/53:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/54:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/55:
root= tk.Tk()
apps=[]
155/56:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/57:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/58: f
155/59: f
155/60:
root= tk.Tk()
apps=[]
155/61:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/62:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/63:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/64:
root= tk.Tk()
apps=[]
155/65:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/66:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/67:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        print(tempApps)
155/68:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/69:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/70:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = tempApps
155/71:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/72:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/73:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = tempApps
155/74:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/75:


canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/76:
with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/77:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = tempApps
155/78:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/79:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/80:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/81:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/82:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/83:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/84:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/85:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/86:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/87:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()
155/88:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/89:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/90:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()
155/91:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/92:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
155/93:
import tkinter as tk
from tkinter import filedialog, Text
import os
155/94:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/95:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/96:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
155/97:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/98:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/99:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/100:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
155/101:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/102:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/103:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/104:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
155/105:
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/106:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/107:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/108:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/109:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/110:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/111:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
155/112: GUI_example.py
155/113: python GUI_example.py
155/114: $python GUI_example.py
155/115:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
155/116:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
155/117:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
157/1:
import tkinter as tk
from tkinter import filedialog, Text
import os
157/2:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
157/3:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
157/4:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
157/5:
import tkinter as tk
from tkinter import filedialog, Text
import os
157/6:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
157/7:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
157/8:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
158/1:
#description: detects if s/o has diabetes using ML and python
#libraries
import sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from PIL import image
import streamlit as st
158/2:
#description: detects if s/o has diabetes using ML and python
#libraries
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from PIL import image
158/3: from PIL import Image
158/4:
#description: detects if s/o has diabetes using ML and python
#libraries
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
158/5:
from PIL import Image
import streamlit as st
158/6: pip install streamlit
158/7: import streamlit as st
158/8: pip install --upgrade protobuf
158/9: import streamlit as st
158/10: import streamlit as st
158/11:
pip uninstall protobuf python3-protobuf
pip install --upgrade pip
pip install --upgrade protobuf
158/12: pip show protobuf
158/13: protoc --version
158/14: pip install --upgrade protobuf
159/1: import streamlit as st
160/1:
import streamlit as st
from PIL import Image
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
160/2:
st.write("""
#Diabetes detection
detect if someone has diabetes using ML and python
""")
160/3:
#open and display an image
image = Image.open('C:/Users/aleja/Pictures/press_4722.jpg')
160/4: st.image(image, caption="Put caption on this", use_column_width=True )
160/5:
#Get user imput

def get_user_imput():
    days= st.sidebar.slider("travel days", 0, 60 ,7)
    budget=st.sidebar.slider("budget $", 200, 5000, 600)
    
    #store dict in variable
    user_data={'days':days,
               'budget':budget}
    #transform into dataframe
    features=pd.DataFrame(user_data, index=[0])
    return features
160/6:
#Get user imput

def get_user_input():
    days= st.sidebar.slider("travel days", 0, 60 ,7)
    budget=st.sidebar.slider("budget $", 200, 5000, 600)
    
    #store dict in variable
    user_data={'days':days,
               'budget':budget}
    #transform into dataframe
    features=pd.DataFrame(user_data, index=[0])
    return features
160/7: user_input=get_user_input()
160/8:
#Get user imput

def get_user_input():
    days= st.sidebar.slider("travel days", 0, 60 ,7)
    budget=st.sidebar.slider("budget $", 200, 5000, 600)
    
    #store dict in variable
    user_data={'days':days,
               'budget':budget}
    #transform into dataframe
    features=pd.DataFrame(user_data, index=[0])
    return features
160/9: user_input=get_user_input()
160/10:
st.subheader("user inputs:")
st.write(user_input)
161/1:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
161/2: from bs4 import BeautifulSoup
161/3:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
161/4:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
161/5:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
161/6:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
161/7:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
161/8: out
161/9:
import spacy
import spacy.displacy as displacy #Text Visualization
161/10: nlp=spacy.load("en_core_web_lg")
161/11: #doc3=nlp(doc)
161/12: type(out)
161/13: out[4][10]
161/14:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
161/15: Book2=nlp(Book)
161/16:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
161/17: lugares
161/18: displacy.render(Book2, style="ent")
161/19:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
161/20: np.save('lugares_diaries', lugares)
165/1:
import bs4 as bs
import urllib.request
import re
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
165/2:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/venezuela')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
165/3: article_text
165/4:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/venezuela')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
165/5: article_text
165/6:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
165/7:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Italy')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
165/8: article_text
165/9:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
165/10:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
165/11:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
165/12:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
165/13:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
165/14:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 45:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
165/15: sentence_scores
165/16:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
166/1: x=1+1
166/2: x
166/3: from PIL import Image
166/4: pip install PIL
166/5: pip install PIL
166/6: import numpy as np
168/1: print("hello")
168/2: import pandas as pd
170/1:
import PyPDF2 as pdf
from tika import parser
168/3: import pandas as pd
168/4: import pandas as pd
168/5: pip install pandas
168/6: import pandas as pd
168/7: pip install pandas
168/8: !pip install pandas
168/9: !pip install numpy
172/1:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
168/10: PYTHONPATH
168/11: pip install import-ipynb
168/12: !pip install import-ipynb
168/13: pip install import-ipynb
168/14: import import_ipynb
174/1: !pip install ebooklib
174/2: !pip install BeautifulSoup4
174/3:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
174/4:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
174/5: from bs4 import BeautifulSoup
174/6:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
174/7:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
174/8:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
174/9:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
174/10: #out
174/11:
import spacy
import spacy.displacy as displacy #Text Visualization
176/1:
import spacy
import spacy.displacy as displacy #Text Visualization
176/2:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
176/3:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
176/4:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
176/5:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
176/6: from bs4 import BeautifulSoup
176/7:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
176/8:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
176/9:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
176/10:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
176/11:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
188/1:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
188/2: chapters
188/3: from bs4 import BeautifulSoup
188/4:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
188/5:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
188/6:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
188/7:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
188/8:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
188/9: out
188/10:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"
chapters = epub2thtml(Book)
chapters
188/11:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"
chapters = epub2thtml(Book)
#chapters
188/12: #out
188/13:
import spacy
import spacy.displacy as displacy #Text Visualization
188/14: nlp=spacy.load("en_core_web_lg")
188/15: #doc3=nlp(doc)
188/16: type(out)
188/17: out[4][10]
188/18:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"
chapters = epub2thtml(Book)
len(chapters)
188/19:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
188/20: Book2=nlp(Book)
188/21:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
188/22: lugares
188/23: #lugares
188/24:
#displacy.render(Book2, style="ent")
svg = spacy.displacy.render(book2, style="ent")
output_path = Path(os.path.join("./", "sentence.svg"))
output_path.open('w', encoding="utf-8").write(svg)
188/25:
#displacy.render(Book2, style="ent")
svg = spacy.displacy.render(Book2, style="ent")
output_path = Path(os.path.join("./", "sentence.svg"))
output_path.open('w', encoding="utf-8").write(svg)
188/26:
#displacy.render(Book2, style="ent")
svg = spacy.displacy.render(Book2, style="ent")
output_path = Path(os.path.join("./", "sentence.svg"))
save=output_path.open('w', encoding="utf-8").write(svg)
188/27:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
188/28: np.save('lugares_diaries', lugares)
190/1:
import tkinter as tk
from tkinter import filedialog, Text
import os
190/2:
import tkinter as tk
from tkinter import filedialog, Text
import os
190/3:
root= tk.Tk()
apps=[]

if os.path.isfile('save.txt'):
    with open('save.txt', 'r') as f:
        tempApps=f.read()
        tempApps=tempApps.split(',')
        apps = [x for x in tempApps if x.strip()]
190/4:
def addApp():
    
    for widget in frame.winfo_children():
        widget.destroy()
    
    filename=filedialog.askopenfilename(initialdir="/", title="Select File",
                                       filetypes=(("executables", "*.exe"), ("all files", "*.*")))
    apps.append(filename)
    print(filename)
    for app in apps:
        label = tk.Label(frame, text=app, bg="gray")
        label.pack()

def runApps():
    for app in apps:
        os.startfile(app)
190/5:
canvas=tk.Canvas(root, height=700, width=700, bg="#263D42")
canvas.pack()

frame= tk.Frame(root,bg="white")
frame.place(relwidth=0.9, relheight=0.9, relx=0.05, rely=0.05)

#add button

openFile=tk.Button(root, text="Open File", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=addApp)
openFile.pack()

runApps=tk.Button(root, text="Run apps", padx=10,
                   pady= 5, fg="white", bg="#263D42", command=runApps)
runApps.pack()

for app in apps:
    label = tk.Label(frame, text=app)
    label.pack()
    
root.mainloop()

with open("save.text", 'w') as f:
    for app in apps:
        f.write(app + ',')
204/1:
# define the world map
world_map = folium.Map()

# display world map
#world_map
204/2:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
#world_map
204/3:
import numpy as np
import pandas as pd
204/4: #!conda install -c conda-forge folium=0.5.0 --yes
204/5: import folium
204/6: import folium
204/7:
# define the world map
world_map = folium.Map()

# display world map
#world_map
204/8:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
#world_map
204/9:
lugares2=np.load('lugares_Diarios.npy')
#lugares2=np.load('lugares_Alchemist.npy')
204/10: lugares2
204/11: import googlemaps
204/12: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
204/13: lugares3=pd.DataFrame(lugares2)
204/14: lugares3
204/15:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
204/16: range(0, len(lugares3), 1)
204/17:
pd.set_option('display.max_rows', None)
lugares3
206/1: #out
206/2:
import ebooklib
from ebooklib import epub
#epub_path="C:\Users\aleja\OneDrive\Documents\MBA\99. Project Work\Libros\Coelho, Paulo - The Alchemist (0) - libgen.lc.epub"

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
206/3: from bs4 import BeautifulSoup
206/4:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
206/5:
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
206/6:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
206/7:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
206/8:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
206/9: nlp=spacy.load("en_core_web_lg")
206/10:
import spacy
import spacy.displacy as displacy #Text Visualization
206/11: nlp=spacy.load("en_core_web_lg")
206/12: #out
206/13: #doc3=nlp(doc)
206/14: type(out)
206/15: out[4][10]
206/16:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
206/17: out[4]
206/18: out[1]
206/19: out[0]
206/20: len(out)
206/21:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
206/22: Book2=nlp(Book)
206/23:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/24:
GPE= []
FAC=[]
ORG=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" :
        GPE.append(ent.text)
    if ent.label_=="FAC" :
        FAC.append(ent.text)
    if ent.label_=="ORG" :
        ORG.append(ent.text)
206/25: #lugares
206/26:
#lugares
GPE
206/27:
#lugares
ORG
206/28:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares[ent,2].append(ent.text)
        lugares[ent,1].append(ent)
206/29:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares[ent;2].append(ent.text)
        lugares[ent;1].append(ent)
206/30:
#lugares
FAC
206/31:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/32:
#lugares
#FAC
train_data = [ ("Cuzco", [(0, 4, "GPE") ]
206/33:
#lugares
#FAC
train_data = [ 
    ("Cuzco", [(0, 4, "GPE"), 
    ]
206/34:
#lugares
#FAC
train_data = [ 
    ("Cuzco", [(0, 4, "GPE"), 
]
206/35:
#lugares
#FAC
train_data = [
    ("Who is Chaka Khan?", [(7, 17, "PERSON")]),
    ("I like London and Berlin.", [(7, 13, "LOC"), (18, 24, "LOC")]),
]
206/36:
#lugares
#FAC
train_data = [
    ("Who is Cuzco?", [(7, 11, "GPE")]),
    ("I like Chuquicamata and Berlin.", [(7, 13, "GPE"), (18, 24, "LOC")]),
]
206/37:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
206/38: Book2=nlp(Book)
206/39:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/40:
GPE= []
FAC=[]
ORG=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" :
        GPE.append(ent.text)
    if ent.label_=="FAC" :
        FAC.append(ent.text)
    if ent.label_=="ORG" :
        ORG.append(ent.text)
206/41:
#lugares
#FAC
train_data = [
    ("Who is Cuzco?", [(7, 11, "GPE")]),
    ("I like Chuquicamata and Berlin.", [(7, 18, "GPE"), (24, 29, "LOC")]),
]
206/42: displacy.serve(Book2, style="ent")
206/43:
#lugares
#FAC
train_data = [
    ("Where is Costa Rica?", [(9, 18, "GPE")]),
]
206/44:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
206/45: Book2=nlp(Book)
206/46:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
206/47: Book2=nlp(Book)
206/48:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/49:
GPE= []
FAC=[]
ORG=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" :
        GPE.append(ent.text)
    if ent.label_=="FAC" :
        FAC.append(ent.text)
    if ent.label_=="ORG" :
        ORG.append(ent.text)
206/50:
#lugares
#FAC
train_data = [
    ("Where is Costa Rica?", [(9, 18, "GPE")]),
]
206/51: displacy.serve(Book2, style="ent")
204/18:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
204/19: type(None)
204/20:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
206/52:
#lugares
#FAC
train_data = [
    ("Where is Barquisimeto?", [(9, 20, "GPE")]),
]
206/53:
GPE= []
FAC=[]
ORG=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" :
        GPE.append(ent.text)
    if ent.label_=="FAC" :
        FAC.append(ent.text)
    if ent.label_=="ORG" :
        ORG.append(ent.text)

# Calling nlp on our tweet texts to return a processed Doc for each
df['doc'] = [nlp(text) for text in df.text]
df.sample(3)
206/54:
GPE= []
FAC=[]
ORG=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" :
        GPE.append(ent.text)
    if ent.label_=="FAC" :
        FAC.append(ent.text)
    if ent.label_=="ORG" :
        ORG.append(ent.text)
206/55:
# Calling nlp on our tweet texts to return a processed Doc for each
df['doc'] = [nlp(Book) for text in df.text]
df.sample(3)
206/56: # Calling nlp on our tweet texts to return a processed Doc for each
206/57:
# Calling nlp on our tweet texts to return a processed Doc for each
GPE
206/58:
# Calling nlp on our tweet texts to return a processed Doc for each
GPE[1]
206/59:
# Calling nlp on our tweet texts to return a processed Doc for each
GPE[0]
206/60:
# Calling nlp on our tweet texts to return a processed Doc for each
Book2.ents
206/61:
# Calling nlp on our tweet texts to return a processed Doc for each
#Book2.ents
206/62:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text, ent.label)
206/63:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text, ent.label_)
206/64:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text: ent.label_)
206/65:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text ent.label_)
206/66:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append([ent.text], [ent.label_])
206/67:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append([ent.text]: [ent.label_])
206/68:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/69:
lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    print(token.pos_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
206/70:
Book2=nlp(Book)
Book2[0]
206/71: Book2[0]
206/72: Book2[1]
206/73: Book2[2]
206/74: Book2[0:5]
206/75: Book2[2]
206/76: Book2[3]
206/77: Book2[4]
206/78: Book2[5]
206/79: Book2[4]
206/80: Book2[0]
206/81:
#Book2[0]
for token in Book2:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)
206/82:
#Book2[0]
for token in Book2[0:500]:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)
206/83:     df = pd.DataFrame (Book2)
206/84:
    import pandas as pd
    df = pd.DataFrame (Book2)
206/85:
    import pandas as pd
    df = pd.DataFrame (Book2)
    df
206/86:
    import pandas as pd
    df = pd.DataFrame (Book2, columns = ['Text','Lemma'])
    df
206/87:
    import pandas as pd
    df = pd.DataFrame (Book2, columns = ['Text'])
    df
206/88:
    Sample=Book2[0:500]
    import pandas as pd
    df = pd.DataFrame (Book2, columns = ['Text'])
    df
206/89:
    Sample=Book2[0:500]
    import pandas as pd
    df = pd.DataFrame (Sample, columns = ['Text'])
    df
206/90:
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)
206/91:
#Book2[0]
for token in Sample:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)
206/92:
Book2=nlp(Book)
len(Book2)
206/93: len(Book2)
206/94:
    Sample=Book2[0:500]
    import pandas as pd
    #df = pd.DataFrame (Sample, columns = ['Text'])
    text_book = {'Text': Sample,
        'Lemma': [22000,25000,27000,35000]
        }

    df = pd.DataFrame(text_book, columns = ['Text', 'Lemma'])
206/95:
    Sample=Book2[0:500]
    import pandas as pd
    #df = pd.DataFrame (Sample, columns = ['Text'])
    text_book = {'Text': Sample
        }

    df = pd.DataFrame(text_book, columns = ['Text', 'Lemma'])
206/96:
    Sample=Book2[0:500]
    import pandas as pd
    #df = pd.DataFrame (Sample, columns = ['Text'])
    text_book = {'Text': Sample
        }

    df = pd.DataFrame(text_book, columns = ['Text', 'Lemma'])
    df
206/97:
    Sample=Book2[0:500]
    import pandas as pd
    #df = pd.DataFrame (Sample, columns = ['Text'])
    text_book = {'Text': Sample
        }

    df = pd.DataFrame(text_book, columns = ['Text'])
    df
206/98:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text
        }
df = pd.DataFrame(text_book, columns = ['Text'])
    df
206/99:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text
        }
df = pd.DataFrame(text_book, columns = ['Text'])
df
206/100:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos, 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop
        }
df = pd.DataFrame(text_book, columns = ['Text'])
df
206/101:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos, 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop
        }
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'is Alpha', 'is Stop'])
df
206/102:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos, 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop
        }
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'is Alpha', 'is Stop'])
#df
pos
206/103:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop
        }
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'is Alpha', 'is Stop'])
text_book
206/104:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop
        }
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'is Alpha', 'is Stop'])
#text_book
pos
206/105:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop}
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'Pos', 'Tag', 'Dep', 'Shape', 'is Alpha', 'is Stop'])
df
#text_book
#pos
206/106:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop}
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop'])
df
#text_book
#pos
206/107:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop}
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop'])
df
#text_book
#pos
df.to_csv("dataframe_book.csv")
206/108:
    Sample=Book2
    import pandas as pd
    #df = pd.DataFrame (Sample, columns = ['Text'])
    text_book = {'Text': Sample
        }

    df = pd.DataFrame(text_book, columns = ['Text'])
    #df
206/109:
text=[]
lemma=[]
pos=[]
tag=[]
dep=[]
shape=[]
is_alpha=[]
is_stop=[]
#Book2[0]
for token in Sample:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
    text.append(token.text)
    lemma.append(token.lemma_)
    pos.append(token.pos_)
    tag.append(token.tag_)
    dep.append(token.dep_)
    shape.append(token.shape_)
    is_alpha.append(token.is_alpha)
    is_stop.append(token.is_stop)

text_book = {'Text': text, 'Lemma': lemma, 'pos':pos , 'tag':tag, 'dep':dep, 'shape':shape, 'is_alpha':is_alpha, 'is_stop':is_stop}
df = pd.DataFrame(text_book, columns = ['Text', 'Lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop'])
df
#text_book
#pos
df.to_csv("dataframe_book.csv")
218/1:
import firebase_admin
from firebase_admin import credentials

cred = credentials.Certificate("path/to/serviceAccountKey.json")
firebase_admin.initialize_app(cred)
218/2:
import firebase-admin
from firebase-admin import credentials

cred = credentials.Certificate("path/to/serviceAccountKey.json")
firebase_admin.initialize_app(cred)
218/3:
import firebase_admin
from firebase_admin import credentials

cred = credentials.Certificate("path/to/serviceAccountKey.json")
firebase_admin.initialize_app(cred)
218/4:
import firebase_admin
from firebase_admin import credentials

cred = credentials.Certificate("path/to/serviceAccountKey.json")
firebase_admin.initialize_app(cred)
218/5:
import firebase_admin
from firebase_admin import credentials

cred = credentials.Certificate("path/to/serviceAccountKey.json")
firebase_admin.initialize_app(cred)
218/6:
import pandas as pd
import numpy as np
from numpy.random import choice, randint
import matplotlib.pyplot as plt

import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

cred = credentials.Certificate('service-account.json')
app = firebase_admin.initialize_app(cred)
db = firestore.client()

print('initialized')
218/7:
import pandas as pd
import numpy as np
from numpy.random import choice, randint
import matplotlib.pyplot as plt

import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

cred = credentials.Certificate('service-account.json')
app = firebase_admin.initialize_app(cred)
db = firestore.client()

print('initialized')
218/8:
import pandas as pd
import numpy as np
from numpy.random import choice, randint
import matplotlib.pyplot as plt

import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

cred = credentials.Certificate('service-account.json')
app = firebase_admin.initialize_app(cred)
db = firestore.client()

print('initialized')
218/9: import firebase-admin
218/10: import firebase_admin
218/11: import firebase_admin
218/12: import import_ipynb
218/13: import import_ipynb
218/14: import import_ipynb
220/1:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Italy')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
220/2:
import bs4 as bs
import urllib.request
import re
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.chunk import named_entity,ne_chunk
220/3:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Italy')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
220/4: #article_text
220/5:
# Removing Square Brackets and Extra Spaces
article_text = r-e.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
220/6:
scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Italy')
article = scraped_data.read()

parsed_article = bs.BeautifulSoup(article,'lxml')

paragraphs = parsed_article.find_all('p')

article_text = ""

for p in paragraphs:
    article_text += p.text
220/7:
# Removing Square Brackets and Extra Spaces
article_text = r-e.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
220/8:
# Removing Square Brackets and Extra Spaces
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
article_text = re.sub(r'\s+', ' ', article_text)
#article_text
220/9:
# Removing special characters and digits
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#formatted_article_text
220/10:
#Converting Text To Sentences
sentence_list = nltk.sent_tokenize(article_text)
220/11:
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}
for word in nltk.word_tokenize(formatted_article_text):
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
220/12:
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
220/13:
#Calculating Sentence Scores
#We have now calculated the weighted frequencies 
#for all the words. Now is the time to calculate the 
#scores for each sentence by adding weighted frequencies of 
#the words that occur in that particular sentence. 
#The following script calculates sentence scores:


sentence_scores = {}
for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 45:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
220/14: #sentence_scores
220/15:
import heapq
summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)
222/1: #out
226/1:
# define the world map
world_map = folium.Map()

# display world map
#world_map
226/2:
import numpy as np
import pandas as pd
226/3: #!conda install -c conda-forge folium=0.5.0 --yes
226/4: import folium
226/5:
# define the world map
world_map = folium.Map()

# display world map
#world_map
232/1:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
#world_map
232/2: #lugares2
234/1:
# define the world map
world_map = folium.Map()

# display world map
#world_map
234/2:
import numpy as np
import pandas as pd
234/3:
import numpy as np
import pandas as pd
import folium
234/4:
# define the world map
world_map = folium.Map()

# display world map
#world_map
234/5:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
#world_map
234/6:
#pd.set_option('display.max_rows', None)
#lugares3
234/7: # instantiate a feature group for the incidents in the dataframe
234/8:
lugares_load=np.load('lugares_Diarios.npy')
#lugares2=np.load('lugares_Alchemist.npy')
234/9: import googlemaps
234/10: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
234/11: lugares_DF=pd.DataFrame(lugares_load)
234/12:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
234/13:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares_DF), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares_DF.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
234/14:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares_DF), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
234/15:
lugares_DF["LAT"]= None
lugares_DF["LON"]= None

for i in range(0, len(lugares_DF), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
234/16:
lugares_DF["LAT"]= None
lugares_DF["LON"]= None

for i in range(0, len(lugares_DF), 1):
    geocode_result=g_key.geocode(lugares_DF.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
234/17: range(0, len(lugares3), 1)
234/18:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
234/19:
from folium import plugins
ff=len(lugares_DF)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares_DF["LAT"][0], lugares_DF["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares_DF["LON"][: ff], lugares_DF[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
234/20:
from folium import plugins
ff=len(lugares_DF)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares_DF["LAT"][0], lugares_DF["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares_DF["LAT"][: ff],lugares_DF["LON"][: ff], lugares_DF[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
234/21: range(0, len(lugares_DF), 1)
235/1:
lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)
235/2:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
235/3:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters
235/4:
blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output
235/5:
def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output
235/6:
def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
235/7:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)
235/8: nlp=spacy.load("en_core_web_lg")
235/9:
def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))
235/10: Book2=nlp(Book)
238/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

    def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

    def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

    Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/2:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

    def thtml2ttext(thtml):
        Output = []
        for html in thtml:
        text =  chap2text(html)
        Output.append(text)
        return Output

    def epub2text(epub_path):
        chapters = epub2thtml(epub_path)
        ttext = thtml2ttext(chapters)
        return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):  
    
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
        
        
# Driver code     
Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/3:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

    def thtml2ttext(thtml):
        Output = []
        for html in thtml:
        text =  chap2text(html)
        Output.append(text)
        return Output

    def epub2text(epub_path):
        chapters = epub2thtml(epub_path)
        ttext = thtml2ttext(chapters)
        return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
    Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/4:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
    text =  chap2text(html)
    Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
    Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/5:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
    Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        #lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/6:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
    Book=(listToString(out))  

lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)

Book2=nlp(Book)

displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/7:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)



displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
238/8: # EBook to Text
238/9: # EBook to Text
238/10:  EBook to Text
250/1:
import numpy as np
import pandas as pd
import folium
import googlemaps

lugares_load=np.load('lugares_Diarios.npy')

g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")

lugares_DF=pd.DataFrame(lugares_load)

lugares_DF["LAT"]= None
lugares_DF["LON"]= None

for i in range(0, len(lugares_DF), 1):
    geocode_result=g_key.geocode(lugares_DF.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
        lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
    
    from folium import plugins
ff=len(lugares_DF)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares_DF["LAT"][0], lugares_DF["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares_DF["LAT"][: ff],lugares_DF["LON"][: ff], lugares_DF[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
254/1:
import numpy as np
import pandas as pd
import folium
254/2:
# define the world map
world_map = folium.Map()

# display world map
#world_map
254/3:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[10, -66], zoom_start=8, tiles='OpenStreetMap')

# display world map
#world_map
254/4:
lugares2=np.load('lugares.npy')
#lugares2=np.load('lugares_Alchemist.npy')
254/5: lugares2
254/6: import googlemaps
254/7: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
254/8: lugares3=pd.DataFrame(lugares2)
254/9: lugares3
254/10:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
254/11: range(0, len(lugares3), 1)
254/12:
pd.set_option('display.max_rows', None)
lugares3
254/13: print(geocode_result[0])
254/14:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
254/15:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
254/16: type(None)
254/17:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
254/18: lugares3["LAT"][: 100]
254/19:
lugares2=np.load('lugares.npy')
#lugares2=np.load('lugares_Alchemist.npy')
254/20: lugares2
254/21: #pip install -U googlemaps
254/22: lugares3=pd.DataFrame(lugares2)
254/23: lugares3
254/24:
lugares3["LAT"]= None
lugares3["LON"]= None

for i in range(0, len(lugares3), 1):
    geocode_result=g_key.geocode(lugares3.iat[i,0])
    try:
        lat=geocode_result[0]["geometry"]["location"]["lat"]
        lon=geocode_result[0]["geometry"]["location"]["lng"]
        lugares3.iat[i,lugares3.columns.get_loc("LAT")]=lat
        lugares3.iat[i,lugares3.columns.get_loc("LON")]=lon
    except:
        lat=None
        lon=None
254/25: range(0, len(lugares3), 1)
254/26:
pd.set_option('display.max_rows', None)
lugares3
254/27: range(0, len(lugares3), 1)
254/28:
pd.set_option('display.max_rows', None)
lugares3
254/29: print(geocode_result[0])
254/30:
world_map = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=6, tiles='OpenStreetMap')

# display world map
world_map
len(lugares3)
254/31:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
Area.add_child(Dots)
254/32: type(None)
254/33:
from folium import plugins
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
254/34: lugares3["LAT"][: 100]
262/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\ernst0cheguavara_m0t0rcycled1ar1es.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
262/2: Book2
262/3:

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"

out=epub2text(Book)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
262/4: Book2
262/5: Book
262/6: out
262/7: Book
262/8: out
262/9: chapters
262/10: chapters=epub2thtml(epub_path)
262/11: chapters=epub2thtml(Book)
262/12: epub2thtml(Book)
262/13:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
epub2thtml(Bpath
262/14:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
262/15:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
262/16:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
chapters
262/17:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
262/18:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
chapters[]0
262/19:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
chapters(0
262/20:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
type(chapters)
262/21:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
type(chapters)
chapters[]0[]0
262/22:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
type(chapters)
chapters[0,0]
262/23:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
type(chapters)
chapters(0,0)
262/24:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
chapters[1]
262/25:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
chapters[2]
262/26:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[2])
262/27:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[1])
262/28:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[3])
262/29:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[4])
262/30:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[5])
262/31:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[6])
262/32:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[7])
262/33:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[8])
262/34:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
len(chapters[9])
262/35:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
for i in len(chapters):
    
ncapitulos.append((chapters[9])i
)
262/36:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
for i in len(chapters):
    capitulos.append(len(chapters[i]))

capitulos
262/37:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
i=1
for i in len(chapters):
    capitulos.append(len(chapters[i]))

capitulos
262/38: chapters[0]
262/39: chapters[1]
262/40: len(chapters)
262/41:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)
for i in tot_cap:
    capitulos.append(len(chapters[i]))

capitulos
262/42:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i in t50
    capitulos.append(len(chapters[i]))

capitulos
262/43:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i to tot_cap:
    capitulos.append(len(chapters[i]))

capitulos
262/44:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i=0 to tot_cap:
    capitulos.append(len(chapters[i]))

capitulos
262/45:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i in chapters:
    capitulos.append(len(chapters[i]))

capitulos
262/46:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i=0 to tot_cap
    capitulos.append(len(chapters[i]))

capitulos
262/47:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i to tot_cap
    capitulos.append(len(chapters[i]))

capitulos
262/48:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i==0 to tot_cap
    capitulos.append(len(chapters[i]))

capitulos
262/49:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i=0 to tot_cap
    capitulos.append(len(chapters[i]))

capitulos
262/50:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i in tot_cap:
    capitulos.append(len(chapters[i]))

capitulos
262/51:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)

for i in chapters:
    capitulos.append(len(chapters[i]))

capitulos
262/52:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)
z=0
for i in chapters:
    z+=1
    capitulos.append(len(chapters[iz))

capitulos
262/53:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)
z=0
for i in chapters:
    z=+1
    capitulos.append(len(chapters[z]))

capitulos
262/54: len(chapters[0])
262/55: len(chapters[1])
262/56: len(chapters[2])
262/57:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)
z=0
for i in chapters:
    z=z+1
    capitulos.append(len(chapters[z]))

capitulos
262/58:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
len(chapters)
capitulos=[]
tot_cap=len(chapters)
z=0
for i in chapters:
    
    capitulos.append(len(chapters[z]))
    z=z+1
capitulos
262/59: len(chapters[3])
262/60:
len(chapters[3])
chapters[3]
262/61:
len(chapters[3])
chapters[2]
262/62:
len(chapters[3])
chapters[1]
262/63:
len(chapters[3])
chapters[3]
262/64:
len(chapters[3])
chapters[2]
262/65:
len(chapters[3])
chapters[3]
262/66:
len(chapters[3])
chapters[3][0]
262/67:
len(chapters[3])
chapters[3][1]
262/68:
len(chapters[3])
chapters[3][3]
262/69:
len(chapters[3])
chapters[3][31]
262/70:
len(chapters[3])
chapters[3][800]
262/71:
len(chapters[3])
chapters[3][:]
262/72:
len(chapters[3])
chapters[3][end]
262/73:
len(chapters[3])
chapters[3][max]
262/74:
len(chapters[3])
len(chapters[3][:])
262/75:
len(chapters[3])
len(chapters[3][2000])
262/76:
len(chapters[3])
(chapters[3][2000])
262/77:
len(chapters[3])
(chapters[3][25154])
262/78:
len(chapters[3])
(chapters[3][20154])
262/79:
len(chapters[3])
(chapters[5])
262/80:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the eedle.epub")

dd=chap2text(chapters)
len(chapters)
capitulos=[]
tot_cap=len(chapters)
z=0
for i in chapters:
    
    capitulos.append(len(chapters[z]))
    z=z+1
capitulos
262/81:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
dd=chap2text(chapters)# 

len(chapte# rs)
capitulos# =[]
tot_cap=len(chapte# rs)
# z=0
for i in chapters:
 #    
    capitulos.append(len(chapters[z# ]))
    z=# z+1
capitulos
262/82:
path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
chapters=epub2thtml("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub")
dd=chap2text(chapters)

# len(chapters)
# capitulos=[]
# tot_cap=len(chapters)
# z=0
# for i in chapters:
    
#     capitulos.append(len(chapters[z]))
#     z=z+1
# capitulos
262/83:

def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext

path="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"

out=epub2text(path)

nlp=spacy.load("en_core_web_lg")

def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
264/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
264/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
264/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
nlp=spacy.load("en_core_web_lg")
264/4: out=epub2text(Book)
264/5: out
264/6: len(out)
264/7: len(out[0])
264/8: len(out[1])
264/9:
number_words=[]
for i in len(out):
    number_words.append(len(out[i])


number_words
264/10:
number_words=[]
for i in len(out):
    number_words.append(len(out[i])
264/11:
number_words=[]
for i in len(out):
    number_words.append(len(out[i])
264/12:
number_words=[]
for i in len(out):
    number_words.append(len(out[i])
264/13:
number_words=[]
for i in len(out):
    number_words.append(len(out[0])
264/14:
number_words=[]
for i in len(out)
    number_words.append(len(out[0])
264/15:
number_words=[]
for i in len(out):
    number_words.append(len(out[0])
264/16:
number_words=[]
for i in len(out):
    number_words.append(len(out[i]))
264/17:
number_words=[]
for i in len(out):
    number_words.append(len(out[0]))
264/18:
number_words=[]
ff=len(out)
for i in ff:
    number_words.append(len(out[0]))
264/19:
number_words=[]
ff=len(out)
print(ff)
for i in ff:
    number_words.append(len(out[0]))
264/20:
number_words=[]
ff=len(out)
print(ff)
for i=0 to ff:
    number_words.append(len(out[0]))
264/21:
number_words=[]
ff=len(out)
print(ff)
i=0
for i to ff:
    number_words.append(len(out[0]))
264/22:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in len(out):
    number_words.append(len(out[z]))
    z+=z
264/23: type(out)
264/24:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z+=z
264/25:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z+=z
number_words
264/26:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
264/27: out[0]
264/28: out[0][1]
264/29: out[0]0:-1]
264/30: out[0]0:8]
264/31: out[0][0:8]
264/32: out[1]
264/33: out[3]
264/34: out[2]
264/35: out[5]
264/36:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)


spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/37:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text, ent.label_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/38:
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text ent.label_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/39:
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/40:
label=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/41:
label=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        POS.append(token.pos_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/42:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        POS.append(token.pos_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/43:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        POS.append(ent.pos_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/44:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
for token in Book2:
    POS.append(ent.pos_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/45:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
for token in Book2:
    POS.append(token.pos_)


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/46: type(lugares)
264/47: POS
264/48:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass
    


#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/49: PS
264/50: POS
264/51: len(POS)
264/52:
for token in Book2:
    POS.append(token.label_)
264/53:
for ents in Book2:
    POS.append(ents.label_)
264/54:
label=[]
POS=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/55:
label=[]
POS=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
264/56: start_ent
264/57: start_ent, end_ent
264/58: #
264/59: Book2.ents
264/60: len(Book2.ents)
264/61:
contatore=[]
z=0

for i in Book2:
    if Book2[i]==Book2.ent[z]:
        z=z+1
        contatore.append(i)
    else:
        pass
264/62:
contatore=[]
z=0

for i in Book2:
    if Book2[i]==Book2.ents[z]:
        z=z+1
        contatore.append(i)
    else:
        pass
264/63: Book2
264/64: Book2[0]
264/65: Book2[1]
264/66: Book2[2]
264/67: type(Book2)
264/68: type(Book2.ents)
264/69: type(Book2)
264/70:
POS=[]
text=[]
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
        text.append(token.text)
    else:
        pass
264/71: type(text)
264/72:
POS=[]
text_list=[]
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
        text_list.append(token.text)
    else:
        pass
264/73: type(text_list)
264/74:
contatore=[]
z=0

for i in text_list:
    if text_list[i]==lugares[z]:
        z=z+1
        contatore.append(i)
    else:
        pass
264/75:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        
        contatore.append(i)
        z=z+1
        y=y+1
    else:
        y=y+1
        pass
264/76: contatore
264/77: text_list
264/78:
POS=[]
text_list=[]
for token in Book2:
    #if token.pos_=="PUNCT":
    POS.append(token.pos_)
    text_list.append(token.text)
264/79: text_list
264/80:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        
        contatore.append(i)
        z=z+1
        y=y+1
    else:
        y=y+1
        pass
264/81: contatore
264/82:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        
        contatore.append(i)
        z=z+1
        y=y+1
    else:
        y=y+1
264/83: contatore
264/84: lugares
264/85:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        contatore.append(z)
        z=z+1
        y=y+1
    else:
        y=y+1
264/86: contatore
264/87: contatore
264/88: len(contatore)
264/89:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        contatore.append(z)
        z=z+1
        y=y+1
    else:
264/90:
contatore=[]
z=0
y=0
for i in text_list:
    if text_list[y]==lugares[z]:
        contatore.append(z)
        z=z+1
        y=y+1
264/91: len(contatore)
264/92:
contatore=[]
z=0
y=0
while i < len(text_list):
    if text_list[i]==lugares[z]:
        contatore.append(z)
        z+=1
        i+=1
    else:
        i+=1
264/93:
contatore=[]
z=0
y=0
i=0
while i < len(text_list):
    if text_list[i]==lugares[z]:
        contatore.append(z)
        z+=1
        i+=1
    else:
        i+=1
264/94: len(contatore)
264/95: lugares[0:10]
264/96: lugares[2]
264/97: type(lugares[2])
264/98: type(text_list[2])
264/99: (text_list[2])
264/100: #lugares
264/101: Book2.ents
264/102: Book[45454]
264/103: Book[4545]
264/104: Book[454]
264/105: type(Book)
264/106: len(Book)
264/107: Book(50:89)
264/108: Book[50:89]
264/109: len(start_ent)
264/110:
len(start_ent)
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            start_frase.append(z)
        else:
            z+=1
264/111:
len(start_ent)
start_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            start_frase.append(z)
        else:
            z+=1
264/112: start_frase
264/113: start_ent[0:10]
264/114:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z])
            start_frase.append(z)
        else:
            z+=1
264/115:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
        else:
            z+=1
264/116:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    #while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
        else:
            z+=1
264/117:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    #while z<len(Book):
    if Book[z]==".":
        print(Book[z],z)
        start_frase.append(z)
    else:
        z+=1
264/118:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            pass
        else:
            z+=1
264/119:
len(start_ent)
start_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
264/120:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    while z>0:
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z-=1
264/121:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z-=1
264/122:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[z]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
264/123: Book[570:603]
264/124: Book[1466:1731]
264/125: lugares
264/126: print(Book[1494:1731], lugares
264/127: print(Book[1494:1731])
264/128:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
264/129: print(Book[1494:1731])
264/130: print(Book[1343:1731])
264/131: print(Book[4577:4686])
264/132: print(Book[3936:4101])
266/1:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
266/2:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
266/3:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
266/4:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
nlp=spacy.load("en_core_web_lg")
266/5: out[5]
266/6: out=epub2text(Book)
266/7: out[5]
266/8: Book[5]
266/9: out[5]
266/10:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
266/11:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
266/12:
POS=[]
text_list=[]
for token in Book2:
    #if token.pos_=="PUNCT":
    POS.append(token.pos_)
    text_list.append(token.text)
266/13: #lugares
266/14:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
266/15: print(Book[3936:4101])
266/16: <<<<<à
266/17: #
266/18:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
266/19:
label=[]

start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
266/20: start_ent[0:10]
266/21:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
266/22: print(Book[3936:4101])
266/23: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_ent)))
266/24: Data_Book
266/25: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_ent, Book[start_frase:end_frase]))
266/26: Data_Book
266/27:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            start_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            end_frase.append(f)
            break
        else:
            f-=1
266/28:
Frases_Book=[]
for i in start_frase:
    Frases_Book=(start_frase[i]:end_frase[i])
266/29: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, Book[start_frase: end_frase]))
266/30: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, Book[start_frase: end_frase])
266/31: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, Book[start_frase: end_frase])))
266/32:
Frases_Book=[]
for i in start_frase:
    Frases_Book=Book[start_frase[i]:end_frase[i]]
266/33:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book=Book[start_frase[z]:end_frase[z]]
    z+=1
266/34: print(Frases_Book[0:2])
266/35: print(Frases_Book[0:10])
266/36:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book=Book[start_frase[z]:end_frase[z]]
    z+=1
266/37: print(Book[3936:4101])
266/38: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, Frases_Book)))
266/39: Data_Book
266/40: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent)))
266/41: Data_Book
266/42: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_frase)))
266/43: Data_Book
266/44:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
266/45:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book=Book[start_frase[z]:end_frase[z]]
    z+=1
266/46: print(Book[3936:4101])
266/47: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_frase)))
266/48: Data_Book
266/49: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_frase, Frases_Book)))
266/50: Data_Book
266/51:
print(Book[3936:4101])
Frases_BBook
266/52:
print(Book[3936:4101])
start_frase[713]
266/53:
print(Book[3936:4101])
start_frase
266/54:
print(Book[3936:4101])
start_frase[0]
266/55:
print(Book[3936:4101])
start_frase[0], end_frase[0]
266/56:
print(Book[3936:4101])
Book[start_frase[0], end_frase[0]]
266/57:
print(Book[3936:4101])
Book[start_frase[0]: end_frase[0]]
266/58:
print(Book[3936:4101])
type(Book[start_frase[0]: end_frase[0]])
266/59:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book=Book[start_frase[z]:end_frase[z]]
    z+=1
266/60:
print(Book[3936:4101])
type(Book[start_frase[0]: end_frase[0]])
266/61: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_frase, Frases_Book)))
266/62: Data_Book
266/63:
print(Book[3936:4101])
(Book[start_frase[0]: end_frase[0]])
266/64:
print(Book[3936:4101])
(Book[start_frase[0]: end_frase[0]])
Frases_Book
266/65:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book
266/66:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
266/67:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book
266/68:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
#Frases_Book
266/69: Data_Book=pd.DataFrame(list(zip(lugares, start_ent, end_ent, start_frase, end_frase, Frases_Book)))
266/70: Data_Book
266/71:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[0]
266/72:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[1]
266/73:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[2]
266/74:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[3]
266/75:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[4]
266/76:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
Frases_Book[9]
266/77:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
lugares[9],Frases_Book[9]
266/78:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
266/79: Data_Book=pd.DataFrame(list(zip(lugares, Frases_Book)), columns=["lugares", "Quotes"])
266/80: Data_Book
266/81: Data_Book.to_csv("Data_Book.csv")
266/82:
label=[]

start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
266/83: #lugares
266/84: start_ent[0:10]
266/85:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
266/86:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
266/87:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
266/88: Data_Book=pd.DataFrame(list(zip(lugares, Frases_Book)), columns=["lugares", "Quotes"])
266/89: Data_Book
266/90: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares", "Quotes"])
266/91: Data_Book
266/92: Data_Book.to_csv("Data_Book.csv")
266/93: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labes", "Quotes"])
266/94: Data_Book
266/95: Data_Book.to_csv("Data_Book.csv")
266/96:
label=[]

start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
266/97: #lugares
266/98: start_ent[0:10]
266/99:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
266/100:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
266/101:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
266/102: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labes", "Quotes"])
266/103: Data_Book
266/104: Data_Book.to_csv("Data_Book.csv")
266/105: Data_Book.to_csv("Data_Book.csv")
266/106:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]=="." or Book[f]==":":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
266/107:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
266/108:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
266/109: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labes", "Quotes"])
266/110: Data_Book
266/111: Data_Book.to_csv("Data_Book.csv")
266/112: Data_Book[:50]
266/113: Data_Book.head()
266/114: Frases_Book[:50]
266/115:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= lugares_Book["lugares"]
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
266/116:
 lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= lugares_Book["lugares"]
266/117:
lugares_Book=pandas.read_csv(Data_Book)
lugares_load= lugares_Book["lugares"]
266/118:
lugares_Book=pd.read_csv(Data_Book)
lugares_load= lugares_Book["lugares"]
266/119:
lugares_Book=pd.read_csv("Data_Book.csv")
lugares_load= lugares_Book["lugares"]
266/120: lugares_load
266/121: type(lugares_load)
266/122:  lugares_load= lugares
266/123:  lugares_load
266/124: #
266/125: Data_Book_all=[Data_Book, lugares_DF]
266/126:
coordinates(lugares)
Data_Book_all=[Data_Book, lugares_DF]
266/127:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= lugares
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
266/128:
coordinates(lugares)
Data_Book_all=[Data_Book, lugares_DF]
266/129:
label=[]

start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
266/130: len(lugares)
266/131:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)
for token in Book2:
    if token.pos_=="PUNCT":
        POS.append(token.pos_)
    else:
        pass



#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

np.save('lugares_Diarios', lugares)
266/132: len(lugares)
266/133: start_ent[0:10]
266/134:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]=="." or Book[f]==":":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
266/135:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
266/136:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
266/137: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labes", "Quotes"])
266/138: Frases_Book[:50]
266/139: Data_Book.to_csv("Data_Book.csv")
266/140: Data_Book[:50]
266/141:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= lugares
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
266/142:
coordinates(lugares)
Data_Book_all=[Data_Book, lugares_DF]
266/143: lugares.DF.head()
266/144: lugares_DF.head()
266/145:
lugares_DF=coordinates(lugares)
Data_Book_all=[Data_Book, lugares_DF]
266/146: Data_Book_all.head()
266/147: Data_Book_all
266/148: Data_Book_all.head()
266/149: lugares_DF.head()
266/150: Data_Book_all=[Data_Book, lugares_DF["LAT", "LON"]]
266/151: Data_Book_all=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
266/152: Data_Book_all
266/153: type(Data_Book_all)
266/154:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_book_all=pd.concat(frames)
266/155:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames)
266/156: Data_Book_all
266/157: Data_Book_all.head()
266/158: lugares_DF["LAT"]
266/159: lugares_DF["LON"]
266/160: Data_Book_all["lugares"]
266/161: Data_Book_all["frases"]
266/162: Data_Book_all["Quotes"]
266/163: Data_Book_all.to_csv("Data_Book.csv")
266/164:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames, axis=1, sort=False)
266/165: Data_Book_all["Quotes"]
266/166: Data_Book_all.to_csv("Data_Book.csv")
266/167:
Data_Book_all['to_drop'] = Data_Book_all.lugares(to_drop)
 
df_cleaned_places = Data_Book_all[Data_Book_all.to_drop==False]
df_cleaned_places.to_csv('cleaned_places.csv', index=False)
 
unique_places = df_cleaned_places.place.unique()
df_unique_places = pd.DataFrame(unique_places)
#df_unique_places.to_csv('unique_places.csv', index=False)
266/168:
Data_Book_all['to_drop'] = Data_Book_all.place.map(to_drop)
 
df_cleaned_places = Data_Book_all[Data_Book_all.to_drop==False]
df_cleaned_places.to_csv('cleaned_places.csv', index=False)
 
unique_places = df_cleaned_places.place.unique()
df_unique_places = pd.DataFrame(unique_places)
#df_unique_places.to_csv('unique_places.csv', index=False)
266/169:
def to_drop(place):
    if not all(c.isalpha() or c.isspace() for c in place) or place.islower():
        return True
    else:
        return False
266/170:
Data_Book_all['to_drop'] = Data_Book_all.place.map(to_drop)
 
df_cleaned_places = Data_Book_all[Data_Book_all.to_drop==False]
df_cleaned_places.to_csv('cleaned_places.csv', index=False)
 
unique_places = df_cleaned_places.place.unique()
df_unique_places = pd.DataFrame(unique_places)
#df_unique_places.to_csv('unique_places.csv', index=False)
270/1: #lugares2
274/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
274/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
274/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
nlp=spacy.load("en_core_web_lg")
274/4: out=epub2text(Book)
274/5: out[5]
274/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
274/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
274/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
274/9: len(lugares)
274/10: start_ent[0:10]
274/11:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
274/12:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
274/13:
geolocator = Nominatim(user_agent="specify_your_app_name_here")
location = geolocator.geocode("175 5th Avenue NYC")
print(location.address)
274/14:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("175 5th Avenue NYC")
print(location.address)
274/15:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("Liverpool")
print(location.address)
274/16:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("Liverpool")
print(location.address)
print((location.latitude, location.longitude))
274/17:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    lugares_DF=pd.DataFrame(lugares)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares_DF[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            location = geolocator.geocode(lugares[i])
            location.latitude=lat
            location.longitude=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
274/18:
lugares_DF=coordinates(lugares)
Data_Book_all=[Data_Book, lugares_DF]
274/19:
lugares_DF=coordinates_geopy(lugares)
Data_Book_all=[Data_Book, lugares_DF]
274/20:
lugares_DF=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
274/21: lugares_DF
274/22:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    lugares_DF=pd.DataFrame(lugares)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares_DF[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            location = geolocator.geocode(lugares[i])
            location.latitude=lat
            location.longitude=lon
        # except:
        #     lat=None
        #     lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
274/23:
lugares_DF=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
274/24: lugares_DF
274/25:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("BRITAIN")
print(location.address)
print((location.latitude, location.longitude))
274/26:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("England")
print(location.address)
print((location.latitude, location.longitude))
274/27:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("Britain")
print(location.address)
print((location.latitude, location.longitude))
274/28:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("Yorkshire")
print(location.address)
print((location.latitude, location.longitude))
274/29:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode("Yorkshire England")
print(location.address)
print((location.latitude, location.longitude))
274/30: lugares[0]
274/31:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    lugares_DF=pd.DataFrame(lugares)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares_DF[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            location = geolocator.geocode(lugares[i])
            lat=location.latitude
            lon=location.longitude
        # except:
        #     lat=None
        #     lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
276/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
276/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
276/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
nlp=spacy.load("en_core_web_lg")
276/4: out=epub2text(Book)
276/5: out[5]
276/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
276/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
276/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
276/9: len(lugares)
276/10: start_ent[0:10]
276/11:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
276/12:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
276/13:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
276/14:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
276/15:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
276/16: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
276/17: Frases_Book[:5]
276/18: Data_Book.to_csv("Data_Book.csv")
276/19: Data_Book[:5]
276/20:
# lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
276/21: Data_Book_all.head()
276/22:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares_DF[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/23:
lugares_DF=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/24: lat
276/25: lugares_DF
276/26:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/27: lat
276/28:
location = geolocator.geocode(lugares)
print(location.address)
print((location.latitude, location.longitude))
276/29:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares)
print(location.address)
print((location.latitude, location.longitude))
276/30:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
    print ("coordinates: ", i, " extracted", sep=' ', end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/31:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/32:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted", sep=' ', end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/33:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/34:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted", sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/35:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/36:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted",end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/37:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/38:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted",end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/39:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/40:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted",end='')
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/41:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/42:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    for i in range(0, len(lugares[0:10]), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, " extracted", end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/43:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/44:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10]
    for i in range(0, all_points), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, "of ", all_points, " extracted", end=" ", flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/45:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/46:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10]
    for i in range(0, all_points), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print (" coordinates: ", i, "of ", all_points, " extracted", end=" ", flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/47:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/48:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10]
    for i in range(0, (all_points), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print (" coordinates: ", i, "of ", all_points, " extracted", end=" ", flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/49:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/50:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10]
    for i in range(0, (all_points), 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/51:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/52:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/53:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=True)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/54:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/55:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=false)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/56:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/57:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[0:10])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/58:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/59:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        lat.append(location.latitude)
        lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/60:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/61: lat
276/62:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location.latitude=None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/63:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/64:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location.latitude==None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/65:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/66:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location.latitude is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/67:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[60:80])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location.latitude is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/68:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/69:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[60:80])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[60+i])
        if location.latitude is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/70:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/71:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[68])
print(location.address)
print((location.latitude, location.longitude))
276/72:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[67])
print(location.address)
print((location.latitude, location.longitude))
276/73:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[69])
print(location.address)
print((location.latitude, location.longitude))
276/74:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[69])
#print(location.address)
print((location.latitude, location.longitude))
276/75:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[69])
#print(location.address)
print(lugares[69])
print((location.latitude, location.longitude))
276/76:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[69])
#print(location.address)
print(location)
print((location.latitude, location.longitude))
276/77:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares[69])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/78:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[60:80])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[60+i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
        print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/79:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/80:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares[60:80])
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[60+i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/81:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/82:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/83:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/84:
def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lat, lon)
276/85:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/86:
import time

main()
print("--- %s seconds ---" % (time.time() - start_time))

def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location is None:
            #print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lat, lon)
276/87:
import time

main()
print("--- %s seconds ---" % (time.time() - start_time))

def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lat, lon)
276/88:
import time



def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lat, lon)
276/89:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/90: lugares_unique=numpy.unique(lugares)
276/91: lugares_unique=np.unique(lugares)
276/92:
lugares_unique=np.unique(lugares)
len(lugares_unique)
276/93:
import time



def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lat, lon)
276/94:
lat, lon=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/95: Data_Book.unique()
276/96: pd.Data_Book.unique()
276/97: pd.Series(Data_Book).unique()
276/98: Data_Book()
276/99: Data_Book
276/100: Data_Book["lugares"]
276/101: Data_Book["lugares"].unique()
276/102: len(Data_Book["lugares"].unique())
276/103: Data_Book["lugares"].dropduplicates()
276/104: Data_Book["lugares"].drop_duplicates()
276/105: len(Data_Book["lugares"].drop_duplicates())
276/106:
fff=Data_Book["lugares"].drop_duplicates()
fff.head(10)
276/107:
fff=Data_Book.drop_duplicates()
fff.head(10)
276/108:
fff=Data_Book.drop_duplicates()
len(fff)
276/109: fff=Data_Book.drop_duplicates(subset ="First Name", keep = False, inplace = True)
276/110: fff=Data_Book.drop_duplicates(subset ="lugares", keep = False, inplace = True)
276/111:
fff=Data_Book.drop_duplicates(subset ="lugares", keep = False, inplace = True)
len(fff)
276/112:
fff=Data_Book.drop_duplicates(subset ="lugares", keep = False, inplace = True)
fff
276/113:
fff=Data_Book.drop_duplicates(subset ="lugares", keep = False, inplace = True)
fff
276/114:
fff=Data_Book.drop_duplicates(subset ="lugares")
fff
276/115:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(lugares)
print(location.address)
print((location.latitude, location.longitude))
276/116:
fff=Data_Book.drop_duplicates(subset ="lugares")
len(fff)
276/117:
fff=Data_Book.drop_duplicates(subset ="lugares")
len(fff)
276/118:
fff=Data_Book.drop_duplicates(subset ="lugares")
len(fff)
len(Data_Book)
276/119:
fff=Data_Book.drop_duplicates(subset ="lugares")

len(Data_Book)
276/120:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
276/121: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
276/122: Frases_Book[:5]
276/123: #Data_Book.to_csv("Data_Book.csv")
276/124: Data_Book[:5]
276/125:
#fff=Data_Book.drop_duplicates(subset ="lugares")

len(Data_Book)
276/126:
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")

len(Data_Book)
276/127:
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")

len(Data_Book_short)
276/128:
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")

len(fff)
276/129: fff
276/130: fff["lugares"]
276/131:
import time
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    Data_Book_short=Data_Book
    fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=fff["lugares"]
    fff["lat"]=None
    fff["lon"]=None

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            fff["lat"].append(location.latitude)
            fff["lon"].append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(fff)
276/132:
fff=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/133:
import time
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(Data_Book):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    Data_Book_short=Data_Book
    fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=fff["lugares"]
    fff["lat"]=None
    fff["lon"]=None

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            fff["lat"].append(location.latitude)
            fff["lon"].append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(fff)
276/134:
fff=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/135:
import time
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(Data_Book):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    Data_Book_short=Data_Book
    fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=fff["lugares"]
    fff["lat"]=None
    fff["lon"]=None

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            fff["lat"][i]=(location.latitude)
            fff["lon"][i]=(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(fff)
276/136:
fff=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/137: type(fff["lugares"])
276/138:
import time
Data_Book_short=Data_Book
fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(Data_Book):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    Data_Book_short=Data_Book
    fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=fff["lugares"]
    fff["lat"]=None
    fff["lon"]=None

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(fff)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(fff["lugares"][i])
        if location is None:
            print(i, " NONE ")
        else:
            fff["lat"][i]=(location.latitude)
            fff["lon"][i]=(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(fff)
276/139:
fff=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/140:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book[])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/141:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book[])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/142:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book[0])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/143:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book[0]
276/144:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book
276/145:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
276/146:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"[0])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/147:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][0])
#print(location.address)
print(type(location))
print((location.latitude, location.longitude))
276/148:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][0])
#print(location.address)
print((location))
print((location.latitude, location.longitude))
276/149:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location))
print((location.latitude, location.longitude))
276/150:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
276/151:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares[0]
276/152:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
len(lugares)
276/153:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.lugares.unique()
    

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat)
276/154:
lugares_unique, lon, lat=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/155:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat)
276/156:
lugares_unique, lon, lat=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/157:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares)
276/158:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares)
lugares_unique
276/159:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat)
276/160:
lugares_unique, lon, lat=coordinates_geopy(Data_Book)
#Data_Book_all=[Data_Book, lugares_DF]
276/161:
lugares_unique, lon, lat=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/162: Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat)), columns=["lugares","LON", "LAT"])
276/163:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat)), columns=["lugares","LON", "LAT"])
Data_Book_Short
276/164:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat)
276/165:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/166:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, adress)
276/167:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
276/168:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/169:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat)), columns=["lugares","LON", "LAT", "address"])
Data_Book_Short
276/170:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON", "LAT", "address"])
Data_Book_Short
276/171: Data_Book_short.to_csv("Data_Book_short.csv")
276/172: Data_Book_Short.to_csv("Data_Book_short.csv")
276/173: ff=2
276/174: ff
276/175:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
276/176:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON", "LAT", "address"])
Data_Book_Short
276/177:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
lugares_unique
276/178: #Frases_Book[:5]
276/179: Data_Book_all.head()
276/180:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
u,lugares_unique=np.unique(lugares, return_index=True)
lugares_unique
276/181:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique)
276/182:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[0])
276/183:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[1])
280/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
280/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
280/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
280/4: out=epub2text(Book)
280/5: out[5]
280/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
280/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
282/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
282/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
282/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Follett, Ken - Eye of the Needle.epub"
nlp=spacy.load("en_core_web_lg")
282/4:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
282/5:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
282/6:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
282/7:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
284/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
284/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
284/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
284/4: out=epub2text(Book)
284/5: out[5]
284/6: out[0]
284/7:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
284/8:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
284/9:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
284/10: len(lugares)
284/11: start_ent[0:10]
284/12:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/13:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/14:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
284/15: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
284/16: #Frases_Book[:5]
284/17: #Data_Book.to_csv("Data_Book.csv")
284/18: Data_Book[:5]
284/19:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames, axis=1, sort=False)
284/20:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[1])
284/21:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/22:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/23:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][0])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/24:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][3])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/25:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][10])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/26:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][80])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/27:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/28:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/29:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app2")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/30:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/31:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/32:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/33:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app2")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/34:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/35:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][200])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/36:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][188])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/37:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][78])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/38:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][14])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/39:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][32])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/40:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][328])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/41:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][900])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/42:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app2")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/43:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/44:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/45:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames, axis=1, sort=False)
284/46:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/47:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[1])
lugares_DF=pd.DataFrame(lugares_unique)
284/48:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[1])
lugares_DF=pd.DataFrame(lugares_unique)
len(lugares_DF)
284/49:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares_load= np.unique(lugares, return_index=False)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/50:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/51:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames, axis=1, sort=False)
284/52:
frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(frames, axis=1, sort=False)
Data_Book_all.to_csv("Data_Book.csv")
284/53: lugares_DF
284/54: Union=pd.merge(Data_Book, lugares_DF)
284/55: lugares_DF["0"]
284/56: lugares_DF[0]
284/57: lugares_DF[1]
284/58: lugares_DF[0]
284/59: lugares_DF["lugares"]=lugares_DF[0]
284/60: Union=pd.merge(Data_Book, lugares_DF)
284/61:
Union=pd.merge(Data_Book, lugares_DF)
Union
284/62:
#frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
Data_Book_all=pd.concat(Union, axis=1, sort=False)
Data_Book_all.to_csv("Data_Book.csv")
284/63:
#frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
#Data_Book_all=pd.concat(Union, axis=1, sort=False)
Union.to_csv("Data_Book.csv")
284/64:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
2210
lugares[i],Frases_Book[i]
284/65:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=25
lugares[i],Frases_Book[i]
284/66:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=26
lugares[i],Frases_Book[i]
284/67:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=27
lugares[i],Frases_Book[i]
284/68:
Union=pd.merge(lugares_DF, Data_Book)
Union
284/69:
Union=pd.merge(Data_Book, lugares_DF)
Union
284/70:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF["lugares"]=lugares_DF[0]
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/71:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, 5, 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF["lugares"]=lugares_DF[0]
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/72:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/73: lugares_DF
284/74: lugares_DF
284/75: lugares_DF[0]
284/76: lugares_DF[-1]
284/77:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/78: lugares_DF
284/79:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, 5, 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF["lugares"]=lugares_DF[0]
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/80:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/81: lugares_DF
284/82: lugares_DF["lugares"]=lugares_DF[0]
284/83:
lugares_DF["lugares"]=lugares_DF[0]
lugares_DF
284/84:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=28
lugares[i],Frases_Book[i]
284/85:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=29
lugares[i],Frases_Book[i]
284/86:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=80
lugares[i],Frases_Book[i]
284/87:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=90
lugares[i],Frases_Book[i]
284/88:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=91
lugares[i],Frases_Book[i]
284/89:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=92
lugares[i],Frases_Book[i]
284/90:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=93
lugares[i],Frases_Book[i]
284/91:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=94
lugares[i],Frases_Book[i]
284/92:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=95
lugares[i],Frases_Book[i]
284/93:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=800
lugares[i],Frases_Book[i]
284/94:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=801
lugares[i],Frases_Book[i]
284/95:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=803
lugares[i],Frases_Book[i]
284/96: start_ent[0:10], end_ent[0:10]
284/97:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i]
284/98: start_ent[0:10], end_ent[0:10], lugares[0:10]
284/99: out[1]
284/100: spacy.displacy.render(Book2, style="ent", page="true")
284/101: #out[1]
284/102:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
284/103:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
284/104:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
284/105: #spacy.displacy.render(Book2, style="ent", page="true")
284/106: Data_Book[10:50]
284/107:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/108: lugares_DF.rename(columns={"0": "a"})
284/109: lugares_DF.rename(columns={0: "a"})
284/110:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, 5, 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF.rename(columns={0: "lugares"})
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/111:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/112: lugares_DF
284/113: lugares_DF
284/114:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, 5, 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF.rename(columns={0: "lugares"})
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/115:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/116: lugares_DF
284/117: lugares_DF.rename(columns={0: "lugares"})
284/118:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, 5, 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF=lugares_DF.rename(columns={0: "lugares"})
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/119:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/120: lugares_DF
284/121:
Union=pd.merge(Data_Book, lugares_DF, left_index=True)
Union
284/122:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
Union
284/123:

geolocator = Nominatim(user_agent="Travel-app2")
location = geolocator.geocode(Data_Book["lugares"][900])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
284/124:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=False)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app2")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/125:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/126:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON", "LAT", "address"])
Data_Book_Short
284/127: Union_2=pd.merge(Data_Book, Data_Book_Short, left_index=True, right_index=True)
284/128:
Union_2=pd.merge(Data_Book, Data_Book_Short, left_index=True, right_index=True)
Union_2
284/129:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON_2", "LAT_2", "address"])
Data_Book_Short
284/130:
Union_2=pd.merge(Data_Book, Data_Book_Short, left_index=True, right_index=True)
Union_2
284/131:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON_2", "LAT_2", "address"])
Data_Book_Short
284/132:
Union_2=pd.merge(Union, Data_Book_Short, left_index=True, right_index=True)
Union_2
284/133:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=False)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app2")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
            lat.append(None)
            lon.append(None)
            address.append(None)
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
284/134:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
284/135:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON_2", "LAT_2", "address"])
Data_Book_Short
284/136:
Union_2=pd.merge(Union, Data_Book_Short, left_index=True, right_index=True)
Union_2
284/137: Union_2.to_csv("Data_Book_short.csv")
284/138: Data_Book[0:10]
284/139:
#frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
#Data_Book_all=pd.concat(Union, axis=1, sort=False)
Union.to_csv("Data_Book.csv")
284/140:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame(pos, lugares_load)
    lugares_DF["LAT"]= None
    lugares_DF["LON"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_load), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    lugares_DF=lugares_DF.rename(columns={0: "lugares"})
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
284/141:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
284/142: lugares_DF
284/143:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
Union
284/144: len(Data_Book)
284/145: len(lugares_DF)
284/146:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=False)
Union
284/147:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_on)
Union
284/148:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_on("lugares"))
Union
284/149:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, right_on="lugares")
Union
284/150:
Union=pd.merge(Data_Book, lugares_DF)
Union
284/151:
Union=pd.merge(Data_Book, lugares_DF, left_index=True)
Union
284/152:
Union=pd.merge(Data_Book, lugares_DF, left_index=True, righ_index=True)
Union
284/153:
Union=pd.merge(Data_Book, lugares_DF, how='left')
Union
284/154:
#frames=[Data_Book, lugares_DF["LAT"], lugares_DF["LON"]]
#Data_Book_all=pd.concat(Union, axis=1, sort=False)
Union.to_csv("Data_Book.csv")
284/155: len(Book)
284/156:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i]
284/157: start_ent[78:80], end_ent[78:80], lugares[78:80]
284/158:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82535: 83389]
284/159: start_ent[78:80], end_ent[78:80], lugares[78:80]
284/160:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82535:82539,]
284/161:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82535:82539]
284/162:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82539]
284/163:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82550]
284/164:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/165:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/166:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i]
284/167:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i]
284/168:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i]
284/169:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i]
284/170: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
284/171: len(Data_Book)
284/172: #Data_Book.to_csv("Data_Book.csv")
284/173: Data_Book[0:10]
284/174:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[70:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/175: len(Book)
284/176:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/177:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i]
284/178: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
284/179: len(Data_Book)
284/180: #Data_Book.to_csv("Data_Book.csv")
284/181: Data_Book[0:10]
284/182:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=9
lugares[i],Frases_Book[i]
284/183:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=8
lugares[i],Frases_Book[i]
284/184:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=7
lugares[i],Frases_Book[i]
284/185:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[70:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/186: len(Book)
284/187:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/188:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=7
lugares[i],Frases_Book[i]
284/189:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/190: len(Book)
284/191:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/192:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=7
lugares[i],Frases_Book[i]
284/193:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i]
284/194:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/195: len(Book)
284/196:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/197:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i]
284/198:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i]
284/199:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[79:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/200: len(Book)
284/201:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/202:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i]
284/203:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i]
284/204:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i]
284/205:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[78:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/206: len(Book)
284/207:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/208:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i]
284/209:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i]
284/210:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[77:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
284/211: len(Book)
284/212:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
284/213:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i]
284/214:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i]
284/215:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            continue
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            continue
        else:
            f-=1
286/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
286/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
286/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
286/4: out=epub2text(Book)
286/5: #out[1]
286/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
286/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
286/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
286/9: #spacy.displacy.render(Book2, style="ent", page="true")
286/10:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82550]
286/11:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82550]
286/12:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
286/13:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
286/14:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i]
286/15:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i]
286/16:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=77
lugares[i],Frases_Book[i]
286/17:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i]
286/18:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            #break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            #break
        else:
            f-=1
287/1:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[49:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/2:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82550]
287/3:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
287/4:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
287/5:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
287/6:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
287/7:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
287/8: out=epub2text(Book)
287/9: #out[1]
287/10:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
287/11:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
287/12:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
287/13: #spacy.displacy.render(Book2, style="ent", page="true")
287/14:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[82530:82550]
287/15:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[49:80]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/16: len(Book)
287/17:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/18:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=29
lugares[i],Frases_Book[i]
287/19:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=28
lugares[i],Frases_Book[i]
287/20:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=27
lugares[i],Frases_Book[i]
287/21:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=30
lugares[i],Frases_Book[i]
287/22:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=31
lugares[i],Frases_Book[i]
287/23: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
287/24: Data_Book[0:10]
287/25:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=28
lugares[i],Frases_Book[i]
287/26:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=27
lugares[i],Frases_Book[i]
287/27:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=26
lugares[i],Frases_Book[i]
287/28:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/29: len(Book)
287/30:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/31:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=26
lugares[i],Frases_Book[i]
287/32:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=25
lugares[i],Frases_Book[i]
287/33:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=24
lugares[i],Frases_Book[i]
287/34:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=23
lugares[i],Frases_Book[i]
287/35:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=22
lugares[i],Frases_Book[i]
287/36:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=21
lugares[i],Frases_Book[i]
287/37: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
287/38:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=20
lugares[i],Frases_Book[i]
287/39:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=19
lugares[i],Frases_Book[i]
287/40:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=18
lugares[i],Frases_Book[i]
287/41:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=17
lugares[i],Frases_Book[i]
287/42:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=16
lugares[i],Frases_Book[i]
287/43:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=15
lugares[i],Frases_Book[i]
287/44:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=14
lugares[i],Frases_Book[i]
287/45:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=13
lugares[i],Frases_Book[i]
287/46:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=12
lugares[i],Frases_Book[i]
287/47:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=11
lugares[i],Frases_Book[i]
287/48:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
287/49:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=9
lugares[i],Frases_Book[i]
287/50:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=8
lugares[i],Frases_Book[i]
287/51:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=7
lugares[i],Frases_Book[i]
287/52:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=6
lugares[i],Frases_Book[i]
287/53:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=6
lugares[i],Frases_Book[i]
287/54:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=5
lugares[i],Frases_Book[i]
287/55:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i]
287/56:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i]
287/57:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i]
287/58:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i]
287/59:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i]
287/60:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=19
lugares[i],Frases_Book[i]
287/61:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=19
lugares[i],Frases_Book[i], tart_frase[i], end_frase[i]
287/62:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=19
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/63:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=18
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/64:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/65:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/66:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/67:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i]
287/68:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i]
287/69:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/70:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1
                
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/71: len(Book)
287/72:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/73:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/74:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/75:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=14
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/76:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=15
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/77:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[3417: 3440]
287/78:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[3417: 3450]
287/79:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[3410: 3450]
287/80:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[3350: 3450]
287/81:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[3340: 3450]
287/82:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=14
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/83:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=13
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/84:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/85:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=77
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/86:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=14
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/87:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=13
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/88:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=12
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/89:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=7
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/90:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=6
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/91:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=5
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/92:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/93:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:5]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1

    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/94: len(Book)
287/95:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/96:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/97:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/98:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:6]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1

    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f-=1
            break
        else:
            f-=1
287/99: len(Book)
287/100:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/101:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/102:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/103:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/104:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/105:
start_ent[78:80], end_ent[78:80], lugares[78:80]

Book[300: 660]
287/106:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:6]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            break
        else:
            z+=1

    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f=f-1
            break
        else:
            f=f-1
287/107: len(Book)
287/108:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/109:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/110:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:6]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            pass
        else:
            z+=1

    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f=f-1
            pass
        else:
            f=f-1
287/111: len(Book)
287/112:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
287/113:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_frase[i], end_frase[i], start_ent[i], end_ent[i]
287/114:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:80]:
    z=i
    f=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            z+=1
            pass
        else:
            z+=1

    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            f=f-1
            pass
        else:
            f=f-1
287/115: len(Book)
287/116:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
289/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
289/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
289/4: out=epub2text(Book)
289/5: out[5]
289/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
289/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
289/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
289/9: len(lugares)
289/10: start_ent[0:10]
289/11:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/12:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/13:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i]
289/14:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
410
lugares[i],Frases_Book[i]
289/15:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i]
289/16:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i]
289/17:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i]
289/18:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i]
289/19:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/20:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:10]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            pass
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            pass
        else:
            f-=1
289/21:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:4]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            pass
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            pass
        else:
            f-=1
289/22:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:4]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            pass
        else:
            z+=1
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/23:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:4]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:4]:
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/24:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/25:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/26:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[0:5]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:5]:
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/27:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/28:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/29:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[::]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[::]:
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/30:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/31:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/32:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:5]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            #print(Book[z],z)
            end_frase.append(z)
            #break
        else:
            z+=1
    
for i in start_ent[:5]:
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            #break
        else:
            f-=1
289/33:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:1]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            #break
        else:
            z+=1
    
for i in start_ent[:1]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            #break
        else:
            f-=1
289/34:
len(start_ent)
start_frase=[]
end_frase=[]
for i in start_ent[:1]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:1]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/35:
len(start_ent)
start_frase=[]
end_frase=[]
o=5
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/36:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/37:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/38:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/39:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/40:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/41:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/42:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/43:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/44:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/45:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/46:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/47:
len(start_ent)
start_frase=[]
end_frase=[]
o=6
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/48:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/49:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/50:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/51:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            #print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/52:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            print(Book[f],f)
            start_frase.append(f)
            break
        else:
            f-=1
289/53:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/54:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    print("start aqui")
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/55:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    print(f)
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/56:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    print(Book[f])
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/57:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:o]:
    z=i
    print(Book[z])
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:o]:
    f=i
    print(Book[f])
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/58:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:15]:
    z=i
    print(Book[z])
    while z<len(Book):
        if Book[z]==".":
            print(Book[z],z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:15]:
    f=i
    while f>0:
        if Book[f]==".":
            print("start aqui")
            start_frase.append(f)
            break
        else:
            f-=1
289/59:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:15]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:15]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui")
            break
        else:
            f-=1
289/60:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:15]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:15]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/61:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/62:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/63:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/64:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/65:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/66:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/67:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=2
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/68:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=3
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/69:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=4
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/70:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=5
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/71:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/72: Book[192:200]
289/73: Book[0:200]
289/74: Book[0:250]
289/75:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
289/76:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
289/77: out=epub2text(Book)
289/78: out[5]
289/79: out=epub2text(Book)
289/80: out[5]
289/81:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
289/82: start_ent[0:10]
289/83: [0:250]
289/84: Book[0:250]
289/85:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
289/86:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
289/87:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
289/88: len(lugares)
289/89: start_ent[0:10]
289/90: Book[0:250]
289/91:
len(start_ent)
start_frase=[]
end_frase=[]
o=1
for i in start_ent[:15]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:15]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/92:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/93:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/94:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[:0]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:0]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/95:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/96:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/97:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[:1]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/98:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/99:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/100:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:1]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/101:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/102:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/103:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("start aqui", z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:1]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/104:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:1]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/105: start_ent[0:1]
289/106: start_ent[0:2]
289/107: start_ent[0]
289/108: start_ent[0:1]
289/109: start_ent[1]
289/110: start_ent[0]
289/111: start_ent[1]
289/112:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:10]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f)
            break
        else:
            f-=1
289/113: start_ent[1]
289/114:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/115:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/116:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for i in start_ent[0:10]:
    f=i
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,i)
            break
        else:
            f-=1
289/117:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,i)
            break
        else:
            f-=1
289/118:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
289/119: start_ent[0:10]
289/120: start_ent[0:10], end_ent[0:10]
289/121:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
289/122:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:1]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            print(f)
289/123:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:1]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            print(f)
        if f<0:
            start_frase.append(null)
289/124:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:1]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            print(f)
        if f<0:
            start_frase.append(Null)
289/125:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:1]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            print(f)
        if f<0:
            start_frase.append("Null")
289/126:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:1]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:1]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            #print(f)
        if f<0:
            start_frase.append("Null")
289/127: start_ent[0:10], end_ent[0:10]
289/128:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/129: start_ent[0:10], end_ent[0:10], start_frase[0]
289/130: start_ent[0:10], end_ent[0:10], start_frase[0:10]
289/131:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            #print(f)
        if f<0:
            start_frase.append("Null")
289/132:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]=="." and Book[z]=="\n":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>=0:
        if Book[f]=="." and Book[f]=="\n":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            #print(f)
        if f<0:
            start_frase.append("Null")
289/133: start_ent[0:10], end_ent[0:10], start_frase[0:10]
289/134:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[0:10]:
    z=i
    
    while z<len(Book):
        if Book[z]=="." or Book[z]=="\n":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[0:10]:
    f=x
    while f>=0:
        if Book[f]=="." or Book[f]=="\n":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            #print(f)
289/135: start_ent[0:10], end_ent[0:10], start_frase[0:10]
289/136:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/137:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/138:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/139:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]=="." or Book[z]=="\n":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]=="." or Book[f]=="\n":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            #print(f)
289/140: start_ent[0:10], end_ent[0:10], start_frase[0:10]
289/141:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/142:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=10
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/143:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/144:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/145:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/146: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
289/147: Data_Book.to_csv("Data_Book.csv")
289/148: Data_Book[:5]
289/149:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]=="." or Book[z]=="\n":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]=="." or Book[f]=="\n":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
289/150: start_ent[0:10], end_ent[0:10], start_frase[0:10]
289/151: start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
289/152:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/153:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/154:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/155:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/156:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase)
289/157:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
289/158:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=14
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/159:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=15
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/160: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
289/161: Data_Book.to_csv("Data_Book.csv")
289/162:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=18
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/163:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/164: Data_Book[:5]
289/165:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
289/166:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
289/167:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/168:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/169:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=78
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/170:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/171: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
289/172: #Frases_Book[:5]
289/173: Data_Book.to_csv("Data_Book.csv")
289/174: Data_Book[:5]
289/175:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_Book.append(Book[start_frase[z]:end_frase[z]])
    z+=1
289/176:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/177:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/178:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_Book.append.Frases_clean
    z+=1
289/179:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/180:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_Book.append(Frases_clean)
    z+=1
289/181:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/182: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
289/183: #Frases_Book[:5]
289/184: Data_Book.to_csv("Data_Book.csv")
289/185: Data_Book[:5]
289/186:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean_1=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_clean_2=Frases_clean_1.replace("  ", "")
    Frases_Book.append(Frases_clean)
    z+=1
289/187:
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean_1=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_clean_2=Frases_clean_1.replace("  ", "")
    Frases_Book.append(Frases_clean_2)
    z+=1
289/188:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/189:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/190:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean_1=(Book[start_frase[z]:end_frase[z]]).replace("\n", "")
    Frases_clean_2=Frases_clean_1.replace("  ", "")
    Frases_Book.append(Frases_clean_2)
    z+=1
289/191:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=(Book[start_frase[z]:end_frase[z]]).re.sub('\n |  |\. ', '', line)
    
    Frases_Book.append(Frases_clean)
    z+=1
289/192:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
    
    Frases_Book.append(Frases_clean)
    z+=1
289/193:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=79
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/194:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/195:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
289/196: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
289/197: #Frases_Book[:5]
289/198: Data_Book.to_csv("Data_Book.csv")
289/199: Data_Book[:5]
289/200:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
289/201:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
289/202: lugares_DF
289/203:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
289/204:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
289/205: lugares_DF
289/206:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    lugares_load, pos= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
289/207:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
289/208: lugares_DF
289/209:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
289/210:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
289/211: lugares_DF
289/212:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF
289/213:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
289/214:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2.head(10)
289/215:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
Data_Book_2.head(10)
289/216:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
289/217:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=False)
len(Data_Book_2)
289/218:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
289/219: Data_Book_2.head(10)
289/220:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_on="lugares", right_on="lugares")
len(Data_Book_2)
289/221: Data_Book_2.head(10)
289/222:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_on="lugares")
len(Data_Book_2)
289/223: Data_Book_2.head(10)
289/224:
Data_Book_2=pd.merge(lugares_DF, Data_Book,  left_index=True, right_index=True)
len(Data_Book_2)
289/225: Data_Book_2.head(10)
289/226:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_on="lugares_x")
len(Data_Book_2)
289/227: Data_Book_2.head(10)
289/228:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_on="lugares_x", right_on="lugares_y")
len(Data_Book_2)
289/229: Data_Book_2.head(10)
289/230:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_on="lugares", right_on="lugares")
len(Data_Book_2)
289/231: Data_Book_2.head(10)
289/232:
Data_Book_2=pd.merge_ordered(lugares_DF, Data_Book, fill_method="ffill", left_by="group"
len(Data_Book_2)
289/233:
Data_Book_2=pd.merge_ordered(lugares_DF, Data_Book, fill_method="ffill", left_by="group")
len(Data_Book_2)
289/234: Data_Book_2.head(10)
289/235:
Data_Book_2=pd.merge_ordered(lugares_DF, Data_Book, fill_method="ffill", left_by="lugares")
len(Data_Book_2)
289/236: Data_Book_2.head(10)
289/237: Data_Book_2.head(25)
289/238:
Data_Book_2.sort_values(by=['Position'])
Data_Book_2.head(25)
289/239:
Data_Book_2.sort_values(by=['Position'], ascending=True)
Data_Book_2.head(25)
289/240:
Data_Book_2=Data_Book_2.sort_values(by=['Position'], ascending=True)
Data_Book_2.head(25)
289/241:
Data_Book_2=Data_Book_2.sort_index()
Data_Book_2.head(25)
289/242:
Data_Book_2=pd.merge_ordered(Data_Book,lugares_DF, fill_method="ffill", left_by="lugares")
len(Data_Book_2)
289/243:
Data_Book_2=Data_Book_2.sort_index()
Data_Book_2.head(25)
289/244:
Data_Book_2=pd.merge_ordered(Data_Book,lugares_DF, fill_method="ffill", left_by="lugares")
len(Data_Book_2)
289/245:

Data_Book_2.head(25)
289/246:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by=index)
len(Data_Book_2)
289/247:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by="index")
len(Data_Book_2)
289/248:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by="lugares")
len(Data_Book_2)
289/249:

Data_Book_2.head(25)
289/250:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book=Data_Book.set_index('Order')
289/251:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book=Index.rename("Order", inplace=False)
289/252:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book.index.name="lala"
289/253: Data_Book[:5]
289/254:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book.index.name="Order"
289/255: #Frases_Book[:5]
289/256: Data_Book.to_csv("Data_Book.csv")
289/257: Data_Book.to_csv("Data_Book.csv")
289/258: Data_Book[:5]
289/259:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF
289/260:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by="lugares")
len(Data_Book_2)
289/261:

Data_Book_2.head(25)
289/262: Data_Book["Order"]
289/263: Data_Book[:5]
289/264:
Data_Book[:5]
Data_Book["Order"]
289/265: Data_Book[:5]
289/266:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by="lugares", how='left')
len(Data_Book_2)
289/267:

Data_Book_2.head(25)
289/268:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF
289/269:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", left_by="lugares", how='left')
len(Data_Book_2)
289/270:

Data_Book_2.head(25)
289/271:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, fill_method="ffill", how='left')
len(Data_Book_2)
289/272:

Data_Book_2.head(25)
289/273:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)
289/274:

Data_Book_2.head(25)
289/275:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='right')
len(Data_Book_2)
289/276:

Data_Book_2.head(25)
289/277:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='outside')
len(Data_Book_2)
289/278:

Data_Book_2.head(25)
289/279:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)
289/280:

Data_Book_2.head(25)
289/281:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)

Data_Book
289/282:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)

Data_Book.index
289/283:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)

Data_Book.index[0]
289/284:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book["order"]=Data_Book.index
289/285: Data_Book[:5]
289/286: Data_Book.to_csv("Data_Book.csv")
289/287: Data_Book[:5]
289/288:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, how='left')
len(Data_Book_2)

Data_Book.index
289/289:

Data_Book_2.head(25)
289/290:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, by="lugares")
len(Data_Book_2)
289/291:

Data_Book_2.head(25)
289/292:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, left_by="lugares")
len(Data_Book_2)
289/293:

Data_Book_2.head(25)
289/294:
Data_Book_2=pd.merge_ordered(Data_Book, lugares_DF, left_by="lugares")
Data_Book_2=Data_Book_2.sort_values("order")
len(Data_Book_2)
289/295:

Data_Book_2.head(25)
289/296:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
289/297:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][0])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
289/298:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
289/299:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
289/300:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=False)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
289/301:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
289/302:
Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON_geopy", "LAT_geopy", "address"])
Data_Book_Short
289/303: Data_book_3=pd.merge(Data_Book_2, Data_Book_Short, left_by="lugares")
289/304: Data_book_3=pd.merge_ordered(Data_Book_2, Data_Book_Short, left_by="lugares")
289/305: Data_book_3.head(10)
289/306:
Data_book_3=pd.merge_ordered(Data_Book_2, Data_Book_Short, left_by="lugares")
Data_book_3=Data_book_3.sort_values("order")
289/307: Data_book_3.head(10)
289/308: Data_Book_3.to_csv("Data_Book_Geocode-py.csv")
289/309: Data_Book_3.head(10)
289/310:
Data_Book_3=pd.merge_ordered(Data_Book_2, Data_Book_Short, left_by="lugares")
Data_Book_3=Data_book_3.sort_values("order")
289/311: Data_Book_3.head(10)
289/312: Data_Book_3.to_csv("Data_Book_Geocode-py.csv")
293/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
293/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
295/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
297/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
297/2:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
297/3:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df["LAT_google"]
297/4:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
297/5:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/6:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
#df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/7:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/8: dups = df.pivot_table(index = ['LAT_google','LON_google'], aggfunc ='size')
297/9: len(dups)
297/10: dups.head(10)
297/11: dups
297/12: dups["size"]
297/13:
used_features =['LAT_google','LON_google']

df['is_duplicated'] = df.duplicated(used_features)
df['is_duplicated'].sum()
297/14:
used_features =['LAT_google','LON_google']

df['is_duplicated'] = df.duplicated(used_features)
df
297/15: dups =df.groupby(['LAT_google','LON_google']).sum()
297/16: len(dups)
297/17:
len(dups)
dups
297/18:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/19: dups =df.groupby(['LAT_google','LON_google']).sum()
297/20:
len(dups)
dups
297/21:
len(dups)
dups.head(10)
297/22:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/23: dups =df.groupby(['LAT_google','LON_google']).sum()
297/24: dups =df.groupby(['LAT_google','LON_google']).sum()
297/25: len(dups)
297/26: dups =df.groupby(['lugares']).sum()
297/27: len(dups)
297/28:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/29: dups =df.groupby(['LAT_google','LON_google']).sum()
297/30: dups =df.groupby(['LAT_google','LON_google'])
297/31: len(dups)
297/32:
len(dups)
dups
297/33: dups =df.groupby(['LAT_google','LON_google']).sum()
297/34:
len(dups)
dups
297/35:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/36: df["count"] =df.groupby(['LAT_google','LON_google']).sum()
297/37:
len(df)
df
297/38: df_2=df.groupby(['LAT_google','LON_google']).sum()
297/39:
len(df_2)
df_2
297/40:
len(df_2)
df_2[0]
297/41:
len(df_2)
df_2
297/42:
len(df_2)
df_2.head()
297/43:
len(df_2)
df_2.head(10)
297/44:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/45: df_2=df.groupby(['LAT_google','LON_google']).sum()
297/46:
len(df_2)
df_2.head(10)
297/47:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
297/48:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
297/49:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(10)
# df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
# len(df)
# df.head(5)
297/50:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
# df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
# len(df)
# df.head(5)
297/51:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
297/52:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11'], axis=1)
297/53:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11'], axis=1)
df
297/54:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', "LON_geopy","LAT_geopy", "address"], axis=1)
df
297/55:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df
297/56: df_2=df.groupby(['LAT_google','LON_google']).sum()
297/57:
len(df_2)
df_2.head(10)
297/58: df_2=df.groupby(['LAT_google','LON_google']).count()
297/59:
len(df_2)
df_2.head(10)
297/60: df_2=df.groupby(['LAT_google','LON_google'])
297/61:
len(df_2)
df_2.head(10)
297/62:
len(df_2)
#df_2.head(10)
297/63:
len(df_2)
df_2.head(10)
297/64:
#df_2=df.groupby(['LAT_google','LON_google'])
df_2=df['LAT_google','LON_google'].value_counts(bins=4)
297/65:
#df_2=df.groupby(['LAT_google','LON_google'])
df_2=df['LAT_google'].value_counts(bins=4)
297/66:
len(df_2)
df_2.head(10)
297/67:
#df_2=df.groupby(['LAT_google','LON_google'])
df_2=df['LAT_google'].value_counts(bins=10)
297/68:
len(df_2)
df_2.head(10)
297/69:
#df_2=df.groupby(['LAT_google','LON_google'])
df_2=df['LAT_google'].value_counts(bins=20)
297/70:
len(df_2)
df_2.head(10)
297/71:
len(df_2)
df_2
297/72:
df_2=df.groupby(['LAT_google','LON_google']).size()
#df_2=df['LAT_google'].value_counts(bins=20)
297/73:
len(df_2)
df_2
297/74:
len(df_2)
df_2.head()
297/75:
len(df_2)
df_2.head(20)
297/76:
df_2=df.groupby(['LAT_google','LON_google']).size().reset_index(name="Time")
#df_2=df['LAT_google'].value_counts(bins=20)
297/77:
len(df_2)
df_2.head(20)
297/78:
df_2=df.groupby(['LAT_google','LON_google']).size().reset_index(name="Times")
#df_2=df['LAT_google'].value_counts(bins=20)
297/79:
len(df_2)
df_2.head(20)
297/80:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=20)
297/81:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=200)
297/82:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=10)
297/83:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=20)
297/84:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
297/85:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(X[X.columns[1:3]]) # Compute k-means clustering.
X['cluster_label'] = kmeans.fit_predict(X[X.columns[1:3]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(X[X.columns[1:3]]) # Labels of each point
X.head(10)
297/86:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
297/87:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(50)
297/88:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(50)
297/89:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
297/90:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 6], centers[:, 7], c='black', s=200, alpha=0.5)
297/91:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
297/92:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 6], centers[:, 7], c='black', s=200, alpha=0.5)
297/93:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
297/94:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
plt.scatter(c='black', s=200, alpha=0.5)
297/95:
df.plot.scatter(x = 'LAT_google', y = 'LON_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
297/96:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
297/97:
kmeans = KMeans(n_clusters = 5, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
297/98:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
299/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
299/2:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df
299/3:
df_2=df.groupby(['LAT_google','LON_google']).size().reset_index(name="Times")
#df_2=df['LAT_google'].value_counts(bins=20)
299/4:
len(df_2)
df_2.head(20)
299/5:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=20)
299/6:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
299/7:
kmeans = KMeans(n_clusters = 5, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
299/8:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
299/9:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
299/10:
df_2=df.groupby(['LAT_google','LON_google']).size().reset_index(name="Times")
df_2=df[['LAT_google','LON_google']].value_counts(bins=20)
299/11:
df_2=df.groupby(['LAT_google','LON_google']).size().reset_index(name="Times")
#df_2=df[].value_counts(bins=20)
299/12:
len(df_2)
df_2.head(20)
299/13:
df_2=df.groupby(bins)(['LAT_google','LON_google']).agg(['count', 'sum'])
#df_2=df[].value_counts(bins=20)
#df_2=df.groupby(bins)['Value'].agg(['count', 'sum'])
299/14:
len(df_2)
df_2.head(20)
299/15:
df_2=df.groupby(['LAT_google','LON_google']).agg(['count', 'sum'])
#df_2=df[].value_counts(bins=20)
#df_2=df.groupby(bins)['Value'].agg(['count', 'sum'])
299/16:
len(df_2)
df_2.head(20)
299/17:
df_2=df.groupby(['LAT_google','LON_google']).agg(['count'])
#df_2=df[].value_counts(bins=20)
#df_2=df.groupby(bins)['Value'].agg(['count', 'sum'])
299/18:
len(df_2)
df_2.head(20)
299/19:
def get_unique_number(lat, lon):
  try:
    lat_double = None
    lon_double = None
    if isinstance(lat, str):
        lat_double = float(lat)
    else:
        lat_double = lat
    if isinstance(lon, str):
        lon_double = float(lon)
    else:
        lon_double = lon

    lat_int = int((lat_double * 1e7))
    lon_int = int((lon_double * 1e7))
    val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
    val = val % 2147483647
    return val
except Exception as e:
    print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
    print("Exception while generating od loc id")
    print(traceback.format_exc())
    return None
299/20:
def get_unique_number(lat, lon):
    try:
        lat_double = None
        lon_double = None
        if isinstance(lat, str):
            lat_double = float(lat)
        else:
            lat_double = lat
        if isinstance(lon, str):
            lon_double = float(lon)
        else:
            lon_double = lon

        lat_int = int((lat_double * 1e7))
        lon_int = int((lon_double * 1e7))
        val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
        val = val % 2147483647
        return val

except Exception as e:
    print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
    print("Exception while generating od loc id")
    print(traceback.format_exc())
    return None
299/21:
def get_unique_number(lat, lon):
    try:
        lat_double = None
        lon_double = None
        if isinstance(lat, str):
            lat_double = float(lat)
        else:
            lat_double = lat
        if isinstance(lon, str):
            lon_double = float(lon)
        else:
            lon_double = lon

        lat_int = int((lat_double * 1e7))
        lon_int = int((lon_double * 1e7))
        val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
        val = val % 2147483647
        return val

# except Exception as e:
#     print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
#     print("Exception while generating od loc id")
#     print(traceback.format_exc())
#     return None
299/22:
def get_unique_number(lat, lon):
    try:
        lat_double = None
        lon_double = None
        if isinstance(lat, str):
            lat_double = float(lat)
        else:
            lat_double = lat
        if isinstance(lon, str):
            lon_double = float(lon)
        else:
            lon_double = lon

        lat_int = int((lat_double * 1e7))
        lon_int = int((lon_double * 1e7))
        val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
        val = val % 2147483647
        return val

# except Exception as e:
#     print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
#     print("Exception while generating od loc id")
#     print(traceback.format_exc())
#     return None
299/23:
def get_unique_number(lat, lon):
    try:
        lat_double = None
        lon_double = None
        if isinstance(lat, str):
            lat_double = float(lat)
        else:
            lat_double = lat
        if isinstance(lon, str):
            lon_double = float(lon)
        else:
            lon_double = lon

        lat_int = int((lat_double * 1e7))
        lon_int = int((lon_double * 1e7))
        val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
        val = val % 2147483647
    return val

# except Exception as e:
#     print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
#     print("Exception while generating od loc id")
#     print(traceback.format_exc())
#     return None
299/24:
def get_unique_number(lat, lon):
    try:
        lat_double = None
        lon_double = None
        if isinstance(lat, str):
            lat_double = float(lat)
        else:
            lat_double = lat
        if isinstance(lon, str):
            lon_double = float(lon)
        else:
            lon_double = lon

        lat_int = int((lat_double * 1e7))
        lon_int = int((lon_double * 1e7))
        val = abs(lat_int << 16 & 0xffff0000 | lon_int & 0x0000ffff)
        val = val % 2147483647
        return val

# except Exception as e:
#     print("marking OD_LOC_ID as -1 getting exception inside get_unique_number function")
#     print("Exception while generating od loc id")
#     print(traceback.format_exc())
#     return None
299/25: df["unique_value"]=df["LON_google"]*1000+df["LAT_google"]
299/26:
df["unique_value"]=df["LON_google"]*1000+df["LAT_google"]
df["unique_value"]
299/27: df_2=df["unique_value"].value_counts(bins=20)
299/28:
df_2=df["unique_value"].value_counts(bins=20)
df_2
299/29:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/30:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0]
299/31:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0:2]
299/32:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0]
299/33:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/34:
df_2=df["unique_value"].value_counts(bins=10)
df_2[1]
299/35:
df_2=df["unique_value"].value_counts(bins=10)
df_2[2]
299/36:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0][1]
299/37:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0]
299/38:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0:40]
299/39:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0:1]
299/40:
df_2=df["unique_value"].value_counts(bins=10)
df_2[0:1][0]
299/41:
df["unique_value"]=df["LON_google"]*1000+df["LAT_google"]
df["unique_value"]
df
299/42: df_2=df["unique_value"].value_counts(bins=10)
299/43:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/44:
df_2=df["unique_value"].value_counts(bins=10)
df_2.1
299/45:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/46:
df_2=df["unique_value"].value_counts(bins=20)
df_2
299/47:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/48:
df_2=df["unique_value"].value_counts(bins=3)
df_2
299/49:
df_2=df["unique_value"].value_counts(bins=4)
df_2
299/50:
df_2=df["unique_value"].value_counts(bins=5)
df_2
299/51:
df_2=df["unique_value"].value_counts(bins=6)
df_2
299/52:
df_2=df["unique_value"].value_counts(bins=10)
df_2
299/53:
df_2=df["LON_google"].value_counts(bins=10)
df_3=df["LAT_google"].value_counts(bins=10)
299/54:
df_2=df["LON_google"].value_counts(bins=10)
df_3=df["LAT_google"].value_counts(bins=10)
df_2, df_3
299/55:
df_2=df["LON_google"].value_counts(bins=40)
df_3=df["LAT_google"].value_counts(bins=40)
df_2, df_3
299/56:
df_2=df["LON_google"].value_counts(bins=50)
df_3=df["LAT_google"].value_counts(bins=50)
df_2, df_3
299/57: len(df_2)
299/58: type(df_2)
299/59:
df_2=df["LON_google"].value_counts(bins=40)
df_3=df["LAT_google"].value_counts(bins=40)
df_2, df_3
299/60:
df_2=df["LON_google"].value_counts(bins=50)
df_3=df["LAT_google"].value_counts(bins=50)
df_2, df_3
303/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
303/2:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df
303/3:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
303/4:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
303/5:
kmeans = KMeans(n_clusters = 5, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
303/6:
df["unique_value"]=df["LON_google"]*1000+df["LAT_google"]
df["unique_value"]
df.head(5)
303/7: #
303/8:
df = pd.read_csv('Data_Book_Geocode-py.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df.head(5)
303/9:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
303/10:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
303/11:
kmeans = KMeans(n_clusters = 5, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(10)
303/12:
kmeans = KMeans(n_clusters = 5, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
303/13:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
303/14:
df_2=df["LON_google"].value_counts(bins=50)
df_3=df["LAT_google"].value_counts(bins=50)
df_2, df_3
303/15: df.value_counts(self, normalize=False, sort=True, ascending=False, bins=None)
303/16: df["LON_google"].value_counts(self, normalize=False, sort=True, ascending=False, bins=None)
303/17: df["LON_google"].value_counts(self, normalize=False, sort=True, ascending=False, bins=50)
303/18:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)
df_2, df_3
303/19: df_2[0]
303/20: df_2[10]
303/21: df_2[1]
303/22: df_2[0]
303/23: df_2[1]
303/24: df_2[2]
303/25: df_2[-1]
303/26: df_2[0:1]
303/27: type(df_2[0:1])
303/28: type(df_2[0:0])
303/29: type(df_2[0:2])
303/30: (df_2[0:2])
303/31: (df_2[0:3])
303/32: (df_2[0:3], df_3[0:3])
303/33: (df_2[0:4], df_3[0:4])
303/34: df_2[0:1]
303/35: df_2[0:1][0]
303/36: (df_2[0:4], df_3[0:4])
303/37: df_2[0:1][1]
303/38: df_2[0:1]
303/39:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_2, df_3
303/40:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).tolist()
df_2, df_3
303/41:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
303/42:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
303/43:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
value
303/44:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
values
303/45: json.loads(df["LON_google"].value_counts(bins=50, normalize=True).to_json())
303/46: import json
303/47: json.loads(df["LON_google"].value_counts(bins=50, normalize=True).to_json())
303/48:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).values()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
303/49:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).values
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys()
df_2, df_3
303/50:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).values
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).values
df_2, df_3
303/51:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).counts
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).values
df_2, df_3
303/52:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).counts()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).values
df_2, df_3
303/53:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).values
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).values
df_2, df_3
303/54:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).values
df_2, df_3
303/55:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2, df_3
303/56: (df_2[0:4], df_3[0:4])
303/57: df_2[0]
303/58: df_2[0:1]
303/59: df_2[1]
303/60: df_2[0:1]
303/61: type(df_2[0:1])
303/62:
df_2=df["LON_google"].value_counts(bins=60, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=60, normalize=True).to_frame()
df_2, df_3
303/63:
df_2=df["LON_google"].value_counts(bins=30, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=30, normalize=True).to_frame()
df_2, df_3
303/64:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2, df_3
303/65:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2, df_3
303/66:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2
303/67:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2[0]
303/68:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2[0][0]
303/69:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2[0][0:1]
303/70:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2[0]
303/71:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
df_2[0].to_frame()
303/72:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).index
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()
303/73: df_2
303/74: type(df_2)
303/75:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys().tolist()

df_2
303/76:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys().tolist()

print(df_2)
303/77:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys().tolist()

dd=df_2.to_frame()
303/78:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).keys().tolist()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).keys().tolist()

dd=df_2.to_frame
303/79:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame
303/80:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame
dd
303/81:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame
dd[1]
303/82:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame
303/83:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
303/84:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
dd
303/85:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
dd.first_valid_index
303/86:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
dd.index()
303/87:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
303/88:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

dd=df_2.to_frame()
dd
303/89:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()

dd=df_2.to_frame()
dd
303/90:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()

df_2
303/91:
df_2=df["LON_google"].value_counts(bins=50, normalize=True).to_frame()
df_3=df["LAT_google"].value_counts(bins=50, normalize=True).to_frame()

df_2.head(5)
303/92: df_2.index
303/93: df_2.index[0]
303/94: df_2.index[0][left]
303/95: df_2.index[0].left
303/96:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

df_2.head(5)
303/97:
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right
303/98:
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
303/99: min_lat
303/100: df_3.values
303/101: df_3.values[0]
303/102: df
303/103: Data_cleaned=df = df[~df.iloc["LAT_google"].between(min_lat, max_lat, inclusive=False)]
303/104: Data_cleaned= df[~df.iloc["LAT_google"].between(min_lat, max_lat, inclusive=False)]
303/105: df
303/106: Data_cleaned= df[~df.iloc[:,6].between(min_lat, max_lat, inclusive=False)]
303/107:
Data_cleaned= df[~df.iloc[:,6].between(min_lat, max_lat, inclusive=False)]
Data_cleaned
303/108:
Data_cleaned= df[~df.iloc[:,6].between(min_lat, max_lat, inclusive=False)]
Data_cleaned=Data_cleaned[~df.iloc[:,7].between(min_lon, max_lon, inclusive=False)]
303/109:
Data_cleaned= df[~df.iloc[:,6].between(min_lat, max_lat, inclusive=False)]
Data_cleaned=Data_cleaned[~df.iloc[:,7].between(min_lon, max_lon, inclusive=False)]
Data_cleaned
303/110:
Data_cleaned= df[~df.iloc[:,6].between(min_lat, max_lat, inclusive=False)]
Data_cleaned=Data_cleaned[~df.iloc[:,7].between(min_lon, max_lon, inclusive=False)]
len(Data_cleaned)
303/111: Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
303/112:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned
303/113:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(10)
303/114:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(20)
303/115:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/116:
Data_cleaned= df.iloc[:,7].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/117:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/118:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=True)
Data_cleaned.head(50)
303/119:
Data_cleaned= df.loc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/120:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/121: min_lat
303/122: min_lat, max_lat
303/123: min_lat, max_lat, min_lon, max_lon
303/124: #json.loads(df["LON_google"].value_counts(bins=50, normalize=True).to_json())
303/125:
Data_cleaned= df.iloc[:,6].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/126:
Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/127:
Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False).index
Data_cleaned.head(50)
303/128:
Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False).index()
Data_cleaned.head(50)
303/129:
Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/130:
Data_cleaned= df.loc[:,5].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/131:
Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False)
Data_cleaned.head(50)
303/132: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LON_google"] < max_lat)]
303/133: Data_clean
303/134: len(Data_clean)
303/135: Data_clean
303/136: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
303/137: Data_clean
303/138: Data_cleaned= df.iloc[:,5].between(min_lat, max_lat, inclusive=False)
303/139: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
303/140: Data_clean.to_csv("Data_clean.csv")
309/1:
from arcgis.gis import GIS
from arcgis.geocoding import geocode
309/2:
from arcgis.gis import GIS
from arcgis.geocoding import geocode
311/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
311/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
311/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
311/4: out=epub2text(Book)
311/5: #out[5]
311/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
311/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
311/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or ent.label_=="WORK_OF_ART":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
311/9: len(lugares)
311/10: start_ent[0:10]
311/11: Book[0:250]
311/12: #Book[0:250]
311/13:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
311/14:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            #print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
311/15:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
311/16:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
    
    Frases_Book.append(Frases_clean)
    z+=1
311/17:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
311/18: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
311/19: Data_Book.to_csv("Data_Book_2.0.csv")
311/20: Data_Book[:5]
311/21: Data_Book[50:55]
311/22:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
311/23:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
311/24:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF
311/25:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
311/26:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
Data_Book_2
311/27:
#Data_Book_all.to_csv("Data_Book.csv")
Data_Book["lugares"][0]
lugares_unique=np.unique(lugares, return_index=True)
len(lugares_unique[1])
311/28:
geolocator = Nominatim(user_agent="Travel-app")
location = geolocator.geocode(Data_Book["lugares"][1])
#print(location.address)
print((location.address))
print((location.latitude, location.longitude))
311/29:
import time
#Data_Book_short=Data_Book
#fff=Data_Book_short.drop_duplicates(subset ="lugares")


def coordinates_geopy(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    #Data_Book_short=Data_Book
    #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
    lat=[]
    lon=[]
    lugares_unique=np.unique(lugares, return_index=True)
    address=[]

    #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    geolocator = Nominatim(user_agent="Travel-app")
    #lugares_DF=pd.DataFrame(lugares)
    # lugares_DF["LAT"]= None
    # lugares_DF["LON"]= None
    #print ("Launching Google API...")
    all_points=len(lugares_unique)
    start_time = time.time()
    for i in range(0, all_points, 1):
        #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
        location = geolocator.geocode(lugares_unique[i])
        if location is None:
            print(i, " NONE ")
        else:
            lat.append(location.latitude)
            lon.append(location.longitude)
            address.append(location.address)
        # except:
        #     lat=None
        #     lon=None
        #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
            #print (i, sep=' ', end='', flush=False)
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    print("--- %s seconds ---" % (time.time() - start_time))
    return(lugares_unique, lon, lat, address)
311/30:
lugares_unique, lon, lat, address=coordinates_geopy(lugares)
#Data_Book_all=[Data_Book, lugares_DF]
311/31:
geolocator = Nominatim(user_agent="Travel-app_2")
location = geolocator.geocode(lugares)
print(location.address)
print((location.latitude, location.longitude))
311/32:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
311/33:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
311/34:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2.head(5)
311/35:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2.sort_values("Position")
311/36: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, index())), columns=["lugares","labels", "Quotes", "Position"])
311/37: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book), columns=["lugares","labels", "Quotes"])
311/38: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book), columns=["lugares","labels", "Quotes"]))
311/39: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book), columns=["lugares","labels", "Quotes"])))
311/40: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
311/41:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
DData_Book.index()
311/42: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
311/43: #Frases_Book[:5]
311/44: Data_Book.to_csv("Data_Book_2.0.csv")
311/45: Data_Book[50:55]
311/46:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2.sort_values("Position")
311/47:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2=Data_Book_2.sort_values("Position")
311/48:
# Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON", "LAT", "address"])
# Data_Book_Short
311/49:
# geolocator = Nominatim(user_agent="Travel-app")
# location = geolocator.geocode(Data_Book["lugares"][1])
# #print(location.address)
# print((location.address))
# print((location.latitude, location.longitude))
311/50: Data_Book_2.to_csv("Data_Book_Geocode.csv")
313/1:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df.head(5)
313/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
313/3:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', "LON_geopy","LAT_geopy", "address"], axis=1)
df.head(5)
313/4:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df.head(5)
313/5:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df.head(5)
313/6:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
313/7:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
313/8:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
313/9:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
313/10:
df_2=df["LON_google"].value_counts(bins=50, normalize=True)
df_3=df["LAT_google"].value_counts(bins=50, normalize=True)

df_2.head(5)
313/11:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=40, normalize=True)

df_2.head(5)
311/51:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
311/52: len(lugares)
311/53: start_ent[0:10]
311/54: #Book[0:250]
311/55:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            #print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
311/56:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
311/57:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
    
    Frases_Book.append(Frases_clean)
    z+=1
311/58:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
311/59: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
311/60: #Frases_Book[:5]
311/61: Data_Book.to_csv("Data_Book_2.0.csv")
311/62: Data_Book[50:55]
311/63:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
311/64:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
311/65:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF
311/66:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2=Data_Book_2.sort_values("Position")
311/67:
# #Data_Book_all.to_csv("Data_Book.csv")
# Data_Book["lugares"][0]
# lugares_unique=np.unique(lugares, return_index=True)
# len(lugares_unique[1])
311/68:
# geolocator = Nominatim(user_agent="Travel-app")
# location = geolocator.geocode(Data_Book["lugares"][1])
# #print(location.address)
# print((location.address))
# print((location.latitude, location.longitude))
311/69:
# import time
# #Data_Book_short=Data_Book
# #fff=Data_Book_short.drop_duplicates(subset ="lugares")


# def coordinates_geopy(lugares):
#     #lugares_load=np.load(lugares)
#     #lugares_Book=pandas.read_csv(Data_Book)
#     #Data_Book_short=Data_Book
#     #fff=Data_Book_short.drop_duplicates(subset ="lugares")  
#     lat=[]
#     lon=[]
#     lugares_unique=np.unique(lugares, return_index=True)
#     address=[]

#     #g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
#     geolocator = Nominatim(user_agent="Travel-app")
#     #lugares_DF=pd.DataFrame(lugares)
#     # lugares_DF["LAT"]= None
#     # lugares_DF["LON"]= None
#     #print ("Launching Google API...")
#     all_points=len(lugares_unique)
#     start_time = time.time()
#     for i in range(0, all_points, 1):
#         #geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        
#         location = geolocator.geocode(lugares_unique[i])
#         if location is None:
#             print(i, " NONE ")
#         else:
#             lat.append(location.latitude)
#             lon.append(location.longitude)
#             address.append(location.address)
#         # except:
#         #     lat=None
#         #     lon=None
#         #print ("coordinates: ", i, "of ", all_points, " extracted", sep=' ', end='', flush=True)
#             #print (i, sep=' ', end='', flush=False)
#     #lugares_DF.to_csv(r'lugares.csv', index = True)
#     print("--- %s seconds ---" % (time.time() - start_time))
#     return(lugares_unique, lon, lat, address)
311/70:
# Data_Book_Short=pd.DataFrame(list(zip(lugares_unique, lon, lat, address)), columns=["lugares","LON", "LAT", "address"])
# Data_Book_Short
311/71: Data_Book_2.to_csv("Data_Book_Geocode.csv")
313/12:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df.head(5)
313/13:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
313/14:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
311/72:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2.head(10)
311/73:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/74:
Data_Book_2=pd.merge(Data_Book, lugares_DF)
len(Data_Book_2)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/75:
Data_Book_2=pd.merge(Data_Book, lugares_DF, left_index=True, right_index=True)
len(Data_Book_2)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/76:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_index=True, right_index=True)
len(Data_Book_2)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/77:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book_2)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/78:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/79:
Data_Book_2=pd.merge(lugares_DF, Data_Book, right_on=True)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/80:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_index=True)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/81:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/82:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/83:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2.head(50)
311/84:
Data_Book_2=pd.merge(lugares_DF, Data_Book)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/85:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on="lugares")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/86:
Data_Book_2=pd.merge(lugares_DF, Data_Book, how="outer")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/87:
Data_Book_2=pd.merge(lugares_DF, Data_Book, how="outer", left_on=True, right_on=True)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/88:
Data_Book_2=pd.merge(lugares_DF, Data_Book, left_on=True, right_on=True)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/89:
Data_Book_2=pd.merge(lugares_DF, Data_Book, "inner")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/90:
Data_Book_2=pd.merge(lugares_DF, Data_Book, "outer")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/91:
Data_Book_2=pd.merge(lugares_DF, Data_Book, "right")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/92:
Data_Book_2=pd.merge(lugares_DF, Data_Book, "left")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/93:
Data_Book_2=pd.merge(lugares_DF, Data_Book, by = "licto", all = TRUE)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/94:
Data_Book_2=pd.merge(lugares_DF, Data_Book, by = "licto")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/95:
Data_Book_2=pd.merge(lugares_DF, Data_Book, by = "lugares")
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/96:
Data_Book_2=pd.merge(lugares_DF, Data_Book, by = "lugares", all = True, sort = False)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/97:
Data_Book_2=merge(lugares_DF, Data_Book, by = "lugares", all = True, sort = False)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/98:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", all = True, sort = False)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/99:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/100:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/101:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/102:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2.head(20)
#Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/103:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
df.drop(['Position'], axis=1)
lugares_DF
311/104:
lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
lugares_DF=lugares_DF.drop(['Position'], axis=1)
lugares_DF
311/105:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2.head(20)
Data_Book_2=Data_Book_2.sort_values("Position")
#Data_Book_2
311/106:
Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2.head(20)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
311/107: Data_Book_2.to_csv("Data_Book_Geocode.csv")
313/15:
df = pd.read_csv('Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
313/16:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
313/17:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
313/18:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
313/19:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
313/20:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
313/21:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
313/22:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=40, normalize=True)

df_2.head(5)
313/23:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
313/24: min_lat, max_lat, min_lon, max_lon
313/25: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
313/26: Data_clean
325/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
325/2:
df = pd.read_csv('Data/Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
325/3:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
325/4:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
325/5:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[6:7]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/6:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/7:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=40, normalize=True)

df_2.head(5)
325/8:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
325/9: min_lat, max_lat, min_lon, max_lon
325/10: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
325/11: Data_clean
325/12: import folium
321/1:
pd.set_option('display.max_rows', None)
<<<<àlugares3
321/2:
pd.set_option('display.max_rows', None)
#lugares3
321/3: #print(geocode_result[0])
325/13:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"][0], Data_clean["LON_google"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"][: ff],Data_clean["LON_google"][: ff], Data_clean["lugares"][0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
325/14: import folium
325/15:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"][0], Data_clean["LON_google"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"][: ff],Data_clean["LON_google"][: ff], Data_clean["lugares"][0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
325/16: Data_clean["LAT_google"][0]
325/17: Data_clean["LAT_google"]
325/18: Data_clean["LAT_google"][0]
325/19: Data_clean["LAT_google"][:]
325/20: Data_clean["LAT_google"][:0]
325/21: Data_clean["LAT_google"][0:]
325/22: Data_clean["LAT_google"][0,0]
325/23: Data_clean["LAT_google"][0:1]
325/24: Data_clean["LAT_google"][0:0]
325/25: Data_clean["LAT_google"][0:1]
325/26: Data_clean["LAT_google"][0:2]
325/27: Data_clean["LAT_google"][1:2]
325/28: Data_clean[0][1:2]
325/29: Data_clean[0][2]
325/30: Data_clean[0]
325/31: Data_clean[0:]
325/32: Data_clean[0:1]
325/33: Data_clean[0:1][0]
325/34: Data_clean[0:1][0:0]
325/35: Data_clean[0:1][1:1]
325/36: Data_clean[0:1][0:1]
325/37: Data_clean[0:1]
325/38: Data_clean["LAT_google"][0]
325/39: Data_clean["LAT_google"](0)
325/40: Data_clean["LAT_google"][0:1]
325/41: Data_clean["LAT_google"][0:2]
325/42: Data_clean["LAT_google"][:2]
325/43: Data_clean["LAT_google"][2]
325/44: Data_clean["LAT_google"].get_value(0)
321/4: #lugares3
321/5: #lugares2
321/6:
#pd.set_option('display.max_rows', None)
#lugares3
325/45: Data_clean["LAT_google"]
325/46: Data_clean["LAT_google"][:0]
325/47: Data_clean["LAT_google"][:1]
325/48: jj=Data_clean["LAT_google"][:1]
325/49:
jj=Data_clean["LAT_google"][:1]
jj
325/50:
jj=Data_clean["LAT_google"][:1]
jj[0]
325/51:
jj=Data_clean["LAT_google"][:1]
jj
325/52:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[min_lon, min_lat, zoom_start=3)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/53:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[min_lon, min_lat, zoom_start=4)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/54:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[[min_lon, min_lat], zoom_start=4)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/55:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[0,40, zoom_start=4)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/56:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/57:
colors = {'A' : 'red', 'B' : 'blue'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=2)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/58:
colors = {'GPE' : 'red', 'LOC' : 'blue', 'FAV' : 'green'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=2)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/59:
colors = {'GPE' : 'red', 'LOC' : 'blue', 'FAC' : 'green'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=2)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/60:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=40, normalize=True)

df_2.head(5), df_3.head(5)
325/61:
df_2=df["LON_google"].value_counts(bins=30, normalize=True)
df_3=df["LAT_google"].value_counts(bins=30, normalize=True)

df_2.head(5), df_3.head(5)
325/62:
df_2=df["LON_google"].value_counts(bins=25, normalize=True)
df_3=df["LAT_google"].value_counts(bins=25, normalize=True)

df_2.head(5), df_3.head(5)
325/63:
df_2=df["LON_google"].value_counts(bins=20, normalize=True)
df_3=df["LAT_google"].value_counts(bins=20, normalize=True)

df_2.head(5), df_3.head(5)
325/64:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
325/65: min_lat, max_lat, min_lon, max_lon
325/66: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
325/67: Data_clean
325/68: Data_clean.to_csv("Data_clean.csv")
325/69: import folium
325/70:
colors = {'GPE' : 'red', 'LOC' : 'blue', 'FAC' : 'green'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=2)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/71:
df_2=df["LON_google"].value_counts(bins=30, normalize=True)
df_3=df["LAT_google"].value_counts(bins=20, normalize=True)

df_2.head(5), df_3.head(5)
325/72:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=20, normalize=True)

df_2.head(5), df_3.head(5)
325/73:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=5, normalize=True)

df_2.head(5), df_3.head(5)
325/74:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
325/75: min_lat, max_lat, min_lon, max_lon
325/76: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
325/77: Data_clean
325/78: Data_clean.to_csv("Data_clean.csv")
325/79: import folium
325/80:
colors = {'GPE' : 'red', 'LOC' : 'blue', 'FAC' : 'green'}

map_osm = folium.Map(location=[min_lon, min_lat], zoom_start=2)

Data_clean.apply(lambda row:folium.CircleMarker(location=[row["LAT_google"], row["LON_google"]], 
                                              radius=10, fill_color=colors[row['labels']], popup=row['lugares'])
                                             .add_to(map_osm), axis=1)

map_osm
325/81:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"][:1], Data_clean["LON_google"][:1]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"][: ff],Data_clean["LON_google"][: ff], Data_clean["lugares"][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
325/82: Area = folium.Map(location=[Data_clean["LAT_google"][:1], Data_clean["LON_google"][:1]], zoom_start=2)
325/83: Area
325/84:
jj=Data_clean["LAT_google"][:1]
jj
325/85:
jj=Data_clean["LAT_google"].average()
jj
325/86:
jj=Data_clean["LAT_google"].mean()
jj
325/87: Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=2)
325/88: Area
325/89: Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=8)
325/90: Area
325/91: Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
325/92: Area
325/93: jj=Data_clean["LAT_google"].iloc[0]
325/94:
jj=Data_clean["LAT_google"].iloc[0]
jj
325/95:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"].iloc[: ff],Data_clean["LON_google"].iloc[: ff], Data_clean["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
325/96:
df_2=df["LON_google"].value_counts(bins=10, normalize=True)
df_3=df["LAT_google"].value_counts(bins=10, normalize=True)

df_2.head(5), df_3.head(5)
325/97:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
325/98: min_lat, max_lat, min_lon, max_lon
325/99: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
325/100: Data_clean
325/101: Data_clean.to_csv("Data_clean.csv")
325/102: import folium
325/103:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"].iloc[: ff],Data_clean["LON_google"].iloc[: ff], Data_clean["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
325/104:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = Data_clean[['LAT_google']]
X_axis = Data_clean[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
325/105: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/106: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, cmap='viridis')
325/107:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=cluster_labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/108:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/109:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/110:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/111:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:3]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/112:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/113:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[3:4]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/114:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[4:5]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/115: df.columns[6:7]
325/116:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[6:7]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[6:7]]) # Labels of each point
df.head(5)
325/117: df.columns[2:3]
325/118: df.columns[2:4]
325/119:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:3]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[3:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:3]]) # Labels of each point
df.head(5)
325/120:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:3]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:3]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:3]]) # Labels of each point
df.head(5)
325/121:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:3]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:3]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:3]]) # Labels of each point
df.head(5)
325/122:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
325/123:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:3]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:3]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:3]]) # Labels of each point
Data_clean.head(5)
325/124: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/125:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:3]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:3]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:3]]) # Labels of each point
Data_clean.head(5)
325/126: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/127:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
325/128: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/129:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
325/130: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/131:
kmeans = KMeans(n_clusters = 2, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
325/132: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/133:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
325/134: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
325/135:
df_2=Data_clean["LON_google"].value_counts(bins=10, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=10, normalize=True)

df_2.head(5), df_3.head(5)
325/136:
df_2=Data_clean["LON_google"].value_counts(bins=3, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=3, normalize=True)

df_2.head(5), df_3.head(5)
325/137:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
325/138: df.columns[2:4]
325/139:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
329/1:
import Tkinter
import tkFileDialog
import os

root = Tkinter.Tk()
root.withdraw() #use to hide tkinter window

currdir = os.getcwd()
tempdir = tkFileDialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
if len(tempdir) > 0:
    print "You chose %s" % tempdir
329/2:
import Tkinter
import tkFileDialog
import os

root = Tkinter.Tk()
root.withdraw() #use to hide tkinter window

currdir = os.getcwd()
tempdir = tkFileDialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
if len(tempdir) > 0:
    print("You chose %s" % tempdir)
329/3:
import tkinter
import tkFileDialog
import os

root = Tkinter.Tk()
root.withdraw() #use to hide tkinter window

currdir = os.getcwd()
tempdir = tkFileDialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
if len(tempdir) > 0:
    print("You chose %s" % tempdir)
329/4:
import tkinter 
from tkinter import messagebox
from tkinter import filedialog

main_win = tkinter.Tk()
main_win.geometry("1000x500")
main_win.sourceFolder = ''
main_win.sourceFile = ''
def chooseDir():
    main_win.sourceFolder =  filedialog.askdirectory(parent=main_win, initialdir= "/", title='Please select a directory')

b_chooseDir = tkinter.Button(main_win, text = "Chose Folder", width = 20, height = 3, command = chooseDir)
b_chooseDir.place(x = 50,y = 50)
b_chooseDir.width = 100


def chooseFile():
    main_win.sourceFile = filedialog.askopenfilename(parent=main_win, initialdir= "/", title='Please select a directory')

b_chooseFile = tkinter.Button(main_win, text = "Chose File", width = 20, height = 3, command = chooseFile)
b_chooseFile.place(x = 250,y = 50)
b_chooseFile.width = 100

main_win.mainloop()
print(main_win.sourceFolder)
print(main_win.sourceFile )
329/5:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/6:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/7:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/8:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/9:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
#root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/10:
import tkinter
from tkinter import filedialog
import os

root = tkinter.Tk()
root.withdraw() #use to hide tkinter window

def search_for_file_path ():
    currdir = os.getcwd()
    tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    return tempdir


file_path_variable = search_for_file_path()
print ("\nfile_path_variable = ", file_path_variable)
329/11:
df = pd.read_csv('Data/Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
329/12:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
329/13:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
329/14:
df = pd.read_csv('Data/Data_Book_Geocode.csv')
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
329/15:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
329/16:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
329/17:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
329/18: df.columns[2:4]
329/19:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
329/20:
df_2=df["LON_google"].value_counts(bins=10, normalize=True)
df_3=df["LAT_google"].value_counts(bins=10, normalize=True)

df_2.head(5), df_3.head(5)
329/21:
#Selection of first row only
min_lon=df_2.index[0].left
max_lon=df_2.index[0].right

min_lat=df_3.index[0].left
max_lat=df_3.index[0].right
329/22: min_lat, max_lat, min_lon, max_lon
329/23: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
329/24: Data_clean
329/25: import folium
329/26:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"].iloc[: ff],Data_clean["LON_google"].iloc[: ff], Data_clean["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
329/27:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = Data_clean[['LAT_google']]
X_axis = Data_clean[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
329/28:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
329/29: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/30:
df_2=Data_clean["LON_google"].value_counts(bins=3, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=3, normalize=True)

df_2.head(5), df_3.head(5)
329/31:
kmeans = KMeans(n_clusters = 2, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
329/32: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/33:
df_2=Data_clean["LON_google"].value_counts(bins=2, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=2, normalize=True)

df_2.head(5), df_3.head(5)
329/34:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"].iloc[: ff],Data_clean["LON_google"].iloc[: ff], Data_clean["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
327/1:
# instantiate a feature group for the incidents in the dataframe
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = folium.map.FeatureGroup()
ff=len(lugares3)
i=0
# loop through the lugares points
for lat, lng, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff]):
    if type(lugares3["LAT"][i])!=type(None):
        Dots.add_child(folium.features.CircleMarker([lat, lng], radius=5, color='red', fill=True, fill_color='blue', fill_opacity=0.6))
    i=i+1

# add pop-up text to each marker on the map
latitudes = list(lugares3["LAT"][: ff])
longitudes = list(lugares3["LON"][: ff])
labels = list(lugares3[0][: ff])

for lat, lng, label in zip(latitudes, longitudes, labels):
    if type(lat)!=type(None):
        folium.Marker([lat, lng], popup=label).add_to(Area) 
    
    
#Area.add_child(Dots)
329/35: oo=Data_clean.groupby(by="cluster_label", axis=0)
329/36:
oo=Data_clean.groupby(by="cluster_label", axis=0)
oo
329/37:
oo=Data_clean.groupby(['cluster_label']).size()
oo
329/38:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
329/39: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/40:
df_2=Data_clean["LON_google"].value_counts(bins=2, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=2, normalize=True)

df_2.head(5), df_3.head(5)
329/41:
oo=Data_clean.groupby(['cluster_label']).size()
oo
329/42:
kmeans = KMeans(n_clusters = 2, init ='k-means++')
kmeans.fit(Data_clean[Data_clean.columns[2:4]]) # Compute k-means clustering.
Data_clean['cluster_label'] = kmeans.fit_predict(Data_clean[Data_clean.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean[Data_clean.columns[2:4]]) # Labels of each point
Data_clean.head(5)
329/43: Data_clean.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/44:
df_2=Data_clean["LON_google"].value_counts(bins=2, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=2, normalize=True)

df_2.head(5), df_3.head(5)
329/45:
oo=Data_clean.groupby(['cluster_label']).size()
oo
329/46:
df_2=Data_clean["LON_google"].value_counts(bins=2, normalize=False)
df_3=Data_clean["LAT_google"].value_counts(bins=2, normalize=False)

df_2.head(5), df_3.head(5)
329/47:
df_2=Data_clean["LON_google"].value_counts(bins=2, normalize=True)
df_3=Data_clean["LAT_google"].value_counts(bins=2, normalize=True)

df_2.head(5), df_3.head(5)
329/48:
oo=Data_clean.groupby(['cluster_label']).size().max
oo
329/49:
oo=Data_clean.groupby(['cluster_label']).size().max()
oo
329/50:
oo=Data_clean.groupby(['cluster_label']).max()
oo
329/51:
oo=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts')
oo
329/52:
oo=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
oo
329/53:
ll,oo=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
oo
329/54:
ll,oo=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
ll,oo
329/55: label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
329/56:
df_2=df["LON_google"].value_counts(bins=40, normalize=True)
df_3=df["LAT_google"].value_counts(bins=40, normalize=True)

df_2.head(5), df_3.head(5)
329/57:
df_2=df["LON_google"].value_counts(bins=10, normalize=True)
df_3=df["LAT_google"].value_counts(bins=10, normalize=True)

df_2.head(5), df_3.head(5)
329/58:
label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean.loc[(Data_clean['cluster_label'] =label_cl]
329/59:
label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean.loc[(Data_clean['cluster_label'] ==label_cl]
329/60:
label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean.loc[(Data_clean['cluster_label'] ==label_cl)]
329/61:
label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean.loc[(Data_clean['cluster_label'] ==label_cl)]
Data_clean_2
329/62:
label_cl,counts_cl=Data_clean.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean.loc[(Data_clean['cluster_label'] !=label_cl)]
Data_clean_2
329/63:
ff=len(Data_clean_2)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean_2["LAT_google"].mean(), Data_clean_2["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean_2["LAT_google"].iloc[: ff],Data_clean_2["LON_google"].iloc[: ff], Data_clean_2["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
329/64:
kmeans = KMeans(n_clusters = 2, init ='k-means++')
kmeans.fit(Data_clean_2[Data_clean_2.columns[2:4]]) # Compute k-means clustering.
Data_clean_2['cluster_label'] = kmeans.fit_predict(Data_clean_2[Data_clean_2.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean_2[Data_clean_2.columns[2:4]]) # Labels of each point
print(Data_clean_2.head(5))
Data_clean_2.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/65:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(Data_clean_2[Data_clean_2.columns[2:4]]) # Compute k-means clustering.
Data_clean_2['cluster_label'] = kmeans.fit_predict(Data_clean_2[Data_clean_2.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean_2[Data_clean_2.columns[2:4]]) # Labels of each point
print(Data_clean_2.head(5))
Data_clean_2.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/66:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').min()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
Data_clean_2
329/67:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').min()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] ==label_cl)]
Data_clean_2
329/68:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(Data_clean_2[Data_clean_2.columns[2:4]]) # Compute k-means clustering.
Data_clean_2['cluster_label'] = kmeans.fit_predict(Data_clean_2[Data_clean_2.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(Data_clean_2[Data_clean_2.columns[2:4]]) # Labels of each point
print(Data_clean_2.head(5))
Data_clean_2.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
329/69:
ff=len(Data_clean_2)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean_2["LAT_google"].mean(), Data_clean_2["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean_2["LAT_google"].iloc[: ff],Data_clean_2["LON_google"].iloc[: ff], Data_clean_2["Quotes"].iloc[: ff], Data_clean_2["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
329/70:
ff=len(Data_clean_2)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean_2["LAT_google"].mean(), Data_clean_2["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean_2["LAT_google"].iloc[: ff],Data_clean_2["LON_google"].iloc[: ff], Data_clean_2["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
329/71:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').min()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
Data_clean_2
329/72:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').min()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
329/73:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
329/74:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts')
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
329/75:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts')
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
329/76:
label_cl,counts_cl=Data_clean_2.groupby(['cluster_label']).size().reset_index(name='counts').max()
Data_clean_2=Data_clean_2.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
329/77: Data_clean=df.loc[(df["LON_google"] > min_lon) & (df["LON_google"] < max_lon) & (df["LAT_google"] > min_lat) & (df["LAT_google"] < max_lat)]
329/78: Data_clean
329/79:
from folium import plugins
ff=len(Data_clean)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[Data_clean["LAT_google"].mean(), Data_clean["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(Data_clean["LAT_google"].iloc[: ff],Data_clean["LON_google"].iloc[: ff], Data_clean["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
333/1: import ntpath
333/2: name=ntpath.basename("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub")
333/3: name
333/4:
import os
base=os.path.basename("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub")
base

os.path.splitext(base)

os.path.splitext(base)[0]
333/5:
import os
base=os.path.basename("C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub")
base
os.path.splitext(base)[0]
340/1:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
340/2:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
340/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
340/4: out=epub2text(Book)
340/5: #out[5]
340/6:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
340/7:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
340/8:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
340/9: len(lugares)
340/10: start_ent[0:10]
340/11: #Book[0:250]
340/12:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            #print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
340/13:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
340/14:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
    
    Frases_Book.append(Frases_clean)
    z+=1
340/15:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=1
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
340/16:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
340/17: Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
340/18:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
340/19:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
Data_Book
344/1:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
344/2: out=epub2text(Book)
344/3:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
344/4:
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import spacy
import spacy.displacy as displacy #Text Visualization
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from geopy.geocoders import Nominatim
344/5:
def epub2thtml(epub_path):
    book = epub.read_epub(epub_path)
    chapters = []
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            chapters.append(item.get_content())
    return chapters

blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script', '\n'   ]
# there may be more elements you don't want, such as "style", etc.
def chap2text(chap):
    output = ''
    soup = BeautifulSoup(chap, 'html.parser')
    text = soup.find_all(text=True)
    for t in text:
        if t.parent.name not in blacklist:
            output += '{} '.format(t)
    return output

def thtml2ttext(thtml):
    Output = []
    for html in thtml:
        text =  chap2text(html)
        Output.append(text)
    return Output

def epub2text(epub_path):
    chapters = epub2thtml(epub_path)
    ttext = thtml2ttext(chapters)
    return ttext
344/6:
Book="C:\\Users\\aleja\\OneDrive\\Documents\\MBA\\99. Project Work\\Libros\\Brown, Dan - Angels & Demons.epub"
nlp=spacy.load("en_core_web_lg")
344/7: out=epub2text(Book)
344/8: #out[5]
344/9:
number_words=[]
ff=len(out)
print(ff)
z=0
for i in (out):
    number_words.append(len(out[z]))
    z=z+1
number_words
344/10:
def listToString(s):
    # initialize an empty string 
    str1 = " " 
    
    # return string   
    return (str1.join(s)) 
    # Driver code     
Book=(listToString(out))  

Book2=nlp(Book)

lugares=[]
344/11:
label=[]
lugares=[]
start_ent=[]
end_ent=[]
for ent in Book2.ents:
    #print(ent.text, ent.label_)
    #if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC" or  ent.label_=="PERSON":
    if ent.label_=="GPE" or  ent.label_=="LOC" or  ent.label_=="FAC":
        lugares.append(ent.text)
        label.append(ent.label_)
        start_ent.append(ent.start_char)
        end_ent.append(ent.end_char)

#spacy.displacy.render(Book2, style="ent", page="true")
#displacy.serve(Book2, style="ent")

#np.save('lugares_Diarios', lugares)
344/12: len(lugares)
344/13: start_ent[0:10]
344/14: #Book[0:250]
344/15:
len(start_ent)
start_frase=[]
end_frase=[]

for i in start_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            #print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
344/16:
len(start_ent)
start_frase=[]
end_frase=[]

for i in end_ent[::]:
    z=i
    
    while z<len(Book):
        if Book[z]==".":
            #print("finish aqui", z, i)
            end_frase.append(z)
            break
        else:
            z+=1
    
for x in start_ent[::]:
    f=x
    while f>=0:
        if Book[f]==".":
            
            start_frase.append(f)
            #print("start aqui", f,x)
            break
        else:
            f-=1
            if f<0:
                Null=None
                start_frase.append(Null)
            #print(f)
344/17:
start_ent[0:10], end_ent[0:10], start_frase[0:10], end_frase[0:10]
len(start_frase), len(end_frase)
344/18:
import re
Frases_Book=[]
z=0
for i in start_frase:
    Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
    
    Frases_Book.append(Frases_clean)
    z+=1
344/19:
# print(Book[3936:4101])
# (Book[start_frase[0]: end_frase[0]])
i=0
lugares[i],Frases_Book[i], start_ent[i], end_ent[i], start_frase[i], end_frase[i]
344/20: Data_Book[50:55]
344/21:
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels", "Quotes", "Position"])
Data_Book
344/22: Data_Book[50:55]
344/23:
import numpy as np
import pandas as pd
import folium
import googlemaps


def coordinates(lugares):
    #lugares_load=np.load(lugares)
    #lugares_Book=pandas.read_csv(Data_Book)
    pos, lugares_load= np.unique(lugares, return_index=True)
    g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
    lugares_DF=pd.DataFrame({"Position": pos, "lugares":lugares_load})
    lugares_DF["LAT_google"]= None
    lugares_DF["LON_google"]= None
    print ("Launching Google API...")
    for i in range(0, len(lugares_DF), 1):
        geocode_result=g_key.geocode(lugares_DF.iat[i,0])
        try:
            print ("coordinates: ", i, " of ", len(lugares_DF), "extracted")
            lat=geocode_result[0]["geometry"]["location"]["lat"]
            lon=geocode_result[0]["geometry"]["location"]["lng"]
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LAT_google")]=lat
            lugares_DF.iat[i,lugares_DF.columns.get_loc("LON_google")]=lon
        except:
            lat=None
            lon=None
    print ("coordinates: ", i, " extracted")
    #lugares_DF.to_csv(r'lugares.csv', index = True)
    return(lugares_DF)
344/24:
lugares_DF=coordinates(lugares)
# Data_Book_all=[Data_Book, lugares_DF]
344/25:
def Data_coord(lugares_DF, Data_Book, Book_name):  
    Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
    Data_Book_2=Data_Book_2.sort_values("Position")
    Data_Book_2.to_csv("Data/Geocode_" + Book_name + ".csv")
    print("saved Data_Book_Geocode.csv")
    return(Data_Book_2)


Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
len(Data_Book)
Data_Book_2.head(20)
Data_Book_2=Data_Book_2.sort_values("Position")
Data_Book_2
344/26: Data_coord(lugares_DF, Data_Book, "sss")
344/27:
def Data_coord(lugares_DF, Data_Book, Book_name):  
    lugares_DF=lugares_DF.rename(columns={"Position": "lugares", "lugares": "Position"})
    lugares_DF=lugares_DF.drop(['Position'], axis=1)
    Data_Book_2=pd.merge(lugares_DF, Data_Book, on = "lugares", sort = False)
    Data_Book_2=Data_Book_2.sort_values("Position")
    Data_Book_2.to_csv("Data/Geocode_" + Book_name + ".csv")
    print("saved Data_Book_Geocode.csv")
    return(Data_Book_2)
344/28: Data_coord(lugares_DF, Data_Book, "sss")
344/29:
def quote_book(lugares, label, start_ent, end_ent, Book):
    len(start_ent)
    start_frase=[]
    end_frase=[]
    for i in end_ent:
        z=i
        while z<len(Book):
            if Book[z]==".":
            #print(Book[z],z)
                end_frase.append(z)
                break
            else:
                z+=1
        f=i
        while f>0:
            if Book[f]==".":
            #print(Book[f],f)
                start_frase.append(f)
                break
            else:
                f-=1
                if f<0:
                    Null=None
                    start_frase.append(Null)
    
    Frases_Book=[]
    z=0
    for i in start_frase:
        Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
        Frases_Book.append(Frases_clean)
        z+=1
    
    Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
    Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels","Quotes", "Position"])
    #Data_Book=pd.DataFrame(list(zip(Data_Book.index)), columns=["Position"])
    Data_Book.to_csv("Data/Data_Book.csv")
    print("Quotes extracted")
    return Frases_Book, Data_Book
344/30: Frases, Data_Book=quote_book(lugares, label, start_ent, end_ent, Book)
344/31: Frases, Data_Book=quote_book(lugares, label, start_ent, end_ent, Book)
344/32: Data_Book
344/33: Frases
344/34: #Frases
344/35:
def quote_book(lugares, label, start_ent, end_ent, Book):
    #len(start_ent)
    start_frase=[]
    end_frase=[]
    for i in end_ent[::]:
        z=i
    
        while z<len(Book):
            if Book[z]==".":
            #print("finish aqui", z, i)
                end_frase.append(z)
                break
            else:
                z+=1
    
    for x in start_ent[::]:
        f=x
        while f>=0:
            if Book[f]==".":
            
                start_frase.append(f)
            #print("start aqui", f,x)
                break
            else:
                f-=1
                if f<0:
                    Null=None
                    start_frase.append(Null)
    Frases_Book=[]
    z=0
    for i in start_frase:
        Frases_clean=re.sub('\n |  |\. ', '', (Book[start_frase[z]:end_frase[z]]))
        Frases_Book.append(Frases_clean)
        z+=1
    
    Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book)), columns=["lugares","labels", "Quotes"])
    Data_Book=pd.DataFrame(list(zip(lugares, label, Frases_Book, Data_Book.index)), columns=["lugares","labels","Quotes", "Position"])
    #Data_Book=pd.DataFrame(list(zip(Data_Book.index)), columns=["Position"])
    Data_Book.to_csv("Data/Data_Book.csv")
    print("Quotes extracted")
    return Frases_Book, Data_Book
344/36: Frases, Data_Book=quote_book(lugares, label, start_ent, end_ent, Book)
344/37: Data_Book
349/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
349/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
349/3:
def search_for_file_path ():
    root = tkinter.Tk()
    root.withdraw() #use to hide tkinter window
    currdir = os.getcwd()
    tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
    if len(tempdir) > 0:
        print ("You chose: %s" % tempdir)
    base=os.path.basename(tempdir)
    base=os.path.splitext(base)[0]

    return tempdir, base
349/4: path_file, base=search_for_file_path ()
349/5: from Book_extraction_single import search_for_file_path
349/6: path_file, base=search_for_file_path ()
349/7:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
349/8:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/9:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').max()
df=df.loc[(Data_clean_2['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/10:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').max()
df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/11:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/12:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
349/13:
K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df[['LAT_google']]
X_axis = df[['LON_google']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
349/14:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/15:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/16:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts')
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/17:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').values()
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/18:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').value
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/19:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts')
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl,counts_cl
349/20:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts')
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/21:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').value_counts
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/22:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').value_counts()
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/23:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').counts
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/24:
label_cl,counts_cl=df.groupby(['cluster_label']).size().reset_index(name='counts').count
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/25:
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift
from matplotlib import pyplot
# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)
# define the model
model = MeanShift()
# fit model and predict clusters
yhat = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()
349/26:
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift
from matplotlib import pyplot
from numpy import unique
from numpy import where
# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)
# define the model
model = MeanShift()
# fit model and predict clusters
yhat = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()
349/27: X
349/28: type(X)
349/29: X.shape
349/30:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/31:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/32:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
df.plot.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/33:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
df.plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/34:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/35:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/36:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/37:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/38:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
349/39:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], 'x', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
349/40:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], 'x', markerfacecolor='black', markeredgecolor='k', markersize=14)
349/41:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5)
349/42:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0],'x', c='black', s=200, alpha=0.5)
349/43:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0],'x', c='black',  alpha=0.5)
349/44:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5)
349/45:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
349/46:
label_cl,counts_cl=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/47:
label_cl,counts_cl=df.groupby(['cluster_label']).size
#df=df.loc[(df['cluster_label'] !=label_cl)]
label_cl
349/48:
counts=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
counts
349/49:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(10)
349/50:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/51:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = kmeans.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(50)
349/52:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(50)
349/53:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/54:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
349/55:
counts=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
counts
349/56:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
349/57:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF
349/58:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values
349/59:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values
clusters_DF
349/60:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF[0].sort_values
clusters_DF
349/61:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values('0')
clusters_DF
349/62:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0)
clusters_DF
349/63:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, 'descending')
clusters_DF
349/64:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF
349/65:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF[0]
349/66:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF[0][0]
349/67:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(cluster_DF[0].sum)
349/68:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(cluster_DF[0].sum())
349/69:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
349/70:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF
349/71:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF
min_value=0.55
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF['cluster_label'][i])
    else:
        cluster_keep.append(clusters_DF['cluster_label'][i+1])
349/72:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF
min_value=0.55
cluster_keep=[]
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF['cluster_label'][i])
    else:
        cluster_keep.append(clusters_DF['cluster_label'][i+1])
349/73:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF)
min_value=0.55
cluster_keep=[]
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF['cluster_label'][i])
    else:
        cluster_keep.append(clusters_DF['cluster_label'][i+1])
349/74:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF)
min_value=0.55
cluster_keep=[]
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[i])
    else:
        cluster_keep.append(clusters_DF.index[i+1])
349/75:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index)
min_value=0.55
cluster_keep=[]
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[i])
    else:
        cluster_keep.append(clusters_DF.index[i+1])
349/76:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.55
cluster_keep=[]
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[i])
    else:
        cluster_keep.append(clusters_DF.index[i+1])
349/77:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.55
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[i])
    else:
        cluster_keep.append(clusters_DF.index[i+1])
    z+=1
349/78:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.55
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[z])
        break
    else:
        cluster_keep.append(clusters_DF.index[z+1])
    z+=1
349/79:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.55
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    if i>min_value:
        cluster_keep.append(clusters_DF.index[z])
        break
    else:
        cluster_keep.append(clusters_DF.index[z+1])
    z+=1
cluster_keep
349/80:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.55
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][z]
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[z])
        break
    else:
        z+=1
cluster_keep
349/81:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][z]
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[z])
        break
    else:
        z+=1
cluster_keep
349/82:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][z]
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/83:
sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][2]
sum_val
349/84:
sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][1]
sum_val
349/85:
sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][0]
sum_val
349/86:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/87:
sum_val=clusters_DF['normalized'][0]+clusters_DF['normalized'][0]
sum_val
349/88:
sum_val=clusters_DF['normalized'][0:1]
sum_val
349/89:
sum_val=clusters_DF['normalized'][0:1].sum()
sum_val
349/90:
sum_val=clusters_DF['normalized'][0:0].sum()
sum_val
349/91:
sum_val=clusters_DF['normalized'][0:1].sum()
sum_val
349/92:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/93:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.89
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/94:
sum_val=clusters_DF['normalized'][0:3].sum()
sum_val
349/95:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.99
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/96:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/97:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/98:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep.size()
349/99:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep.size
349/100:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
type(cluster_keep)
349/101:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
len(cluster_keep)
349/102:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
print(cluster_keep)
349/103:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep[0]
349/104:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep[0][0]
349/105:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep[0][1]
349/106:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep[0][3]
349/107:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep
349/108:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep)
349/109:
# K_clusters = range(1,10)
# kmeans = [KMeans(n_clusters=i) for i in K_clusters]
# Y_axis = df[['LAT_google']]
# X_axis = df[['LON_google']]
# score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# # Visualize
# plt.plot(K_clusters, score)
# plt.xlabel('Number of Clusters')
# plt.ylabel('Score')
# plt.title('Elbow Curve')
# plt.show()
349/110: path_file, base=search_for_file_path ()
349/111:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
349/112:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
349/113:
# K_clusters = range(1,10)
# kmeans = [KMeans(n_clusters=i) for i in K_clusters]
# Y_axis = df[['LAT_google']]
# X_axis = df[['LON_google']]
# score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# # Visualize
# plt.plot(K_clusters, score)
# plt.xlabel('Number of Clusters')
# plt.ylabel('Score')
# plt.title('Elbow Curve')
# plt.show()
349/114:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
349/115:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
349/116:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep)
354/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
354/2:
# def search_for_file_path ():
#     root = tkinter.Tk()
#     root.withdraw() #use to hide tkinter window
#     currdir = os.getcwd()
#     tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
#     if len(tempdir) > 0:
#         print ("You chose: %s" % tempdir)
#     base=os.path.basename(tempdir)
#     base=os.path.splitext(base)[0]

#     return tempdir, base
354/3: from Book_extraction_single import search_for_file_path
354/4: path_file, base=search_for_file_path ()
354/5:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/6:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
354/7:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
354/8:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/9:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster impoMeanShiftans
import seaborn as sns; sns.set()
import csv
354/10:
import pandas as pd
import numpy as npimport matplotlib.pyplot as plt
from sklearn.cluster import MeanShiftrt KMeans
import seaborn as sns; sns.set()
import csv
354/11:
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
354/12:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
354/13:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/14:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep)
354/15: clusters_DF
354/16: Data_cluster_1=pd.merge(df, cluster_keep)
354/17: Data_cluster_1=pd.merge(df, pd.DataFrame(cluster_keep))
354/18: pd.DataFrame(cluster_keep)
354/19:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep, axis=1)
354/20:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep, axis=0)
354/21:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep, len(cluster_keep)
354/22: len(cluster_keep)
354/23: cluster_keep.size
354/24: print(cluster_keep)
354/25:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
len(cluster_keep)
354/26:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
cluster_keep.size
354/27: Data_cluster_1=pd.merge(df, clusters_DF(0:range_1-1))
354/28: Data_cluster_1=pd.merge(df, clusters_DF[0:range_1-1])
354/29:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/30: Data_cluster_1=pd.merge(df, clusters_DF[0:range_1-1])
354/31: clusters_DF[0:range_1-1]
354/32: clusters_DF[0:range_1]
354/33: Data_cluster_1=pd.merge(df, clusters_DF[0:range_1].index)
354/34: Data_cluster_1=pd.merge(df, clusters_DF[0:range_1].index())
354/35: Data_cluster_1=pd.merge(df, clusters_DF.index()[0:range_1])
354/36: clusters_DF[0:range_1].index()
354/37: clusters_DF.index()
354/38: clusters_DF
354/39: clusters_DF[0]
354/40: clusters_DF[0][0]
354/41: clusters_DF[0]
354/42:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["CLUSTER"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/43: clusters_DF[0]
354/44: clusters_DF["CLUSTER"]
354/45: clusters_DF
354/46:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head(5)
354/47:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/48: clusters_DF
354/49:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/50: clusters_DF
354/51: Data_cluster_1=pd.merge(df, clusters_DF)
354/52: clusters_DF
354/53: Data_cluster_1=pd.merge(df, clusters_DF, on = "cluster_label", sort = False)
354/54:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
# cluster_keep=pd.DataFrame(cluster_keep)
# range_1=cluster_keep.size
354/55: clusters_DF
354/56: Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
354/57:
Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
Data_cluster_1
354/58:
Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
Data_cluster_1.head(50)
354/59:
Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
Data_cluster_1.head(5)
354/60:
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
Data_cluster_1.head(5)
354/61:
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/62:
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/63:
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
Data_cluster_1.head(50)
354/64:
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
#Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/65:
clf() 
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
#Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/66:
plt.clf() 
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
#Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/67:
Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
Data_cluster_1
354/68:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.head
354/69:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/70:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
# cluster_keep=pd.DataFrame(cluster_keep)
# range_1=cluster_keep.size
354/71: clusters_DF
354/72:
Data_cluster_1 = df[~df['cluster_label'].isin(cluster_keep)]
Data_cluster_1
354/73:
Data_cluster_1 = df[df['cluster_label'].isin(cluster_keep)]
Data_cluster_1
354/74:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/75: clusters_DF
354/76:
Data_cluster_1 = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
Data_cluster_1
354/77:
plt.clf() 
centers=[]
labels=[]
model = MeanShift()
model.fit(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Compute k-means clustering.
Data_cluster_1['cluster_label'] = model.fit_predict(Data_cluster_1[Data_cluster_1.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(Data_cluster_1[Data_cluster_1.columns[2:4]]) # Labels of each point
#Data_cluster_1.head(5)
Data_cluster_1.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/78: Data_cluster_1.head(5)
354/79: Data_cluster_1.head
354/80: Data_cluster_1
354/81:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/82:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/83:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/84: clusters_DF
354/85:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/86: clusters_DF
354/87: cluster_keep
354/88: cluster_keep, range_1
354/89:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/90:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/91:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/92: cluster_keep, range_1
354/93: clusters_DF, cluster_keep, range_1
354/94:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/95:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/96: clusters_DF, cluster_keep, range_1
354/97:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/98:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/99: clusters_DF, cluster_keep, range_1
354/100:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/101: clusters_DF, cluster_keep, range_1
354/102:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/103:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/104:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/105: clusters_DF, cluster_keep, range_1
354/106:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/107:
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/108:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
354/109: import folium
354/110:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
354/111:
# K_clusters = range(1,10)
# kmeans = [KMeans(n_clusters=i) for i in K_clusters]
# Y_axis = df[['LAT_google']]
# X_axis = df[['LON_google']]
# score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# # Visualize
# plt.plot(K_clusters, score)
# plt.xlabel('Number of Clusters')
# plt.ylabel('Score')
# plt.title('Elbow Curve')
# plt.show()
354/112:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/113:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/114:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/115:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/116: clusters_DF
354/117:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/118:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/119:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/120:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/121: clusters_DF
354/122:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/123:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/124:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/125:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/126: clusters_DF
354/127:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/128: clusters_DF
354/129:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/130:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/131:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/132:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/133: clusters_DF
354/134:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/135: clusters_DF
354/136:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/137:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/138:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/139:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/140: clusters_DF
354/141:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap.html')
354/142: path_file, base=search_for_file_path ()
354/143:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/144:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
354/145:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/146:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/147:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/148: clusters_DF
354/149:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/150:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/151:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/152:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/153: clusters_DF
354/154:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/155:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/156:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/157:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/158: clusters_DF
354/159:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/160:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/161:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/162:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/163: clusters_DF
354/164:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_1.html')
354/165: path_file, base=search_for_file_path ()
354/166:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/167:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
354/168:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/169:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/170:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/171: clusters_DF
354/172:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/173:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/174:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/175:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/176: clusters_DF
354/177:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/178:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/179:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/180:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/181: clusters_DF
354/182:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/183:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/184:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/185:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.8
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/186: clusters_DF
354/187:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/188:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/189:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/190:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.9
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/191: clusters_DF
354/192:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_2.html')
354/193: path_file, base=search_for_file_path ()
354/194:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/195:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
354/196:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/197:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/198:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/199: clusters_DF
354/200:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/201:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/202:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/203:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/204: clusters_DF
354/205:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/206:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/207:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/208:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/209: clusters_DF
354/210:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_3.html')
354/211:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_3.html')
354/212:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/213: clusters_DF
354/214:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/215:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/216:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/217:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.8
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/218: clusters_DF
354/219:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_3.html')
354/220:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/221:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/222:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/223:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.9
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/224: clusters_DF
354/225:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/226:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_3.html')
354/227:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
354/228:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
354/229:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/230:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/231:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.5
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/232: clusters_DF
354/233:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/234:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/235:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/236:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.6
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/237: clusters_DF
354/238:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/239:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/240:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/241:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.7
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/242: clusters_DF
354/243:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/244:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/245:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/246:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/247:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/248:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.8
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/249: clusters_DF
354/250:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.75
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/251: clusters_DF
354/252:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.85
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/253: clusters_DF
354/254:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/255:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
centers=[]
labels=[]
model = MeanShift()
model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df
354/256:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
354/257:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.9
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/258: clusters_DF
354/259:
counts_clusters=df.groupby(['cluster_label']).size()
#df=df.loc[(df['cluster_label'] !=label_cl)]
clusters_DF=pd.DataFrame(counts_clusters)
clusters_DF=clusters_DF.sort_values(0, ascending=[False])
clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
clusters_DF["cluster_label"]=clusters_DF.index
#print(clusters_DF.index[0])
min_value=0.95
cluster_keep=[]
z=0
sum_val=0
for i in clusters_DF['normalized']:
    sum_val=clusters_DF['normalized'][0:z].sum()
    if sum_val>min_value:
        cluster_keep.append(clusters_DF.index[0:z])
        break
    else:
        z+=1
cluster_keep=pd.DataFrame(cluster_keep)
range_1=cluster_keep.size
354/260: clusters_DF
354/261:
df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
354/262:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_3.html')
357/1:
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
357/2: from Book_extraction_single import search_for_file_path
357/3:
# def search_for_file_path ():
#     root = tkinter.Tk()
#     root.withdraw() #use to hide tkinter window
#     currdir = os.getcwd()
#     tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
#     if len(tempdir) > 0:
#         print ("You chose: %s" % tempdir)
#     base=os.path.basename(tempdir)
#     base=os.path.splitext(base)[0]

#     return tempdir, base
357/4: path_file, base=search_for_file_path ()
357/5:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/6:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3,1):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=1
357/7:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/8:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3,1):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=0.1
357/9:
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
357/10:
centers = model.cluster_centers_ # Coordinates of cluster centers.
labels = model.predict(df[df.columns[2:4]]) # Labels of each point
df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
357/11:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
357/12:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=0.1
357/13:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
357/14:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=0.1
357/15:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/16:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=0.1
357/17:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
357/18:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/19:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    f+=0.05
357/20:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/21:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
357/22:
# K_clusters = range(1,10)
# kmeans = [KMeans(n_clusters=i) for i in K_clusters]
# Y_axis = df[['LAT_google']]
# X_axis = df[['LON_google']]
# score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# # Visualize
# plt.plot(K_clusters, score)
# plt.xlabel('Number of Clusters')
# plt.ylabel('Score')
# plt.title('Elbow Curve')
# plt.show()
357/23:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    # df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    # plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.05
357/24:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
 
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.05
357/25:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
357/26:
ff=len(df)
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_', base, '.html')
357/27:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_', base, '.html')
357/28:
ff=len(df)

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_',base, '.html')
357/29:
ff=len(df)

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_' +base + '.html')
357/30:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
357/31:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
 
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
import folium
from folium import plugins
359/2: from Book_extraction_single import search_for_file_path
359/3: path_file, base=search_for_file_path ()
359/4:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
359/5:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
 
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.05
359/6:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.05
359/7:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
359/8:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/9:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/10:
ff=len(df)
imp

# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/11:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/12:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/13:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/14:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
359/15:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/16:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(2):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/17:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/18:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(2):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/19:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
359/20:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/21:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF
359/22:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/23:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/24:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/25:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/26:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title("Clustering with min_value= " + min_value)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/27:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title("Clustering with min_value= ", min_value)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/28:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value =%i' %min_value)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/29:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.1
359/30:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/31:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/32:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(3):
    f=0
    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/33:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/34:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/35:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/36:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/37:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/38:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/39:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(4):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/40:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/41:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/42:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/43:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/44:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/45:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,label_2,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/46:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
        popup=label_2,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/47:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + label_2,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/48:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + '\n' +'<b>'label_2'</b>',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/49:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + '\n' +'<b>' label_2 '</b>',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/50:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Quotes"].iloc[: ff], df["lugares"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + '\n' +label_2,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/51:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + '\n' +label_2,
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/52:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/53: path_file, base=search_for_file_path ()
359/54:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/55:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(4):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/56:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/57:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/58:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
359/59:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
359/60:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
359/61:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/62:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, position, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=position + ' ' + label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/63:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= pos + ' ' + label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/64:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/65:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= pos+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/66:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
359/67:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('YourMap_'+ base+ '.html')
361/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
361/2: from Book_extraction_single import search_for_file_path
361/3: path_file, base=search_for_file_path ()
361/4:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
361/5:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
361/6:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
361/7:
ff=len(df)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Cleaned_data/Geocode_" + base + ".csv")
361/8:
ff=len(df)
import folium
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Cleaned_data/Geocode_" + base + ".csv")
361/9:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Cleaned_data/Geocode_" + base + ".csv")
361/10:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
367/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
367/2: path_file, base=search_for_file_path ()
367/3:
# define the world map centered around Canada with a low zoom level



df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median, df['LON_google'].median], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
367/4:
lat_med=df['LAT_google'].median
lon_med=df['LON_google'].median
367/5: lat_med
367/6: lat_med, lon_med
367/7:
lat_med=df['LAT_google'].median()
lon_med=df['LON_google'].median()
367/8:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
367/9: lat_med, lon_med
367/10:
# define the world map centered around Canada with a low zoom level



df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median(axis=0)], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
367/11: world_map
367/12:
# define the world map centered around Canada with a low zoom level



df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median(axis=0)], zoom_start=13, tiles='OpenStreetMap')

# display world map
#world_map
367/13: world_map
367/14: g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
367/15:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = gmaps.places_nearby(location=lat_med, lon_med, radius = 40000, open_now =False , type = 'restaurant')
367/16:
lat_med, lon_med
location_med=str(lat_med, lon_med)
367/17:
lat_med, lon_med
location_med=str(lat_med + ',' + lon_med)
367/18:
lat_med, lon_med
location_med=str(lat_med, ','  ,lon_med)
367/19:
lat_med, lon_med
location_med=str(lat_med)
367/20:
lat_med, lon_med
location_med=str(lat_med), str(lon_med)
367/21:
lat_med, lon_med
location_med=str(lat_med), str(lon_med)
location_med
367/22:
lat_med, lon_med
location_med=str(lat_med), str(lon_med)
location_med=str(location_med)
367/23:
lat_med, lon_med
location_med=str(lat_med), str(lon_med)
location_med=str(location_med)
location_med
367/24:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me, lon_med
367/25:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me, lon_med
location_med
367/26:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me, lon_med
lat_me
367/27:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + lon_med
lat_me
367/28:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + lon_med
location_med
367/29:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
367/30:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = gmaps.places_nearby(location=location_med, lon_med, radius = 40000, open_now =False , type = 'restaurant')
367/31:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = gmaps.places_nearby(location='-33.8670522,151.1957362', lon_med, radius = 40000, open_now =False , type = 'restaurant')
367/32:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = gmaps.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'restaurant')
367/33:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = gkey.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'restaurant')
367/34:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'restaurant')
367/35: places_result
367/36: df=pd.DataFrame(places_result)
367/37: places_result['results']
367/38: places_result['results'][0]
367/39: places_result['results'][0][0]
367/40: places_result['results'][0]
367/41: places_result['price_level'][0]
367/42: places_result['results'][0]
367/43: places_result['results'][1]
367/44: places_result['results'][1]['business_status]
367/45: places_result['results'][1]['business_status']
367/46: places_result['results'][1]['place_id']
367/47: places_result['results'][1]['price_level']
367/48:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'landmark')
367/49: places_result['results'][1]['price_level']
367/50: places_result['results'][1]
367/51: places_result['results']
367/52: places_result
367/53:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'art_gallery')
367/54: places_result
367/55: places_result[0]
367/56: places_result['results']
367/57: places_result['results'][0]
367/58:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'airport')
367/59: places_result['results'][0]
367/60: places_result['results'][0]['lat']
367/61: places_result['results'][0]
367/62: places_result['results'][0]['viewport']
367/63: places_result['results'][0]
367/64: places_result['results'][0]['location']
367/65: places_result['results'][0]
367/66: places_result['results'][0]['name']
367/67: places_result['results'][0]
371/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
371/2: path_file, base=search_for_file_path ()
371/3:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
371/4:
# define the world map centered around Canada with a low zoom level

df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
371/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
371/6:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
371/7:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'airport')
371/8: places_result['results'][0]
371/9: Data_PD=pd.DataFrame(places_result['results'][0])
371/10: len(places_result)
371/11: Data_PD=pd.DataFrame(places_result['results'])
371/12: Data_PD
371/13:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'museum')
371/14: len(places_result)
371/15: Data_PD=pd.DataFrame(places_result['results'])
371/16: Data_PD
371/17: Data_PD['geometry']
371/18: Data_PD['geometry'][0]
371/19: Data_PD['geometry'][0][5:8]
371/20: Data_PD['geometry'][0][0]
371/21: Data_PD['geometry'][0]
371/22: Data_PD['geometry'][0]['location']
371/23: Data_PD['geometry'][0]['location']['lat']
371/24: Data_PD['geometry'][1]['location']['lat']
371/25: Data_PD['geometry'][0]['location']['lat']
371/26:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
371/27:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 40000, open_now =False , type = 'town_square')
371/28: len(places_result)
371/29:
Data_PD=pd.DataFrame(places_result['results'])
#Data_PD.to_csv('prova.csv')
371/30: Data_PD['geometry'][0]['location']['lat']
371/31: Data_PD
371/32: Data_PD[1]
371/33:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
371/34:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'town_square')
371/35: len(places_result)
371/36:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'landmark')
371/37: len(places_result)
371/38: len(places_result)
371/39:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'landmark')
371/40: len(places_result)
371/41:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
371/42: Data_PD
371/43:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'tourist_attraction')
371/44: len(places_result)
371/45:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
371/46: Data_PD
379/1:
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
379/2:
path_file='Donation.csv'
df = pd.read_csv(path_file)
379/3: df
379/4:
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
379/5:
df
df = DataFrame(Data,columns=['Emotion','Challenge'])
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/6:
df
df = df(Data,columns=['Emotion','Challenge'])
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/7:
df
df = df(Data,columns=['Emotion','Challenge'])
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/8:
df
df = df(df,columns=['Emotion','Challenge'])
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/9:
df
df = df,columns=['Emotion','Challenge']
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/10:
df
df = df['Emotion','Challenge']
  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/11:
df

  
kmeans = KMeans(n_clusters=3).fit(df['Emotion','Challenge'])
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/12: df["Emotion"]
379/13: df
379/14:
path_file='Donation.csv'
df = pd.read_csv(path_file)
379/15: df
379/16:


  
kmeans = KMeans(n_clusters=3).fit(df['Emotion','Challenge'])
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/17: df["Emotion"]
379/18: df['Emotion','Challenge']
379/19: df['Emotion']
379/20: df[['Emotion']. ["Challenge"]]
379/21: df[['Emotion'],["Challenge"]]
379/22:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/23:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
379/24:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Challenge'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/25: df
379/26:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['Emotion'], df['Empowered'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/27:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df, c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/28:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

# plt.scatter(df, c= kmeans.labels_.astype(float), s=50, alpha=0.5)
# plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
# plt.show()
379/29:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df["Emotion"], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
379/30:


  
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df.index(), df["Emotion"], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
381/1:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
381/2: df
381/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
381/4:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
381/5: df
381/6:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
381/7: df
381/8:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
381/9: df
381/10:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
381/11: df
381/12:
kmeans = KMeans(n_clusters=3).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

# plt.scatter(df.index(), df["Emotion"], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
# plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
# plt.show()
381/13: df=df.drop(df["Transportation means Share "], axis=1)
381/14: df=df.drop(df["Transportation means Share"], axis=1)
381/15: df.describe()
385/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()
import csv
385/2:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
385/3: df.describe()
385/4: Data_description=df.describe()
385/5: Data_description
385/6: df
385/7:
path_file='Monopattino1.csv'
df = pd.read_csv(path_file)
Data_Monopat=df.drop(['Transportation means'], axis=1)
385/8: Data_Monopat
385/9: Data_description=df.describe()
385/10: Data_description
385/11: Data_description=Data_description.drop(["ID"], axis=1)
385/12:
Data_description=Data_description.drop(["ID"], axis=1)
Data_description
385/13: Data_description
385/14: Data_Monopat.shape
385/15: Data_description.mean
385/16: Data_description.mean()
385/17: Data_description
385/18: Data_description["Interest"]
385/19: Data_description["Interest"][1]
385/20: Data_description[:,[1]
385/21: Data_description[:,1]
385/22: Data_description[1]
385/23: Data_description.iloc[:,1]
385/24: Data_description.iloc[1,:]
385/25: Mean_values=Data_description.iloc[1,:]
385/26:
sns.set_theme(style="whitegrid")

Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Datos, kind="bar",
    x="Value", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/27:
#sns.set_theme(style="whitegrid")

Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Datos, kind="bar",
    x="Value", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/28:
#sns.set_theme(style="whitegrid")

#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Mean_values, kind="bar",
    x="Value", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/29:
#sns.set_theme(style="whitegrid")

#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/30:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/31:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/32:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/33:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/34:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
tips = sns.load_dataset(Mean_values)
ax = sns.barplot(x="day", y="total_bill", data=tips)
385/35:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="day", y="total_bill", data=Mean_values)
385/36:
Mean_values=Data_description.iloc[1,:]
Mean_values
385/37:
Mean_values=Data_description.iloc[1,:]
type(Mean_values)
385/38:
Mean_values=Data_description.iloc[1,:]
Mean_values_M=pd.DataFrame(Mean_values)
385/39:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="day", y="total_bill", data=Mean_values_M)
385/40:
Mean_values=Data_description.iloc[1,:]
Mean_values_M=pd.DataFrame(Mean_values)
Mean_values_M
385/41:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="day", y="total_bill", data=Mean_values_M["mean"])
385/42:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="day", y="total_bill", data=Mean_values_M.mean)
385/43:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x=Mean_values_M.index, y=Mean_values_M.mean)
385/44:
Mean_values=Data_description.iloc[1,:]
Mean_values_M=pd.DataFrame(Mean_values)
Mean_values_M.mean
385/45:
Mean_values=Data_description.iloc[1,:]
Mean_values_M=pd.DataFrame(Mean_values)
Mean_values_M.index
385/46:
Mean_values=Data_description.iloc[1,:]
Mean_values_M=pd.DataFrame(Mean_values)
Mean_values_M.mean
385/47: Mean_values=Data_description.iloc[1,:]
385/48:
Mean_values=Data_description.iloc[1,:]
Mean_values
385/49:
Mean_values=Data_description.iloc[1,:]
Mean_values[:.1]
385/50:
Mean_values=Data_description.iloc[1,:]
Mean_values[:,1]
385/51:
Mean_values=Data_description.iloc[1,:]
Mean_values[0]
385/52:
Mean_values=Data_description.iloc[1,:]
Mean_values[:]
385/53:
Mean_values=Data_description.iloc[1,:]
Mean_values.index
385/54:
Mean_values=Data_description.iloc[1,:]
Mean_values.mean
385/55:
Mean_values=Data_description.iloc[1,:]
Mean_values[0]
385/56:
Mean_values=Data_description.iloc[1,:]
Mean_values[0:-1]
385/57:
Mean_values=Data_description.iloc[1,:]
Mean_values[1]
385/58:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x=Mean_values.index, y=Mean_values.mean)
385/59:
Mean_values=Data_description.iloc[1,:]
pd.DataFrame(Mean_values)
385/60:
Mean_values=Data_description.iloc[1,:]
Mean=pd.DataFrame(Mean_values)
385/61:
Mean_values=Data_description.iloc[1,:]
Mean=pd.DataFrame(Mean_values)
Mean
385/62:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="Mean", y="mean", Data=Mean)
385/63:
Mean_values=Data_description.iloc[1,:]
Mean=pd.DataFrame(Mean_values)
Mean["mean"]
385/64:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(x="Mean", y="mean", Data=Mean.reset_index())
385/65:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot( y="mean", Data=Mean.reset_index())
385/66:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot( y="mean", Data=Mean)
385/67:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot( y=Mean.mean, Data=Mean)
385/68:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Data=Mean)
385/69:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Mean)
385/70:
Mean_values=Data_description.iloc[1,:]
Mean=pd.DataFrame(Mean_values)
Mean
385/71:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot()
385/72:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Data=MEan)
385/73:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Data=Me_an)
385/74:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Data=Mean)
385/75:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Mean.mean)
385/76:
#sns.set_theme(style="whitegrid")
import seaborn as sns
#Datos = sns.load_dataset(Mean_values)

# Draw a nested barplot by species and sex
sns.set(style="whitegrid")
ax = sns.barplot(Mean["mean"])
385/77:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)
385/78:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)
titanic
385/79:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", data=titanic)
titanic
385/80:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", data=titanic)
titanic
385/81:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)
titanic
385/82:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="whitegrid")
ax = sns.barplot(x=Mean["mean"])
385/83:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="whitegrid")
ax = sns.barplot(y=Mean["mean"])
385/84:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="whitegrid")
ax = sns.barplot(x= Mean.index, y=Mean["mean"])
385/85:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="whitegrid")
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/86:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/87:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
sns.set_title('myTitle')
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/88:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
sns.set('myTitle')
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/89:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
sns.title('myTitle')
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/90:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
set_title('myTitle')
ax = sns.barplot(y= Mean.index, x=Mean["mean"])
385/91:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"]).set_title('myTitle')
385/92:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"]).set_title('Mean Values')
385/93: sns.catplot(x=Data_Monopat.index, y=Data_Monopat.values)
385/94: sns.catplot(x=Data_Monopat.index)
385/95: Data_Monopat.index
385/96: Data_Monopat.index()
385/97: Data_Monopat.index
385/98: Data_Monopat
385/99: Data_Monopat.iloc[0,:]
385/100: Data_Monopat.iloc[0,0]
385/101: Data_Monopat.iloc[0,:]
385/102: Data_Monopat.iloc[0,2:]
385/103: Data_Monopat.iloc[:,2:]
385/104:
Data_Monopat.iloc[:,2:]
Data_Monopat.columns
385/105:
ax = sns.barplot(y= Data_Monopat.columns, x=Data_Monopat.iloc[:,2:]).set_title('
Values')
385/106: ax = sns.barplot(y= Data_Monopat.columns, x=Data_Monopat.iloc[:,2:]).set_title('Values')
385/107: ax = sns.barplot(x= Data_Monopat.columns, y=Data_Monopat.iloc[:,2:]).set_title('Values')
385/108:
Data_Monopat.iloc[:,2:]
Data_Monopat.columns.iloc[:,2:]
385/109:
Data_Monopat.iloc[:,2:]
Data_Monopat.columns[0:]
385/110:
Data_Monopat.iloc[:,2:]
Data_Monopat.columns[2:]
385/111: ax = sns.barplot(x= Data_Monopat.columns[2:], y=Data_Monopat.iloc[:,2:]).set_title('Values')
385/112: ax = sns.catplot(x= Data_Monopat.columns[2:], y=Data_Monopat.iloc[:,2:]).set_title('Values')
385/113:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.catplot(y= Mean.index, x=Mean["mean"]).set_title('Mean Values')
385/114:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"]).set_title('Mean Values')
385/115:
sns.set_theme(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/116:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/117:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
#g.legend.set_title("")
385/118: penguins
385/119:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Mean, kind="bar",
    y= Mean.index, x=Mean["mean"],
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
#g.legend.set_title("")
385/120:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Mean, kind="bar",
    y= Mean.index, x=Mean["mean"],
    palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
#g.legend.set_title("")
385/121:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Mean, kind="bar",
    y= Mean.index, x=Mean["mean"])
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
#g.legend.set_title("")
385/122:
sns.set_theme(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/123:
sns.set_theme(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/124:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
g.legend.set_title("")
385/125:
sns.set(style="whitegrid")

penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=penguins, kind="bar",
    x="species", y="body_mass_g", hue="sex",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/126: Data_Monopat
385/127:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x="Share mobility", y="Interest", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/128:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x="Share mobility", y="Interest", "easy", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/129:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x="Share mobility", y="easy", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/130:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x="Share mobility", y="Easy", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/131:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x="Share mobility", y="environment", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
385/132:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"], alpha=.6, height=6).set_title('Mean Values')
385/133:
#sns.set_theme(style="whitegrid")
import seaborn as sns

sns.set(style="dark")
ax = sns.barplot(y= Mean.index, x=Mean["mean"], alpha=.6).set_title('Mean Values')
385/134:
sns.set(style="whitegrid")

#penguins = sns.load_dataset("penguins")

# Draw a nested barplot by species and sex
g = sns.catplot(
    data=Data_Monopat, kind="bar",
    x=Data_Monopat.columns, y="environment", hue="Share mobility",
    ci="sd", palette="dark", alpha=.6, height=6
)
g.despine(left=True)
g.set_axis_labels("", "Body mass (g)")
# g.legend.set_title("")
391/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
394/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
394/2: df=read_csv("Monopattino.1.csv")
394/3: df=pd.read_csv("Monopattino.1.csv")
394/4: df=pd.read_csv("Monopattino1.csv")
394/5:
df=pd.read_csv("Monopattino1.csv")
df
394/6:
df=pd.read_csv("Monopattino_corrected.csv")
df
397/1:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
397/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape()
397/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
397/4:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape()
397/5:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
411/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
411/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
411/3: df_description=df.describe
411/4:
df_description=df.describe
df_description
411/5:
df_description=df.describe
pd.DataFrame(df_description)
411/6:
df_description=df.describe
pd.DataFrame(df_description)
411/7:
df_description=df.describe
FF=DataFrame(df_description)
411/8:
df_description=df.describe
FF=pd.DataFrame(df_description)
411/9:
df_description=df.describe
FF=pd.DataFrame(df)
411/10: df_description=df.describe()
411/11:
df_description=df.describe()
df_description
411/12: df
411/13: df=df.drop([0])
411/14: df
411/15:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
411/16: df
411/17: df.columns
411/18: df=df.drop(df.columns)
411/19:
df=pd.read_csv("Monopattino_corrected.csv" ,header=False, index=False)
df.shape
411/20:
df=pd.read_csv("Monopattino_corrected.csv", header=False)
df.shape
411/21:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df[:].values
411/22:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
pd.DataFrame(df[:].values)
411/23:
df=pd.read_csv("Monopattino_corrected.csv", header=None,index=False)
df.shape
pd.DataFrame(df[:].values)
411/24:
df=pd.read_csv("Monopattino_corrected.csv", header=None)
df.shape
pd.DataFrame(df[:].values)
411/25:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
pd.DataFrame(df[:].values)
411/26:
df=pd.read_csv("Monopattino_corrected.csv", header=None)
df.shape
#pd.DataFrame(df[:].values)
411/27:
df=pd.read_csv("Monopattino_corrected.csv", header=None)
df.shape
df
#pd.DataFrame(df[:].values)
411/28:
df=pd.read_csv("Monopattino_corrected.csv", header=None)
df.shape
df
#pd.DataFrame(df[:].values)
411/29:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
411/30:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
411/31:
df_description=df.describe()
df_description
411/32: df_description.mean
411/33: df_description.mean()
411/34: df_description.mean
411/35: pd.DataFrame(df_description.mean)
411/36: pd.DataFrame(df_description.mean())
416/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
416/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
416/3:
df_description=df.describe()
df_description
416/4: Reaction_DF=df_description.iloc[:,0:8]
416/5:
Reaction_DF=df_description.iloc[:,0:8]
Reaction_DF
416/6:
Reaction_DF=df_description.iloc[:,0:8]
SocialMedia_DF=df_description.iloc[:,9:]
Reaction_DF
416/7:
Reaction_DF=df_description.iloc[:,0:8]
SocialMedia_DF=df_description.iloc[:,9:]
SocialMedia_DF
416/8:
Reaction_DF=df_description.iloc[:,0:8]
SocialMedia_DF=df_description.iloc[:,10:]
SocialMedia_DF
416/9: print("Data shape is: ", df.shape)
416/10:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
416/11:
print("Data shape is: ", df.shape)
Reaction_DF=df.iloc[:,0:8]
SocialMedia_DF=df.iloc[:,10:]
416/12: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
416/13: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
416/14: df['Transportation mean'].value_counts()
416/15: df['Used of shared mobility'].value_counts()
416/16: Reaction_DF.value_counts().plot(kind="bar")
416/17: Reaction_DF["safe"].value_counts().plot(kind="bar")
416/18: Reaction_desc[0].value_counts().plot(kind="bar")
416/19: Reaction_desc[1].value_counts().plot(kind="bar")
416/20: Reaction_desc[1]
416/21: Reaction_desc[0]
416/22: Reaction_desc[0,0]
416/23: Reaction_desc.iloc[0:]
416/24: Reaction_desc.iloc[0,:]
416/25: Reaction_desc.iloc[1,:]
416/26: Reaction_mean=Reaction_desc.iloc[1,:]
416/27:
Reaction_mean=Reaction_desc.iloc[1,:]
Reaction_mean.plot(kind="bar")
416/28:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
Reaction_mean.plot(kind="bar")
416/29:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
Reaction_mean.plot(kind="bar")
SocialMedia_mean.plot(kind="bar")
416/30:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar")
g2=SocialMedia_mean.plot(kind="bar")
416/31:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar")
416/32:

g2=SocialMedia_mean.plot(kind="bar")
416/33:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
416/34:

g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d200")
416/35:

g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
419/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
419/2: from Book_extraction_single import search_for_file_path
419/3:
# def search_for_file_path ():
#     root = tkinter.Tk()
#     root.withdraw() #use to hide tkinter window
#     currdir = os.getcwd()
#     tempdir = filedialog.askopenfilename(parent=root, initialdir=currdir, title='Please select a directory')
#     if len(tempdir) > 0:
#         print ("You chose: %s" % tempdir)
#     base=os.path.basename(tempdir)
#     base=os.path.splitext(base)[0]

#     return tempdir, base
419/4: path_file, base=search_for_file_path ()
419/5:

ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
419/6:
# K_clusters = range(1,10)
# kmeans = [KMeans(n_clusters=i) for i in K_clusters]
# Y_axis = df[['LAT_google']]
# X_axis = df[['LON_google']]
# score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# # Visualize
# plt.plot(K_clusters, score)
# plt.xlabel('Number of Clusters')
# plt.ylabel('Score')
# plt.title('Elbow Curve')
# plt.show()
419/7:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
for k in range(4):
    f=0
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    min_value=0.5+f
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
 
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f+=0.05
419/8:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
df
420/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
420/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
420/3:
print("Data shape is: ", df.shape)
Reaction_DF=df.iloc[:,0:8]
SocialMedia_DF=df.iloc[:,10:]
420/4:
df_description=df.describe()
df_description
420/5:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
420/6: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
420/7: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
420/8: #First lets take a look at the data with graphs
420/9: df['Transportation mean'].value_counts()
420/10: df['Used of shared mobility'].value_counts()
420/11:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/12:

g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
420/13:
Reaction_mean=((Reaction_desc.iloc[1,:]-Reaction_desc.iloc[1,:].min())/((Reaction_desc.iloc[1,:].max-Reaction_desc.iloc[1,:].min())
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/14: df['Used of shared mobility'].value_counts()
420/15:
Reaction_mean=((Reaction_desc.iloc[1,:]-Reaction_desc.iloc[1,:].min())/((Reaction_desc.iloc[1,:].max-Reaction_desc.iloc[1,:].min())
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/16:
Reaction_mean=((Reaction_desc.iloc[1,:]-Reaction_desc.iloc[1,:].min())/((Reaction_desc.iloc[1,:].max-Reaction_desc.iloc[1,:].min())
#SocialMedia_mean=SocialMedia_desc.iloc[1,:]
#g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/17:
Reaction_mean=((Reaction_desc.iloc[1,:]-Reaction_desc.iloc[1,:].min())/((Reaction_desc.iloc[1,:].max-Reaction_desc.iloc[1,:].min())
#SocialMedia_mean=SocialMedia_desc.iloc[1,:]
#g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/18:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import 
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
420/19:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
420/20:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
420/21:
X, _ = make_blobs(n_samples=271, centers=3, n_features=8)

Prueba = pd.DataFrame(X, columns=['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4'])

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(df[['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4']])

Prueba['Cluster'] = y

print(Prueba.head())
420/22:
X, _ = make_blobs(n_samples=10, centers=3, n_features=8)

Prueba = pd.DataFrame(X, columns=['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4'])

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(df[['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4']])

Prueba['Cluster'] = y

print(Prueba.head())
420/23:
X, _ = make_blobs(n_samples=10, centers=3, n_features=4)

Prueba = pd.DataFrame(X, columns=['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4'])

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(df[['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4']])

Prueba['Cluster'] = y

print(Prueba.head())
420/24:
X, _ = make_blobs(n_samples=10, centers=3, n_features=4)

zzz = pd.DataFrame(X, columns=['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4'])

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(zzz[['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4']])

zzz['Cluster'] = y

print(zzz.head())
420/25: X
420/26: Reaction_DF.columns
420/27: columns_reaction=Reaction_DF.columns
420/28:
#X, _ = make_blobs(n_samples=10, centers=3, n_features=4)

zzz = pd.DataFrame(Reaction_DF, columns=columns_reaction)

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(zzz[columns_reaction])

zzz['Cluster'] = y

print(zzz.head())
420/29: Reaction_DF
420/30: df.dtypes
420/31:
df.dtypes
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
420/32:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
420/33:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
420/34: obj_df["Transportation mean"].value_counts()
420/35: obj_df["Used of shared mobility"].value_counts()
420/36: obj_df["Pricing 1 or 0"].value_counts()
420/37:
cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
                "Used of shared mobility": {"Public transportation": 1, "Car": 0 }}
420/38:
cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
                "Used of shared mobility": {"Yes": 1, "No": 0 },
                "Transportation mean": {"Public transportation": 1, "Car": 0 }}
420/39:
obj_df = obj_df.replace(cleanup_nums)
obj_df.head()
420/40: obj_df["Transportation mean"].value_counts()
420/41:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
420/42: obj_df["Transportation mean"].value_counts()
420/43: obj_df["Used of shared mobility"].value_counts()
420/44: obj_df["Pricing 1 or 0"].value_counts()
420/45: obj_df["Pricing 1 or 0"].value_counts()
420/46: pd.get_dummies(obj_df, columns=["Transportation mean"]).head()
420/47: pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]).head()
423/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
423/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
423/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
423/4: obj_df["Transportation mean"].value_counts()
423/5: obj_df["Used of shared mobility"].value_counts()
423/6: obj_df["Pricing 1 or 0"].value_counts()
423/7:
# cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
#                 "Used of shared mobility": {"Yes": 1, "No": 0 },
#                 "Transportation mean": {"Public transportation": 1, "Car": 0 }}
423/8: pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]).head()
423/9:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
423/10: obj_df
423/11:
oe_style = OneHotEncoder()
oe_results = oe_style.fit_transform(obj_df[["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]])
pd.DataFrame(oe_results.toarray(), columns=oe_style.categories_).head()
423/12:
oe_style = OneHotEncoder()
oe_results = oe_style.fit_transform(obj_df[["Transportation mean"]])
pd.DataFrame(oe_results.toarray(), columns=oe_style.categories_).head()
423/13: Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]).head()
423/14:
Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]).head()
Obj_DF
423/15: TOTAL_DF=pd.concat(df, obj_df, axis=1)
423/16: TOTAL_DF=pd.concat(df, obj_df, axis=0)
423/17: TOTAL_DF=pd.concat(df, obj_df)
423/18: TOTAL_DF=pd.concat([df, obj_df], axis=1)
423/19:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
TOTAL_DF
423/20:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
TOTAL_DF
TOTAL_DF.select_dtypes(include=['object']).drop()
423/21:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
TOTAL_DF
TOTAL_DF.select_dtypes(exclude=['object'])
423/22:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])
423/23:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/24: TOTAL_DF.describe
423/25: TOTAL_DF.describe()
423/26:
#X, _ = make_blobs(n_samples=10, centers=3, n_features=4)

X, _ = make_blobs(n_samples=10, centers=3, n_features=4)

zzz = pd.DataFrame(X, columns=['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4'])

kmeans = KMeans(n_clusters=3)

y = kmeans.fit_predict(zzz[['Feat_1', 'Feat_2', 'Feat_3', 'Feat_4']])

zzz['Cluster'] = y

print(zzz.head())
423/27:
clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters'] = clustering_kmeans.fit_predict(TOTAL_DF)
423/28: TOTAL_DF
423/29: TOTAL_DF=TOTAL_DF.drop(columns="clusters")
423/30:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
423/31:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_3'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-1])
423/32: TOTAL_DF.save_csv("prova.csv")
423/33: TOTAL_DF.to_csv("prova.csv")
423/34: breakpoint()
423/35:
*These are italicized words, not a bullet list*
**These are bold words, not a bullet list**
423/36:
##*These are italicized words, not a bullet list*
##**These are bold words, not a bullet list**
423/37:
##*These are italicized words, not a bullet list*
##**These are bold words, not a bullet list**
423/38:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 300,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_features)
    sse.append(kmeans.inertia_)
423/39:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 300,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
423/40:
plt.style.use("fivethirtyeight")
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/41:
kmeans_kwargs = {
"init": "random",
"n_init": 15,
"max_iter": 300,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
423/42:
plt.style.use("fivethirtyeight")
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/43:
plt.style.use("fivethirtyeight")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/44:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 300,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
423/45:
plt.style.use("fivethirtyeight")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/46:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
423/47:
plt.style.use("fivethirtyeight")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/48:
kl = KneeLocator(
range(1, 11), sse, curve="convex", direction="decreasing")
kl.elbow
423/49: from kneed import KneeLocator
423/50:
kl = KneeLocator(
range(1, 11), sse, curve="convex", direction="decreasing")
kl.elbow
423/51:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
423/52:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_4'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-2])
423/53:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/54:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
423/55:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
423/56:
df_description=df.describe()
df_description
423/57:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
423/58: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
423/59:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
423/60:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
423/61:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse, figsize=(15,7))
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/62:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/63: TOTAL_DF
423/64: Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["cluster_2"]).mean()
423/65: Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
423/66: Group_Cluster_2
423/67:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
423/68: Group_Cluster_gender
423/69: TOTAL_DF
423/70: TOTAL_DF.iloc[:,:-3]
423/71: TOTAL_DF
423/72:
Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"]).head()
Obj_DF
423/73:
TOTAL_DF=pd.concat([df, obj_df], axis=1)
#TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/74:
TOTAL_DF=pd.concat([df, obj_DF], axis=1)
#TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/75:
TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
#TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/76:
TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/77: TOTAL_DF.describe()
423/78:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
423/79:
Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
Obj_DF
423/80:
TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
423/81: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
423/82:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
423/83:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_3'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-1])
423/84:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_4'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-2])
423/85: TOTAL_DF.to_csv("prova.csv")
423/86:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
423/87:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
423/88:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
423/89:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
423/90:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
423/91: TOTAL_DF
423/92: Group_Cluster_gender
423/93:
plt.style.use("seaborn")
g2=Group_Cluster_gender.plot(kind="bar", figsize=(15,7), color="#61d150")
423/94:
plt.style.use("seaborn")
Group_Cluster_gender.plot(kind="bar", figsize=(15,7), color="#61d150")
428/1:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
428/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
428/3:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
428/4:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
428/5: obj_df["Transportation mean"].value_counts()
428/6: obj_df["Used of shared mobility"].value_counts()
428/7: obj_df["Pricing 1 or 0"].value_counts()
428/8:
# cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
#                 "Used of shared mobility": {"Yes": 1, "No": 0 },
#                 "Transportation mean": {"Public transportation": 1, "Car": 0 }}
428/9:
Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
Obj_DF
428/10:
TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
428/11:
print("Data shape is: ", df.shape)
Reaction_DF=df.iloc[:,0:8]
SocialMedia_DF=df.iloc[:,10:]
428/12:
df_description=df.describe()
df_description
428/13: TOTAL_DF.describe()
428/14:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
428/15: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
428/16: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
428/17: #First lets take a look at the data with graphs
428/18:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
428/19:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
428/20: columns_reaction=Reaction_DF.columns
428/21:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
428/22:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_3'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-1])
428/23:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_4'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-2])
428/24: TOTAL_DF.to_csv("prova.csv")
428/25:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
428/26:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
428/27:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
428/28:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
428/29:
cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
                "Used of shared mobility": {"Yes": 1, "No": 0 },
                "Transportation mean": {"Public transportation": 1, "Car": 0 }}
428/30: cleanup_nums
428/31:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
428/32:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
428/33: #from kneed import KneeLocator
428/34:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()


Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
428/35:
plt.style.use("seaborn")
Group_Cluster_gender.plot(kind="bar", figsize=(15,7), color="#61d150")
428/36: Group_Cluster_gender
428/37:
import seaborn as sns
sns.set_theme(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
428/38:
import seaborn as sns
sns.set(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)
428/39: ax = sns.barplot(x="day", y="total_bill", hue="sex", data=tips)
428/40: tips
428/41: ax = sns.barplot(x="Gender (1-male; 2-female)", y="interesting solution", hue="Gender (1-male; 2-female)", data=Group_Cluster_gender)
428/42: ax = sns.barplot(x="Gender (1-male; 2-female)", y="interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/43: ax = sns.barplot( y="interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/44: ax = sns.barplot( x="interesting solution", y="interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/45: ax = sns.barplot( x="Gender (1-male; 2-female)", y="interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/46: ax = sns.barplot( x="Gender (1-male; 2-female)", y=["interesting solution","interesting solution"], hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/47: ax = sns.barplot( x="Gender (1-male; 2-female)", y="interesting solution","interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/48: ax = sns.barplot( x="Gender (1-male; 2-female)", y="interesting solution":"interesting solution", hue="Gender (1-male; 2-female)", data=TOTAL_DF)
428/49:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
428/50: Group_Cluster_gender.columns
428/51: Group_Cluster_gender.columns, Group_Cluster_2.index
428/52:
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
my_list=list(Group_Cluster_gender.columns)
del my_list[0]
Features = my_list
values = Group_Cluster_gender
ax.bar(Features,values)
plt.show()
428/53:
my_list=list(Group_Cluster_gender.columns)
my_list
428/54:
my_list=list(Group_Cluster_gender.columns)
my_list[0]
428/55:
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
my_list=list(Group_Cluster_gender.columns)
#del my_list[0]
Features = my_list
values = Group_Cluster_gender
ax.bar(Features,values)
plt.show()
428/56: Group_Cluster_gender.iloc[0:]
428/57: Group_Cluster_gender.iloc[0,:]
428/58: Group_Cluster_gender.iloc[0,:]()
428/59: Group_Cluster_gender.iloc[0,:]
428/60:
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
my_list=list(Group_Cluster_gender.columns)
#del my_list[0]
Features = my_list
values = Group_Cluster_gender.iloc[0,:]
ax.bar(Features,values)
plt.show()
428/61: Group_Cluster_gender.T.plot()
428/62: Group_Cluster_gender.T.plot(kind='bar')
428/63: Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar')
428/64: Group_Cluster_gender.iloc[:,0:9].T.plot(kind='bar')
428/65: Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar')
428/66: Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
428/67: Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7)).title("fff")
428/68:
plt.style.use('seaborn')
Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
428/69:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.legend(loc=(0.65, 0.8))
428/70:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.legend(loc=(0.5, 0.8))
428/71:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
#plt1.legend(loc=(0.5, 0.8))
428/72:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.set("hh")
428/73:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.set_title("hh")
428/74:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.set_title("Perceptions by gender", fontsize=16)
428/75:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7))
plt1.set_title("Perceptions by gender", fontsize=18)
428/76:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12)
plt1.set_title("Perceptions by gender", fontsize=18)
428/77:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12)
plt1.set_title("Perceptions by gender", fontsize=18)
plt1.savefig("Perceptions by gender.pdf", bbox_inches='tight')
428/78:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.pdf", bbox_inches='tight')
428/79:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/80:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color='red')
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/81:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color='red, blue')
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/82:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/83:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['red', 'blue'])
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/84:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'])
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/85:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.2)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/86:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.8)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/87:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
428/88:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalgreen', 'lightcoral'], alpha=0.85)
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/89:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['lightgreen', 'lightcoral'], alpha=0.85)
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/90:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/91:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.label(loc = 'upper right')
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/92:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'upper right')
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/93:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/94:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=14, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/95:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/96:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=TOTAL_DF.iloc[:,:-1].groupby(["clusters_3"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
428/97:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("Perceptions in 3 clusters.jpeg", bbox_inches='tight')
428/98:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=TOTAL_DF.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=TOTAL_DF.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
428/99:
plt.style.use('seaborn')
plt4=Group_Cluster_4.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt4.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("Perceptions in 4 clusters.jpeg", bbox_inches='tight')
428/100:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/101:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/102:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/103:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-5].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/104:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['lightgreen', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/105:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'lightcoral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/106:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/107:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.8)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/108:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/109:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt2.set_title("Perceptions", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/110:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/111:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("Perceptions in 4 clusters.jpeg", bbox_inches='tight')
428/112:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("Perceptions in 3 clusters.jpeg", bbox_inches='tight')
428/113:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
428/114:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/115:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
428/116:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("Social Media in 3 clusters.jpeg", bbox_inches='tight')
428/117:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("Social Media in 3 clusters.jpeg", bbox_inches='tight')
428/118:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'violet'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("Social Media in 4 clusters.jpeg", bbox_inches='tight')
428/119:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'violet'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("Social Media in 4 clusters.jpeg", bbox_inches='tight')
428/120:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("Social Media in 4 clusters.jpeg", bbox_inches='tight')
428/121:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("Social Media by gender.jpeg", bbox_inches='tight')
428/122:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/123:

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
#%config InlineBackend.figure_format='retina'
428/124:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/125:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=25)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/126:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=30)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/127:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=30)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/128:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/129: X_std
428/130: pd.DataFrame(X_std)
428/131:
features = TOTAL_DF.columns
# Separating out the features
x = TOTAL_DF.loc[:, features].values
# Separating out the target
y = TOTAL_DF.loc[:,['target']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)

#%config InlineBackend.figure_format='retina'
428/132:
features = TOTAL_DF.columns
# Separating out the features
x = TOTAL_DF.loc[:, features].values
# Separating out the target
#y = TOTAL_DF.loc[:,['']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)

#%config InlineBackend.figure_format='retina'
428/133:
features = TOTAL_DF.columns
# Separating out the features
x = TOTAL_DF.loc[:, features].values
# Separating out the target
#y = TOTAL_DF.loc[:,['']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)
x
#%config InlineBackend.figure_format='retina'
428/134:
features = TOTAL_DF.columns
# Separating out the features
x = TOTAL_DF.loc[:, features].values
# Separating out the target
#y = TOTAL_DF.loc[:,['']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)
pd.DataFrame(x)
#%config InlineBackend.figure_format='retina'
428/135:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/136:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
428/137: principalComponents
428/138: pd.DataFrame(principalComponents)
428/139: PCA_components
428/140:
correlation = TOTAL_DF.corr()
correlation.style.background_gradient(cmap='coolwarm')
428/141:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
428/142:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/143:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
correlation.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/144:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/145:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/146:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=plt.matshow(TOTAL_clean.corr())
plt.savefig('correlation.png'.format(subject,trial), bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/147:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=plt.matshow(TOTAL_clean.corr())
plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/148:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=plt.(TOTAL_clean.corr())
plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/149:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=plt.TOTAL_clean.corr()
plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/150:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=TOTAL_clean.corr()
plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/151:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
#correlation = TOTAL_clean.corr()
#correlation.style.background_gradient(cmap='coolwarm')
img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
428/152:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
431/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler<
431/2: correlation
431/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
431/4:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
431/5:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
431/6: obj_df["Transportation mean"].value_counts()
431/7: obj_df["Used of shared mobility"].value_counts()
431/8: obj_df["Pricing 1 or 0"].value_counts()
431/9:
cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
                "Used of shared mobility": {"Yes": 1, "No": 0 },
                "Transportation mean": {"Public transportation": 1, "Car": 0 }}
431/10:
Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
Obj_DF
431/11:
TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

TOTAL_DF
431/12:
print("Data shape is: ", df.shape)
Reaction_DF=df.iloc[:,0:8]
SocialMedia_DF=df.iloc[:,10:]
431/13:
df_description=df.describe()
df_description
431/14: TOTAL_DF.describe()
431/15:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
431/16: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
431/17: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
431/18: #First lets take a look at the data with graphs
431/19:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
431/20:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
431/21: columns_reaction=Reaction_DF.columns
431/22:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
431/23:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_3'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-1])
431/24:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_4'] = clustering_kmeans.fit_predict(TOTAL_DF.iloc[:,:-2])
431/25: TOTAL_DF.to_csv("prova.csv")
431/26:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
431/27:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(TOTAL_DF)
    sse.append(kmeans.inertia_)
431/28:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
431/29:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
431/30: #from kneed import KneeLocator
431/31:
Group_Cluster_2=TOTAL_DF.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=TOTAL_DF.iloc[:,:-1]
Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=TOTAL_DF.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=TOTAL_DF.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
431/32:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
431/33:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("Social Media by gender.jpeg", bbox_inches='tight')
431/34:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
431/35:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
431/36:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("Perceptions in 3 clusters.jpeg", bbox_inches='tight')
431/37:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("Social Media in 3 clusters.jpeg", bbox_inches='tight')
431/38:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("Perceptions in 4 clusters.jpeg", bbox_inches='tight')
431/39:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("Social Media in 4 clusters.jpeg", bbox_inches='tight')
431/40:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
431/41:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
431/42:
TOTAL_clean=TOTAL_DF.iloc[:,:-4]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
431/43: correlation
431/44:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("Perceptions in 3 clusters.jpeg", bbox_inches='tight')
431/45: correlation.to_csv("correlation.csv")
431/46:
TOTAL_clean=TOTAL_DF.iloc[:,:-3]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
431/47: correlation.to_csv("correlation.csv")
431/48: from factor_analyzer import FactorAnalyzer
431/49: from factor_analyzer import FactorAnalyzer
431/50:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
431/51:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
431/52: kmo_model
431/53:
fa = FactorAnalyzer()
fa.analyze(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/54:
from factor_analyzer import analyze
fa = FactorAnalyzer()
fa.analyze(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/55:
fa = FactorAnalyzer()
fa.analyze(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/56:
fa = FactorAnalyzer()
fa(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/57:
from factor_analyzer import FactorAnalyzer
import factor_analyzer.analyze
431/58: from factor_analyzer import FactorAnalyzer
431/59:
fa = FactorAnalyzer()
fa.analyze(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/60:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/61:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 25)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/62:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
431/63:
fa = FactorAnalyzer()
fa.analyze(TOTAL_clean, 6, rotation="varimax")
431/64:
a = FactorAnalyzer()
a.analyze(TOTAL_clean, 6, rotation="varimax")
431/65:
a = FactorAnalyzer()
a.set_params(TOTAL_clean, 6, rotation="varimax")
431/66:
a = FactorAnalyzer()
a.set_params(TOTAL_clean, 6)
431/67:
fa = FactorAnalyzer()
fa.set_params(TOTAL_clean, 6)
431/68:
FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,
        method='minres', n_factors=3, rotation=None, rotation_kwargs={<!-- -->},
        use_smc=True)
431/69:
FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,
        method='minres', n_factors=3, rotation=None)
431/70:
fa=FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,
        method='minres', n_factors=3, rotation=None)
431/71: fa.loadings_
431/72: fa.fit(TOTAL_clean)
431/73:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 6, rotation="varimax")
431/74:
fa = FactorAnalyzer( rotation="varimax")
fa.fit(TOTAL_clean, 6)
431/75:
fa = FactorAnalyzer( rotation="varimax")
fa.fit(TOTAL_clean, 6)
fa.loadings_
431/76:
fa = FactorAnalyzer( rotation="varimax")
fa.fit(TOTAL_clean, 10)
fa.loadings_
431/77:
fa = FactorAnalyzer( rotation="varimax")
fa.fit(TOTAL_clean, 10)
Factors=fa.loadings_
431/78: pd.DataFrame(Factors)
431/79:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 10)
Factors=fa.loadings_
431/80: pd.DataFrame(Factors)
431/81:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 10)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/82:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
431/83:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean, 10)
Factors=fa.loadings_
431/84:
fa = FactorAnalyzer(n_factors=10)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/85: pd.DataFrame(Factors)
431/86:
fa = FactorAnalyzer(n_factors=25)
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/87:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
431/88:
fa = FactorAnalyzer(n_factors=20)
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/89:
fa = FactorAnalyzer(n_factors=25)
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/90:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
431/91:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
431/92:
fa = FactorAnalyzer(n_factors=9)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/93: pd.DataFrame(Factors)
431/94:
pd.DataFrame(Factors)
Factors.style.background_gradient(cmap='coolwarm')
431/95:
Factors_1pd.DataFrame(Factors)
Factors_1.style.background_gradient(cmap='coolwarm')
431/96:
Factors_1=pd.DataFrame(Factors)
Factors_1.style.background_gradient(cmap='coolwarm')
431/97:
fa = FactorAnalyzer(n_factors=2)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/98:
Factors_1=pd.DataFrame(Factors)
Factors_1.style.background_gradient(cmap='coolwarm')
431/99:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/100:
Factors_1=pd.DataFrame(Factors)
Factors_1.style.background_gradient(cmap='coolwarm')
431/101:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
431/102:
fa = FactorAnalyzer(n_factors=5)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/103:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
431/104:
fa = FactorAnalyzer(n_factors=3)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/105:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
431/106: fa.get_factor_variance()
431/107:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
431/108:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
431/109: fa.get_factor_variance()
431/110:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(TOTAL_DF)
# Create a PCA instance: pca
pca = PCA(n_components=25)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
436/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
436/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
436/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
436/4: obj_df["Transportation mean"].value_counts()
436/5: obj_df["Used of shared mobility"].value_counts()
436/6: obj_df["Pricing 1 or 0"].value_counts()
436/7:
cleanup_nums = {"Pricing 1 or 0":     {"Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?": 1, "Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?": 0},
                "Used of shared mobility": {"Yes": 1, "No": 0 },
                "Transportation mean": {"Public transportation": 1, "Car": 0 }}
436/8: cleanup_nums
436/9:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    else:
        df.iloc[i,0]=0
436/10: df
436/11:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    else if df.iloc[i,0]=="Car":
        df.iloc[i,0]=0
436/12:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0
436/13:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
436/14: len(df)
436/15:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
436/16: df
436/17:
# Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
# Obj_DF
436/18:
# TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
# TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

# TOTAL_DF
436/19:
print("Data shape is: ", df.shape)
Reaction_DF=df.iloc[:,0:8]
SocialMedia_DF=df.iloc[:,10:]
436/20:
df_description=df.describe()
df_description
436/21: # TOTAL_DF.describe()
436/22:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
436/23: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
436/24:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
436/25:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
436/26: columns_reaction=Reaction_DF.columns
436/27:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
TOTAL_DF['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
436/28:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df['clusters_2'] = clustering_kmeans.fit_predict(TOTAL_DF)
436/29:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df['clusters_2'] = clustering_kmeans.fit_predict(df)
436/30:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df['clusters_3'] = clustering_kmeans.fit_predict(df.iloc[:,:-1])
436/31:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df['clusters_4'] = clustering_kmeans.fit_predict(df.iloc[:,:-2])
436/32: df.to_csv("prova_replace.csv")
436/33:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df)
    sse.append(kmeans.inertia_)
436/34:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
436/35:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
436/36:
#from kneed import KneeLocator
df
436/37:
#from kneed import KneeLocator
df.iloc[:,:-2]
436/38:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("Perceptions by gender.jpeg", bbox_inches='tight')
436/39:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
436/40:
Group_Cluster_2=df.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
436/41: Group_Cluster_gender
436/42: Group_Cluster_2
436/43: Group_Cluster_2.columns
436/44: Group_Cluster_2
436/45: Group_Cluster_gender
436/46:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
436/47:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,10:-6].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/48:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,10:-3].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/49:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,9:-3].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/50:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,9:-1].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/51:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,9:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/52:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("Perceptions in 2 clusters.jpeg", bbox_inches='tight')
436/53:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
436/54:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,9:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("Social Media in 2 clusters.jpeg", bbox_inches='tight')
436/55:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,9:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
436/56:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
436/57:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,9:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
436/58:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
436/59:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,10:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
436/60:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
436/61:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
436/62:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
436/63:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,10:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
436/64:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
436/65:
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_2 = pd.DataFrame(x_scaled)
436/66: df_2
436/67:
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)
436/68: df
436/69:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
436/70:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
436/71: obj_df["Transportation mean"].value_counts()
436/72: obj_df["Used of shared mobility"].value_counts()
436/73: obj_df["Pricing 1 or 0"].value_counts()
436/74:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
436/75:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)
df
436/76:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)
df.columns=columns_names
436/77: df
436/78:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
436/79:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
436/80: obj_df["Transportation mean"].value_counts()
436/81: obj_df["Used of shared mobility"].value_counts()
436/82: obj_df["Pricing 1 or 0"].value_counts()
436/83:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
436/84:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_2 = pd.DataFrame(x_scaled)
df_2.columns=columns_names
436/85: df
436/86: columns_names
436/87: df.columns
436/88: df_2.columns
436/89: df_2
436/90:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)
df_normalized.columns=columns_names
436/91: df_normalized
436/92:
print("Data shape is: ", df_normalized.shape)
Reaction_DF=df_normalized.iloc[:,0:8]
SocialMedia_DF=df_normalized.iloc[:,10:]
436/93:
df_description=df_normalized.describe()
df_description
436/94:
Reaction_desc=df_description.iloc[:,0:8]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
436/95: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
436/96: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
436/97: #First lets take a look at the data with graphs
436/98:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
436/99:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,10:]
#SocialMedia_DF
436/100: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
436/101: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
436/102: #First lets take a look at the data with graphs
436/103:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
436/104:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
436/105:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,13:]
#SocialMedia_DF
436/106: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
436/107: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
436/108: #First lets take a look at the data with graphs
436/109:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
436/110:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
436/111:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/112:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d156")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/113:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d170")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/114:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d370")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/115:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61j370")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/116:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d370")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/117:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#61d001")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/118:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#31d001")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/119:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#31d501")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/120:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="#38d501")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/121:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="royalblue")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
436/122: columns_reaction=Reaction_DF.columns
436/123:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized)
436/124:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-1])
436/125:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-2])
436/126: df_normalized.to_csv("prova_replace.csv")
436/127:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized)
    sse.append(kmeans.inertia_)
436/128:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
436/129:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
436/130:
#from kneed import KneeLocator
df_normalized.iloc[:,:-2]
436/131:
#from kneed import KneeLocator
#df_normalized.iloc[:,:-2]
436/132:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
436/133: Group_Cluster_gender
436/134:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
436/135:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,9:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/136:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
436/137:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,4:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/138:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,13:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/139:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
436/140:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
436/141:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
436/142:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
436/143:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
436/144:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,0:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
436/145:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
436/146:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,10:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
436/147:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,12:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
436/148:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,0:8].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
436/149:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,0:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
436/150:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
436/151:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
436/152:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,9:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
436/153:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
436/154:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df_normalized)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
436/155:
TOTAL_clean=df_normalized.iloc[:,:-3]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
436/156: correlation.to_csv("correlation_normalized_data.csv")
436/157: # Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.
436/158: from factor_analyzer import FactorAnalyzer
436/159:
# Adequacy Test
# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy:

# Bartlett’s Test
# Kaiser-Meyer-Olkin Test
# Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.
436/160:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
436/161:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
436/162:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
436/163:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
436/164:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
436/165:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/166:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/167: fa.get_factor_variance()
436/168:
fa = FactorAnalyzer(n_factors=8)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/169:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/170: fa.get_factor_variance()
436/171:
fa = FactorAnalyzer(n_factors=2)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/172:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/173:
fa = FactorAnalyzer(n_factors=8)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/174:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/175:
fa = FactorAnalyzer(n_factors=7)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/176:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/177:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/178:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/179:
fa = FactorAnalyzer(n_factors=3)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/180:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/181:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/182:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/183:
fa = FactorAnalyzer(n_factors=5)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/184:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/185:
fa = FactorAnalyzer(n_factors=8)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/186:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
436/187:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
436/188:
plt.scatter(PCA_components[0], PCA_components[2], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
436/189:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
436/190:
ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
436/191:
kmeans = KMeans(n_clusters = 2, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:1]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:1]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:1]]) # Labels of each point
PCA_components.head(5)
436/192: PCA_components.plot.scatter(x = 'PCA_1', y = 'PCA_2', c=labels, s=50, cmap='viridis')
436/193: PCA_components.plot.scatter(x = '0', y = '1', c=labels, s=50, cmap='viridis')
436/194: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
436/195:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:1]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:1]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:1]]) # Labels of each point
PCA_components.head(5)
436/196: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
436/197:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
436/198: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
436/199:
fa = FactorAnalyzer(n_factors=2)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
436/200:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
447/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
447/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
447/4: obj_df["Transportation mean"].value_counts()
447/5: obj_df["Used of shared mobility"].value_counts()
447/6: obj_df["Pricing 1 or 0"].value_counts()
447/7:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
447/8:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)
df_normalized.columns=columns_names
447/9: df_normalized
447/10:
# Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
# Obj_DF
447/11:
# TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
# TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

# TOTAL_DF
447/12:
print("Data shape is: ", df_normalized.shape)
Reaction_DF=df_normalized.iloc[:,0:8]
SocialMedia_DF=df_normalized.iloc[:,10:]
447/13:
df_description=df_normalized.describe()
df_description
447/14: # TOTAL_DF.describe()
447/15:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,13:]
#SocialMedia_DF
447/16: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
447/17: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
447/18: #First lets take a look at the data with graphs
447/19:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
447/20:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
447/21:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="royalblue")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
447/22: columns_reaction=Reaction_DF.columns
447/23:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized)
447/24:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-1])
447/25:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-2])
447/26: df_normalized.to_csv("prova_replace.csv")
447/27:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
447/28:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized)
    sse.append(kmeans.inertia_)
447/29:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
447/30:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
447/31:
#from kneed import KneeLocator
#df_normalized.iloc[:,:-2]
447/32:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
447/33: Group_Cluster_gender
447/34:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
447/35:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
447/36:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
447/37:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
447/38:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
447/39:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,12:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
447/40:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
447/41:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
447/42:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
447/43:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df_normalized)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
447/44:
plt.scatter(PCA_components[0], PCA_components[], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
447/45:
plt.scatter(PCA_components[0], PCA_components[2], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
447/46:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
447/47:
plt.scatter(PCA_components[1], PCA_components[0], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
447/48:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
447/49:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
447/50: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
447/51:
ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
447/52:
TOTAL_clean=df_normalized.iloc[:,:-3]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
447/53: correlation.to_csv("correlation_normalized_data.csv")
447/54: # Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.
447/55: from factor_analyzer import FactorAnalyzer
447/56:
# Adequacy Test
# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy:

# Bartlett’s Test
# Kaiser-Meyer-Olkin Test
# Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.
447/57:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
447/58: # In this Bartlett ’s test, the p-value is close to 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
447/59:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
447/60:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
447/61:
# Choosing the Number of Factors
# For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
447/62:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
447/63:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
447/64: # Choose only factors greater than 1, so here 8
447/65:
fa = FactorAnalyzer(n_factors=2)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/66:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/67:
fa = FactorAnalyzer(n_factors=4)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/68:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/69:
fa = FactorAnalyzer(n_factors=8)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/70:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/71:
fa = FactorAnalyzer(n_factors=6)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/72:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/73:
fa = FactorAnalyzer(n_factors=7)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/74:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/75: Factors_1.order
447/76: Factors_1.sort_values
447/77: Factors_1.sort_values()
447/78: Factors_1.sort_values(by=['0'])
447/79: Factors_1.sort_values(by=[0])
447/80: Factors_1.sort_values(by=[0], ascending=False)
447/81: Factors_1.sort_values(by=[0,1], ascending=False)
447/82: Factors_1.sort_values(by=[0,1,2,3,4,5,6,7], ascending=False)
447/83: Factors_1.sort_values(by=[0,1,2,3,4,5,6], ascending=False)
447/84:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5,6], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
447/85: fa.get_factor_variance()
447/86: Factors_1_ordered.to_csv("Factor_analysis.csv")
447/87: principalComponents
447/88: X_std
447/89: df_normalized
447/90: df_NEW=df_normalized
447/91: df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
447/92: df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
447/93:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
447/94: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW["clear rules"])/2
447/95: df_NEW["safe and clear"]=(df_NEW["Safe"]+df_NEW["clear rules"])/2
447/96: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW["clear rules"])/2
447/97: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW["clear rules"])/2
447/98: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW["clear rules "])/2
447/99: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules"])/2
447/100: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW["clear rules"])/2
447/101: df_NEW["clear rules"]
447/102: df_NEW["clear rules"]
447/103: df_NEW.columns
447/104: df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
447/105:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
447/106: df_NEW=df_normalized
447/107:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["Social networks (FB-IG)"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
447/108: df_NEW.columns
447/109:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
447/110: df_NEW.columns
447/111: df_NEW=pd.DataFrame()
447/112: df_NEW=df_normalized
447/113:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
447/114:
df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
       'Fun', 'positive impact on urban transportation',
       'positive impact on the environment','Facebook', 'Instagram',
       'Twitter', 'Snapchat', 'YouTube', 'Printed newspaper / magazine',
       'Online newspaper / magazine', 'Traditional radio (e.g. in car)',
       'On-demand radio', 'Traditional TV', 'On-demand TV'], axis=1)
447/115: df_NEW
447/116: df_NEW.columns
447/117: #df_NEW.columns
447/118:
#CLUSTERS=4
Group_Cluster_4_NEW=df_NEW.iloc[:,:].groupby(["clusters_4"]).mean()
447/119:
#CLUSTERS=4
Group_Cluster_4_NEW=df_NEW.iloc[:,:].groupby(["clusters_4"]).mean()
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 Clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/120:
#CLUSTERS=4
Group_Cluster_4_NEW=df_NEW.iloc[:,:].groupby(["clusters_4"]).mean()
plt.style.use('seaborn')
plt2=Group_Cluster_4_NEW.iloc[:,:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 Clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/121: df_NEW.columns
447/122:
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3'], axis=1)
447/123:
#CLUSTERS=4
Group_Cluster_4_NEW=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
plt.style.use('seaborn')
plt2=Group_Cluster_4_NEW.iloc[:,:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 Clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/124:
#CLUSTERS=4
Group_Cluster_4_NEW=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
plt.style.use('seaborn')
plt2=Group_Cluster_4_NEW.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 Clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/125:
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4' ], axis=1)
447/126:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4.iloc[:,:])
447/127: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
447/128:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/129:
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4' ], axis=1)
447/130:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
447/131: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
447/132:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/133: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
447/134:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/135: df_NEW_4.columns
447/136:
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
       'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
447/137: df_NEW_4.columns
447/138:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
447/139: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
447/140:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/141: Group_Cluster_4
447/142:
from mlxtend.preprocessing import standardize
standardize(Group_Cluster_4)
447/143:
from mlxtend.preprocessing import standardize
standardize(Group_Cluster_4)
447/144:
plt.style.use('seaborn')
Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/145:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/146: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
447/147:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/148:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/149:
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
       'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
447/150: df_NEW_4.columns
447/151:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
447/152: df_NEW_4.columns
447/153:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
447/154: Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
447/155:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/156:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/157:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart.jpeg", bbox_inches='tight')
447/158:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart standardized.jpeg", bbox_inches='tight')
447/159:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
447/160: df_NEW_4.columns
447/161:
#CLUSTERS=2
clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_2'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_2"]).mean()
447/162:
#CLUSTERS=2
clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_2'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_2"]).mean()
447/163:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("2 clusters", fontsize=18)
plt.savefig("images/Factorized chart 2 clusters.jpeg", bbox_inches='tight')
447/164:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters", fontsize=18)
plt.savefig("images/Factorized chart standardized.jpeg", bbox_inches='tight')
447/165:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
447/166:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
447/167:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
447/168: df_NEW_4.columns
447/169:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
447/170:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
447/171:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
447/172:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
447/173:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
447/174:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
447/175:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')
447/176:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters STD.jpeg", bbox_inches='tight')
447/177:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters STD.jpeg", bbox_inches='tight')
447/178:
fa = FactorAnalyzer(n_factors=6)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
447/179:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
447/180:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5,6], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
447/181:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
447/182:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse,  '-o')
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
447/183:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
447/184: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
447/185: pd.DataFrame(X_std)
447/186:
dd=pd.DataFrame(X_std)
dd.head(5)
447/187:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
447/188: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
450/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
450/2: path_file, base=search_for_file_path ()
450/3:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
450/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
450/5:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
450/6:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'tourist_attraction')
450/7: len(places_result)
450/8:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
450/9: Data_PD
457/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from mlxtend.preprocessing import standardize
457/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
457/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
457/4: obj_df["Transportation mean"].value_counts()
457/5: obj_df["Used of shared mobility"].value_counts()
457/6: obj_df["Pricing 1 or 0"].value_counts()
457/7:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
457/8:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)
df_normalized.columns=columns_names
457/9: df_normalized
457/10:
# Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
# Obj_DF
457/11:
# TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
# TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

# TOTAL_DF
457/12:
print("Data shape is: ", df_normalized.shape)
Reaction_DF=df_normalized.iloc[:,0:8]
SocialMedia_DF=df_normalized.iloc[:,10:]
457/13:
df_description=df_normalized.describe()
df_description
457/14: # TOTAL_DF.describe()
457/15:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,13:]
#SocialMedia_DF
457/16: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
457/17: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
457/18: #First lets take a look at the data with graphs
457/19:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
457/20:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
457/21:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="royalblue")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
457/22: columns_reaction=Reaction_DF.columns
457/23:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized)
457/24:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-1])
457/25:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-2])
457/26: df_normalized.to_csv("prova_replace.csv")
457/27:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
457/28:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized)
    sse.append(kmeans.inertia_)
457/29:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse,  '-o')
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
457/30:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
457/31:
#from kneed import KneeLocator
#df_normalized.iloc[:,:-2]
457/32:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
457/33: Group_Cluster_gender
457/34:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
457/35:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
457/36:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
457/37:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
457/38:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/39:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,12:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
457/40:
plt.style.use('seaborn')
plt3=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/41:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/42:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
457/43:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df_normalized)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
457/44:
dd=pd.DataFrame(X_std)
dd.head(5)
457/45:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
457/46:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
457/47: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
457/48:
ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
457/49:
TOTAL_clean=df_normalized.iloc[:,:-3]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
457/50: correlation.to_csv("correlation_normalized_data.csv")
457/51: # Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.
457/52: from factor_analyzer import FactorAnalyzer
457/53:
# Adequacy Test
# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy:

# Bartlett’s Test
# Kaiser-Meyer-Olkin Test
# Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.
457/54:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
457/55: # In this Bartlett ’s test, the p-value is close to 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
457/56:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
457/57:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
457/58:
# Choosing the Number of Factors
# For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
457/59:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
457/60:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
457/61: # Choose only factors greater than 1, so here 8
457/62:
fa = FactorAnalyzer(n_factors=6)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
457/63:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
457/64:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
457/65: # Factors_1_ordered.to_csv("Factor_analysis.csv")
457/66: fa.get_factor_variance()
457/67:
# Total 38% cumulative Variance explained by the 8 factors.

# Pros and Cons of Factor Analysis
# Factor analysis explores large dataset and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, It helps in improve questionnaire in for future surveys. Factors make for more natural data interpretation.

# Results of factor analysis are controversial. Its interpretations can be debatable because more than one interpretation can be made of the same data factors. After factor identification and naming of factors requires domain knowledge.
457/68: df_normalized
457/69: df_NEW=df_normalized
457/70:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
457/71:
df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
       'Fun', 'positive impact on urban transportation',
       'positive impact on the environment','Facebook', 'Instagram',
       'Twitter', 'Snapchat', 'YouTube', 'Printed newspaper / magazine',
       'Online newspaper / magazine', 'Traditional radio (e.g. in car)',
       'On-demand radio', 'Traditional TV', 'On-demand TV'], axis=1)
457/72: df_NEW.columns
457/73:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
457/74: df_NEW_4.columns
457/75:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
457/76:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
457/77:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
457/78:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
457/79:
#CLUSTERS=3
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
457/80:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')
457/81:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters STD.jpeg", bbox_inches='tight')
457/82:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
#plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
457/83:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
#plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
457/84:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
457/85:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')
457/86:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/87:
Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/88:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,12:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
457/89:
Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/90:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/91:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4_ST.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/92:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4_ST.iloc[:,2:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/93:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4_ST.iloc[:,2:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/94:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4_ST.iloc[:,2:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/95:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt3=Group_Cluster_4_ST.iloc[:,2:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/96:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt1=Group_Cluster_4_ST.iloc[:,2:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/97:
plt.style.use('seaborn')
Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt1=Group_Cluster_4_ST.iloc[:,:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
457/98:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/99:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
457/100:
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/101:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/102:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')

plt3=standardize(Group_Cluster_3.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right Standardized')
plt3.set_title("Perceptions of 3 clusters", fontsize=18)
457/103:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
plt3=standardize(Group_Cluster_2.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 2 clusters", fontsize=18)
457/104:
Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/105:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
457/106:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/107:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
457/108:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,12:-2].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/109:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-2])
457/110:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
457/111:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/112:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-4].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/113:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/114:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/115:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/116:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt3=standardize(Group_Cluster_4.iloc[:,:-8]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 4 clusters Std", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
457/117:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/118:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/119:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')

plt3=standardize(Group_Cluster_3.iloc[:,:-7]).T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right Standardized')
plt3.set_title("Perceptions of 3 clusters", fontsize=18)
457/120:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:12].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/121:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:13].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
457/122:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
plt3=standardize(Group_Cluster_2.iloc[:,:-6]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 2 clusters", fontsize=18)
457/123:
fa = FactorAnalyzer(n_factors=7)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
457/124:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
457/125:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
457/126:
#CLUSTERS=3
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
457/127:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4_Std.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
461/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from mlxtend.preprocessing import standardize
461/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
461/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
461/4: obj_df["Transportation mean"].value_counts()
461/5: obj_df["Used of shared mobility"].value_counts()
461/6: obj_df["Pricing 1 or 0"].value_counts()
461/7:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
461/8:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)
df_normalized.columns=columns_names
461/9: df_normalized
461/10:
# Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
# Obj_DF
461/11:
# TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
# TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

# TOTAL_DF
461/12:
print("Data shape is: ", df_normalized.shape)
Reaction_DF=df_normalized.iloc[:,0:8]
SocialMedia_DF=df_normalized.iloc[:,10:]
461/13:
df_description=df_normalized.describe()
df_description
461/14: # TOTAL_DF.describe()
461/15:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,13:]
#SocialMedia_DF
461/16: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
461/17: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
461/18: #First lets take a look at the data with graphs
461/19:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
461/20:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
461/21:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="royalblue")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
461/22: columns_reaction=Reaction_DF.columns
461/23:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized)
461/24:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-1])
461/25:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,:-2])
461/26: df_normalized.to_csv("prova_replace.csv")
461/27:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
461/28:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized)
    sse.append(kmeans.inertia_)
461/29:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse,  '-o')
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
461/30:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
461/31:
#from kneed import KneeLocator
#df_normalized.iloc[:,:-2]
461/32:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
461/33: Group_Cluster_gender
461/34:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
#plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
461/35:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,12:].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
#plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
461/36:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
#plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
461/37:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
plt3=standardize(Group_Cluster_2.iloc[:,:-6]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 2 clusters", fontsize=18)
461/38:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,:-7].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')

plt3=standardize(Group_Cluster_3.iloc[:,:-7]).T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right Standardized')
plt3.set_title("Perceptions of 3 clusters", fontsize=18)
461/39:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,:13].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
461/40:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,12:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
#plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
461/41:
plt.style.use('seaborn')
#Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt1=Group_Cluster_4.iloc[:,:-1].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
461/42:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt3=standardize(Group_Cluster_4.iloc[:,:-8]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 4 clusters Std", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
461/43:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
461/44:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df_normalized)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
461/45:
dd=pd.DataFrame(X_std)
dd.head(5)
461/46:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
461/47:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
461/48: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
461/49:
ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
461/50:
TOTAL_clean=df_normalized.iloc[:,:-3]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
461/51: correlation.to_csv("correlation_normalized_data.csv")
461/52: # Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.
461/53: from factor_analyzer import FactorAnalyzer
461/54:
# Adequacy Test
# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy:

# Bartlett’s Test
# Kaiser-Meyer-Olkin Test
# Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.
461/55:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
461/56: # In this Bartlett ’s test, the p-value is close to 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
461/57:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
461/58:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
461/59:
# Choosing the Number of Factors
# For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
461/60:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
461/61:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
461/62: # Choose only factors greater than 1, so here 8
461/63:
fa = FactorAnalyzer(n_factors=7)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
461/64:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
461/65:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
461/66: # Factors_1_ordered.to_csv("Factor_analysis.csv")
461/67: fa.get_factor_variance()
461/68:
# Total 38% cumulative Variance explained by the 8 factors.

# Pros and Cons of Factor Analysis
# Factor analysis explores large dataset and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, It helps in improve questionnaire in for future surveys. Factors make for more natural data interpretation.

# Results of factor analysis are controversial. Its interpretations can be debatable because more than one interpretation can be made of the same data factors. After factor identification and naming of factors requires domain knowledge.
461/69: df_normalized
461/70: df_NEW=df_normalized
461/71:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
461/72:
df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
       'Fun', 'positive impact on urban transportation',
       'positive impact on the environment','Facebook', 'Instagram',
       'Twitter', 'Snapchat', 'YouTube', 'Printed newspaper / magazine',
       'Online newspaper / magazine', 'Traditional radio (e.g. in car)',
       'On-demand radio', 'Traditional TV', 'On-demand TV'], axis=1)
461/73: df_NEW.columns
461/74:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
461/75: df_NEW_4.columns
461/76:
#CLUSTERS=4
clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
461/77:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
461/78:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
461/79:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2',
       'clusters_3', 'clusters_4'], axis=1)
461/80:
#CLUSTERS=3
clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
461/81:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4_Std.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
461/82:
plt.style.use('seaborn')
Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=Group_Cluster_4_Std.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
plt.savefig("images/Factorized chart 3 clusters STD.jpeg", bbox_inches='tight')
461/83:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
461/84:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
461/85:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
461/86: df_normalized.to_csv("prova_replace.csv")
461/87:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
461/88:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized.iloc[:,2:10])
    sse.append(kmeans.inertia_)
461/89:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse,  '-o')
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
461/90:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
461/91:
Group_Cluster_2=df_normalized.iloc[:,2:10].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,2:10].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,2:10].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,2:10].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
461/92:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
461/93: Group_Cluster_gender
461/94:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
#plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
461/95:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
#plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
461/96:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
#plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
461/97:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
plt3=standardize(Group_Cluster_2.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 2 clusters", fontsize=18)
461/98:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')

plt3=standardize(Group_Cluster_3.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right Standardized')
plt3.set_title("Perceptions of 3 clusters", fontsize=18)
461/99:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
461/100:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,12:20].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
461/101:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,12:22].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
461/102:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,12:23].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
461/103:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,2:12].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
#plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
461/104:
plt.style.use('seaborn')
#Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt1=Group_Cluster_4.iloc[:,2:12].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
461/105:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt3=standardize(Group_Cluster_4.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 4 clusters Std", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
461/106:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-3].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
461/107:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-6].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
461/108:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
461/109:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt.style.use('seaborn')
plt2=standardize(Group_Cluster_4.iloc[:,:-8]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
461/110:
TOTAL_clean=df_normalized.iloc[:,2:10]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
461/111: correlation.to_csv("correlation_normalized_data_perception.csv")
461/112:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
461/113: # In this Bartlett ’s test, the p-value is close to 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
461/114:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
461/115:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
461/116:
# Choosing the Number of Factors
# For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
461/117:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
461/118:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
461/119:
fa = FactorAnalyzer(n_factors=3)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
461/120:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
461/121:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2,3,4,5], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
461/122:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
461/123: fa.get_factor_variance()
461/124: df_normalized
461/125:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
# df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
# df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
# df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
461/126:

df_normalized=df_normalized.drop['Enthusiasm',
       'safe and clear', 'Sustainable', axis=1]
df_NEW=df_normalized
461/127:

df_normalized=df_normalized.drop['Enthusiasm',
       'safe and clear', 'Sustainable'], axis=1)
df_NEW=df_normalized
461/128:

df_normalized=df_normalized.drop['Enthusiasm', 'safe and clear', 'Sustainable'], axis=1)
df_NEW=df_normalized
461/129:

df_normalized=df_normalized.drop(['Enthusiasm', 'safe and clear', 'Sustainable'], axis=1)
df_NEW=df_normalized
461/130:

#df_normalized=df_normalized.drop(['Enthusiasm', 'safe and clear', 'Sustainable'], axis=1)
df_normalized=df_normalized.drop(['Newspapers/magazines', 'Traditional Media', 'Social networks (FB-IG)'], axis=1)
df_NEW=df_normalized
461/131:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
# df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
# df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
# df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
461/132:
# df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
#        'Fun', 'positive impact on urban transportation',
#        'positive impact on the environment','Facebook', 'Instagram',
#        'Twitter', 'Snapchat', 'YouTube', 'Printed newspaper / magazine',
#        'Online newspaper / magazine', 'Traditional radio (e.g. in car)',
#        'On-demand radio', 'Traditional TV', 'On-demand TV'], axis=1)

df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
       'Fun', 'positive impact on urban transportation',
       'positive impact on the environment'], axis=1)
461/133: df_NEW.columns
461/134:
#CLUSTERS=4
# clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_4"]).mean()
461/135:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
461/136:
#CLUSTERS=4
# clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW.iloc[:,:].groupby(["clusters_4"]).mean()
461/137:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
461/138:
plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4_Std.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
461/139:
plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
461/140:
#CLUSTERS=3
# clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW.iloc[:,:].groupby(["clusters_3"]).mean()
461/141:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
461/142:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2', 'clusters_4'], axis=1)
461/143:
#CLUSTERS=3
# clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW.iloc[:,:].groupby(["clusters_3"]).mean()
461/144:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
461/145:
#CLUSTERS=3
# clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
461/146:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
461/147:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, color = "green")
plt.title("simple 3D scatter plot")
 
# show plot
plt.show()
461/148:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, color = cluster)
plt.title("simple 3D scatter plot")
cbar.set_label("cluster")
# show plot
plt.show()
461/149:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster)
plt.title("simple 3D scatter plot")
cbar.set_label("cluster")
# show plot
plt.show()
461/150:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster)
plt.title("simple 3D scatter plot")
#cbar.set_label("cluster")
# show plot
plt.show()
461/151:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster)
plt.title("simple 3D scatter plot")
cbar.set("cluster")
# show plot
plt.show()
461/152:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster)
plt.title("simple 3D scatter plot")
fig.set("cluster")
# show plot
plt.show()
461/153:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster)
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
461/154: df_NEW.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
461/155: df_NEW.plot.scatter(x = 15, y = 16, c=labels, s=50, cmap='viridis')
461/156: df_NEW.plot.scatter(x = 16, y = 17, c=labels, s=50, cmap='viridis')
461/157: df_NEW.plot.scatter(x = 17, y = 18, c=labels, s=50, cmap='viridis')
461/158: df_NEW.plot.scatter(x = 18, y = 19, c=labels, s=50, cmap='viridis')
461/159: df_NEW.plot.scatter(x = 18, y = 20, c=labels, s=50, cmap='viridis')
461/160: df_NEW
461/161: df_NEW.plot.scatter(x = 18, y = 19, c=labels, s=50, cmap='viridis')
461/162: df_NEW.plot.scatter(x = 19, y = 20, c=labels, s=50, cmap='viridis')
461/163:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
461/164:
import plotly.plotly as py
import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/alpha_shape.csv')
df.head()

scatter = dict(
    mode = "markers",
    name = "y",
    type = "scatter3d",
    x = df['x'], y = df['y'], z = df['z'],
    marker = dict( size=2, color="rgb(23, 190, 207)" )
)
clusters = dict(
    alphahull = 7,
    name = "y",
    opacity = 0.1,
    type = "mesh3d",
    x = df['x'], y = df['y'], z = df['z']
)
layout = dict(
    title = '3d point clustering',
    scene = dict(
        xaxis = dict( zeroline=False ),
        yaxis = dict( zeroline=False ),
        zaxis = dict( zeroline=False ),
    )
)
fig = dict( data=[scatter, clusters], layout=layout )
# Use py.iplot() for IPython notebook
py.iplot(fig, filename='3d point clustering')
461/165:
import chart_studio.plotly as py
import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/alpha_shape.csv')
df.head()

scatter = dict(
    mode = "markers",
    name = "y",
    type = "scatter3d",
    x = df['x'], y = df['y'], z = df['z'],
    marker = dict( size=2, color="rgb(23, 190, 207)" )
)
clusters = dict(
    alphahull = 7,
    name = "y",
    opacity = 0.1,
    type = "mesh3d",
    x = df['x'], y = df['y'], z = df['z']
)
layout = dict(
    title = '3d point clustering',
    scene = dict(
        xaxis = dict( zeroline=False ),
        yaxis = dict( zeroline=False ),
        zaxis = dict( zeroline=False ),
    )
)
fig = dict( data=[scatter, clusters], layout=layout )
# Use py.iplot() for IPython notebook
py.iplot(fig, filename='3d point clustering')
466/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from kneed import KneeLocator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from mlxtend.preprocessing import standardize
466/2:
df=pd.read_csv("Monopattino_corrected.csv")
df.shape
df
#pd.DataFrame(df[:].values)
466/3:
print(df.dtypes)
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()
466/4: obj_df["Transportation mean"].value_counts()
466/5: obj_df["Used of shared mobility"].value_counts()
466/6: obj_df["Pricing 1 or 0"].value_counts()
466/7:
for i in range(0,len(df)):
    if df.iloc[i,0]=="Public transportation":
        df.iloc[i,0]=1
    elif df.iloc[i,0]=="Car":
        df.iloc[i,0]=0

    if df.iloc[i,1]=="Yes":
        df.iloc[i,1]=1
    elif df.iloc[i,1]=="No":
        df.iloc[i,1]=0

    if df.iloc[i,10]=="Consider that the service costs &euro;0,15 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=1
    elif df.iloc[i,10]=="Consider that the service requires a monthly subscription of &euro;8, and costs&euro;0,05 per minute of use. How likely are you going to try the service?":
        df.iloc[i,10]=0
466/8:
columns_names=df.columns
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)
df_normalized.columns=columns_names
466/9: df_normalized
466/10:
# Obj_DF=pd.get_dummies(obj_df, columns=["Transportation mean", "Used of shared mobility", "Pricing 1 or 0"])
# Obj_DF
466/11:
# TOTAL_DF=pd.concat([df, Obj_DF], axis=1)
# TOTAL_DF=TOTAL_DF.select_dtypes(exclude=['object'])

# TOTAL_DF
466/12:
print("Data shape is: ", df_normalized.shape)
Reaction_DF=df_normalized.iloc[:,0:8]
SocialMedia_DF=df_normalized.iloc[:,10:]
466/13:
df_description=df_normalized.describe()
df_description
466/14: # TOTAL_DF.describe()
466/15:
Reaction_desc=df_description.iloc[:,2:10]
SocialMedia_desc=df_description.iloc[:,13:]
#SocialMedia_DF
466/16: print("Data about reaction is: ", Reaction_DF.shape, "; Data about Social media is: ", SocialMedia_DF.shape)
466/17: #We need to transform fields of "Transportation, used of shared mobility, question A/B and gender in dummy variables (1 -0"
466/18: #First lets take a look at the data with graphs
466/19:
Reaction_mean=Reaction_desc.iloc[1,:]
SocialMedia_mean=SocialMedia_desc.iloc[1,:]
g1=Reaction_mean.plot(kind="bar", figsize=(15,7), color="#61d199")
plt.savefig("images/Perceptions_mean.jpeg", bbox_inches='tight')
466/20:
plt.style.use("seaborn")
g2=SocialMedia_mean.plot(kind="bar", figsize=(15,7), color="#61d150")
plt.savefig("images/SocialMedia_mean.jpeg", bbox_inches='tight')
466/21:
plt.style.use("seaborn")
g2=df_description.iloc[1,:].plot(kind="bar", figsize=(15,7), color="royalblue")
plt.savefig("images/Todo_mean.jpeg", bbox_inches='tight')
466/22: columns_reaction=Reaction_DF.columns
466/23:
#CLUSTERS=2

clustering_kmeans = KMeans(n_clusters=2, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_2'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
466/24:
#CLUSTERS=3

clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_3'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
466/25:
#CLUSTERS=4

clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
df_normalized['clusters_4'] = clustering_kmeans.fit_predict(df_normalized.iloc[:,2:10])
466/26: df_normalized.to_csv("prova_replace.csv")
466/27:
# Choosing the Appropriate Number of Clusters
# In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters:

# The elbow method
# The silhouette coefficient
# These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:
466/28:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized.iloc:,2:10)
    sse.append(kmeans.inertia_)
466/29:
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"max_iter": 1000,
 "random_state": 42,
}

# A list holds the SSE values for each k
sse = []
for k in range(1, 15):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(df_normalized.iloc[:,2:10])
    sse.append(kmeans.inertia_)
466/30:
plt.style.use("seaborn")
plt.plot(range(1, 15), sse,  '-o')
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()
466/31:
kl = KneeLocator(
range(1, 15), sse, curve="convex", direction="decreasing")
kl.elbow
466/32:
Group_Cluster_2=df_normalized.iloc[:,:-2].groupby(["clusters_2"]).mean()
# Data_mod_c3=df.iloc[:,:-1]
Group_Cluster_gender=df_normalized.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
Group_Cluster_3=df_normalized.iloc[:,:-1].groupby(["clusters_3"]).mean()
Group_Cluster_4=df_normalized.iloc[:,:].groupby(["clusters_4"]).mean()

#Group_Cluster_gender=TOTAL_DF.iloc[:,:-3].groupby(["Gender (1-male; 2-female)"]).mean()
466/33: Group_Cluster_gender
466/34:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Perceptions by gender", fontsize=18)
#plt.savefig("images/Perceptions by gender.jpeg", bbox_inches='tight')
466/35:
plt.style.use('seaborn')
plt1=Group_Cluster_gender.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, color=['royalblue', 'lightcoral'], alpha=0.85)
plt1.set_title("Social Media by gender", fontsize=18)
#plt.savefig("images/Social Media by gender.jpeg", bbox_inches='tight')
466/36:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=12, alpha=0.85)
plt2.set_title("Perceptions in 2 clusters", fontsize=18)
plt.legend(loc = 'outside upper right')
#plt.savefig("images/Perceptions in 2 clusters.jpeg", bbox_inches='tight')
466/37:
plt.style.use('seaborn')
plt2=Group_Cluster_2.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 2 clusters", fontsize=18)
#plt.savefig("images/Social Media in 2 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
plt3=standardize(Group_Cluster_2.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 2 clusters", fontsize=18)
466/38:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')

plt3=standardize(Group_Cluster_3.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right Standardized')
plt3.set_title("Perceptions of 3 clusters", fontsize=18)
466/39:
#Group_Cluster_3=standardize(Group_Cluster_3)
plt.style.use('seaborn')
plt3=Group_Cluster_3.iloc[:,12:23].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt2.set_title("Perceptions of 3 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 3 clusters.jpeg", bbox_inches='tight')
466/40:
plt.style.use('seaborn')
plt2=Group_Cluster_3.iloc[:,2:12].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 3 clusters", fontsize=18)
#plt.savefig("images/Social Media in 3 clusters.jpeg", bbox_inches='tight')
466/41:
plt.style.use('seaborn')
#Group_Cluster_4_ST=standardize(Group_Cluster_4)
plt1=Group_Cluster_4.iloc[:,2:12].T.plot(kind='bar', figsize=(15,7), fontsize=13, alpha=0.9)
plt.legend(loc = 'outside upper right')
plt1.set_title("Perceptions of 4 clusters", fontsize=18)
#plt.savefig("images/Perceptions in 4 clusters.jpeg", bbox_inches='tight')
466/42:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,2:10].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt3=standardize(Group_Cluster_4.iloc[:,2:10]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt3.set_title("Social Media of 4 clusters Std", fontsize=18)
#plt.savefig("images/Social Media in 4 clusters.jpeg", bbox_inches='tight')
466/43:
plt.style.use('seaborn')
plt2=Group_Cluster_4.iloc[:,:-8].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)

plt.style.use('seaborn')
plt2=standardize(Group_Cluster_4.iloc[:,:-8]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("Social Media of 4 clusters", fontsize=18)
466/44:
# import seaborn as sns
# sns.set(style="whitegrid")
# tips = sns.load_dataset("tips")
# ax = sns.barplot(x="day", y="total_bill", data=tips)
466/45:
# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df_normalized)
# Create a PCA instance: pca
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(X_std)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='blue')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
466/46:
dd=pd.DataFrame(X_std)
dd.head(5)
466/47:
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
466/48:
kmeans = KMeans(n_clusters = 4, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
466/49: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
466/50:
kmeans = KMeans(n_clusters = 3, init ='k-means++')
kmeans.fit(PCA_components[PCA_components.columns[0:2]]) # Compute k-means clustering.
PCA_components['cluster_label'] = kmeans.fit_predict(PCA_components[PCA_components.columns[0:2]])
centers = kmeans.cluster_centers_ # Coordinates of cluster centers.
labels = kmeans.predict(PCA_components[PCA_components.columns[0:2]]) # Labels of each point
PCA_components.head(5)
466/51: PCA_components.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
466/52:
ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()
466/53:
TOTAL_clean=df_normalized.iloc[:,2:10]
correlation = TOTAL_clean.corr()
correlation.style.background_gradient(cmap='coolwarm')
#img=TOTAL_clean.corr()
#plt.savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
#savefig('correlation.png', bbox_inches='tight', pad_inches=0.0)
466/54: correlation.to_csv("correlation_normalized_data_perception.csv")
466/55: # Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.
466/56: from factor_analyzer import FactorAnalyzer
466/57:
# Adequacy Test
# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy:

# Bartlett’s Test
# Kaiser-Meyer-Olkin Test
# Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.
466/58:
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(TOTAL_clean)
chi_square_value, p_value
466/59: # In this Bartlett ’s test, the p-value is close to 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
466/60:
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(TOTAL_clean)
466/61:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate.


kmo_model
466/62:
# Choosing the Number of Factors
# For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
466/63:
fa = FactorAnalyzer()
fa.fit(TOTAL_clean)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
466/64:
plt.scatter(range(1,TOTAL_clean.shape[1]+1),ev)
plt.plot(range(1,TOTAL_clean.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
466/65: # Choose only factors greater than 1, so here 8
466/66:
fa = FactorAnalyzer(n_factors=3)
fa.fit(TOTAL_clean)
Factors=fa.loadings_
466/67:
Factors_1=pd.DataFrame(Factors)
Factors_1["Features"]=TOTAL_clean.columns
Factors_1.style.background_gradient(cmap='coolwarm')
466/68:
Factors_1_ordered=Factors_1.sort_values(by=[0,1,2], ascending=False)
Factors_1_ordered.style.background_gradient(cmap='coolwarm')
466/69: # Factors_1_ordered.to_csv("Factor_analysis.csv")
466/70: fa.get_factor_variance()
466/71:
# Total 60% cumulative Variance explained by the 8 factors.

# Pros and Cons of Factor Analysis
# Factor analysis explores large dataset and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, It helps in improve questionnaire in for future surveys. Factors make for more natural data interpretation.

# Results of factor analysis are controversial. Its interpretations can be debatable because more than one interpretation can be made of the same data factors. After factor identification and naming of factors requires domain knowledge.
466/72: df_normalized.
466/73: df_normalized
466/74:

#df_normalized=df_normalized.drop(['Enthusiasm', 'safe and clear', 'Sustainable'], axis=1)
df_normalized=df_normalized.drop(['Newspapers/magazines', 'Traditional Media', 'Social networks (FB-IG)'], axis=1)
df_NEW=df_normalized
466/75:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
# df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
# df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
# df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
466/76:

#df_normalized=df_normalized.drop(['Enthusiasm', 'safe and clear', 'Sustainable'], axis=1)
#df_normalized=df_normalized.drop(['Newspapers/magazines', 'Traditional Media', 'Social networks (FB-IG)'], axis=1)
df_NEW=df_normalized
466/77:
df_NEW["Enthusiasm"]=(df_NEW["practical"]+df_NEW["Fun"]+df_NEW["easy"]+df_NEW["interesting solution"])/4
df_NEW["safe and clear"]=(df_NEW["safe"]+df_NEW[" clear rules "])/2
df_NEW["Sustainable"]=(df_NEW["positive impact on urban transportation"]+df_NEW["positive impact on the environment"])/2
# df_NEW["Newspapers/magazines"]=(df_NEW["Printed newspaper / magazine"]+df_NEW["Online newspaper / magazine"])/2
# df_NEW["Traditional Media"]=(df_NEW["Traditional TV"]+df_NEW["Traditional radio (e.g. in car)"])/2
# df_NEW["Social networks (FB-IG)"]=(df_NEW["Facebook"]+df_NEW["Instagram"])/2
466/78:
# df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
#        'Fun', 'positive impact on urban transportation',
#        'positive impact on the environment','Facebook', 'Instagram',
#        'Twitter', 'Snapchat', 'YouTube', 'Printed newspaper / magazine',
#        'Online newspaper / magazine', 'Traditional radio (e.g. in car)',
#        'On-demand radio', 'Traditional TV', 'On-demand TV'], axis=1)

df_NEW=df_NEW.drop(['interesting solution', 'easy', 'safe', 'practical', ' clear rules ',
       'Fun', 'positive impact on urban transportation',
       'positive impact on the environment'], axis=1)
466/79: df_NEW.columns
466/80: df_NEW
466/81: df_NEW.to_csv("cluster_data")
466/82:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2', 'clusters_4'], axis=1)
466/83: df_NEW_4.columns
466/84: df_NEW_4.to_csv("cluster_data_3")
466/85: df_NEW.to_csv("cluster_data.csv")
466/86:
#CLUSTERS=4
# clustering_kmeans = KMeans(n_clusters=4, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_4'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW.iloc[:,:].groupby(["clusters_4"]).mean()
466/87:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters.jpeg", bbox_inches='tight')
466/88:
plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("4 clusters factorized - standardized", fontsize=18)
#plt.savefig("images/Factorized chart 4 clusters STD.jpeg", bbox_inches='tight')
466/89:
# df_NEW_4=df_NEW.drop(['clusters_2',
#        'clusters_3', 'clusters_4' ,'Transportation mean', 'Used of shared mobility', 'Pricing 1 or 0',
#        'Response 1 or 0', 'Gender (1-male; 2-female)'], axis=1)
df_NEW_4=df_NEW.drop(['clusters_2', 'clusters_4'], axis=1)
466/90:
#CLUSTERS=3
# clustering_kmeans = KMeans(n_clusters=3, precompute_distances="auto", n_jobs=-1)
# df_NEW_4['clusters_3'] = clustering_kmeans.fit_predict(df_NEW_4)
Group_Cluster_4=df_NEW_4.iloc[:,:].groupby(["clusters_3"]).mean()
466/91: Group_Cluster_4
466/92: Group_Cluster_4.to_csv("summary_cluster_3.csv")
466/93:
plt.style.use('seaborn')
#Group_Cluster_4=standardize(Group_Cluster_4)
plt2=Group_Cluster_4.iloc[:,:].T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized", fontsize=18)
#plt.savefig("images/Factorized chart 3 clusters.jpeg", bbox_inches='tight')

plt.style.use('seaborn')
#Group_Cluster_4_Std=standardize(Group_Cluster_4)
plt2=standardize(Group_Cluster_4.iloc[:,:]).T.plot(kind='bar', figsize=(15,7), fontsize=13, color=['green', 'coral','blue', 'orange'], alpha=0.5)
plt.legend(loc = 'outside upper right')
plt2.set_title("3 clusters factorized - standardized", fontsize=18)
466/94:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/95:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o', c=self.cluster[i].color, label="Cluster " + str(i + 1))
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/96:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o', c=self.cluster.color, label="Cluster " + str(i + 1))
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/97:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o', c=self.cluster, label="Cluster " + str(i + 1))
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/98:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o', c=cluster, label="Cluster " + str(i + 1))
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/99:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o', c=cluster, label="Cluster ")
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/100:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
ax.plot([], [], 'o')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/101:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/102:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="2d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/103:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
# show plot
plt.show()
466/104:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
ax.set_xlabel('X-axis', fontweight ='bold') 
ax.set_ylabel('Y-axis', fontweight ='bold') 
ax.set_zlabel('Z-axis', fontweight ='bold')
# show plot
plt.show()
466/105:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
ax.set_xlabel('Safe & Clear', fontweight ='bold') 
ax.set_ylabel('Sustainable', fontweight ='bold') 
ax.set_zlabel('Enthusiasm', fontweight ='bold')
# show plot
plt.show()
466/106:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
 
# Creating dataset
z = df_NEW["Enthusiasm"]


x = df_NEW["safe and clear"]
y = df_NEW["Sustainable"]
cluster=df_NEW["clusters_3"]
 
# Creating figure
fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
 
# Creating plot
ax.scatter3D(x, y, z, c = cluster, s=50, cmap='viridis', marker ='o')
plt.title("simple 3D scatter plot")
fig.set_label("cluster")
ax.set_xlabel('Safe & Clear', fontweight ='bold') 
ax.set_ylabel('Sustainable', fontweight ='bold') 
ax.set_zlabel('Enthusiasm', fontweight ='bold')
# show plot
plt.show()
466/107: df_NEW.plot.scatter(x = 0, y = 1, c=labels, s=50, cmap='viridis')
466/108: df_NEW.plot.scatter(x = 11, y = 12, c=labels, s=50, cmap='viridis')
466/109: df_NEW_4.to_csv("cluster_data_3.csv")
473/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
from folium import plugins
473/2: from Book_extraction_single import search_for_file_path
473/3: path_file, base=search_for_file_path ()
475/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
475/2: path_file, base=search_for_file_path ()
475/3:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
475/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
475/5:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
475/6:
accounting
airport
amusement_park
aquarium
art_gallery
atm
bakery
bank
bar
beauty_salon
bicycle_store
book_store
bowling_alley
bus_station
cafe
campground
car_dealer
car_rental
car_repair
car_wash
casino
cemetery
church
city_hall
clothing_store
convenience_store
courthouse
dentist
department_store
doctor
drugstore
electrician
electronics_store
embassy
fire_station
florist
funeral_home
furniture_store
gas_station
gym
hair_care
hardware_store
hindu_temple
home_goods_store
hospital
insurance_agency
jewelry_store
laundry
lawyer
library
light_rail_station
liquor_store
local_government_office
locksmith
lodging
meal_delivery
meal_takeaway
mosque
movie_rental
movie_theater
moving_company
museum
night_club
painter
park
parking
pet_store
pharmacy
physiotherapist
plumber
police
post_office
primary_school
real_estate_agency
restaurant
roofing_contractor
rv_park
school
secondary_school
shoe_store
shopping_mall
spa
stadium
storage
store
subway_station
supermarket
synagogue
taxi_stand
tourist_attraction
train_station
transit_station
travel_agency
university
veterinary_care
zoo
475/7:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
475/8:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
475/9:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
475/10:
accounting
airport
amusement_park
aquarium
art_gallery
atm
bakery
bank
bar
beauty_salon
bicycle_store
book_store
bowling_alley
bus_station
cafe
campground
car_dealer
car_rental
car_repair
car_wash
casino
cemetery
church
city_hall
clothing_store
convenience_store
courthouse
dentist
department_store
doctor
drugstore
electrician
electronics_store
embassy
fire_station
florist
funeral_home
furniture_store
gas_station
gym
hair_care
hardware_store
hindu_temple
home_goods_store
hospital
insurance_agency
jewelry_store
laundry
lawyer
library
light_rail_station
liquor_store
local_government_office
locksmith
lodging
meal_delivery
meal_takeaway
mosque
movie_rental
movie_theater
moving_company
museum
night_club
painter
park
parking
pet_store
pharmacy
physiotherapist
plumber
police
post_office
primary_school
real_estate_agency
restaurant
roofing_contractor
rv_park
school
secondary_school
shoe_store
shopping_mall
spa
stadium
storage
store
subway_station
supermarket
synagogue
taxi_stand
tourist_attraction
train_station
transit_station
travel_agency
university
veterinary_care
zoo
475/11:
# Do a simple nearby search where we specify the location
# in lat/lon format, along with a radius measured in meters
places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'tourist_attraction')
475/12: len(places_result)
475/13:
Data_PD=pd.DataFrame(places_result['results'])
Data_PD.to_csv('prova.csv')
483/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
483/2: path_file, base=search_for_file_path ()
483/3:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
483/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
483/5:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
483/6: df
483/7: unicos=pd.unique(df["lugares"])
483/8:
unicos=pd.unique(df["lugares"])
unicos
483/9: import mlrose
483/10: import mlrose
483/11:
# Create list of city coordinates
coords_list = df.iloc[:,3:4]

# Initialize fitness function object using coords_list
#fitness_coords = mlrose.TravellingSales(coords = coords_list)
483/12: coords_list
483/13:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
#fitness_coords = mlrose.TravellingSales(coords = coords_list)
483/14: coords_list
483/15:
# Create list of city coordinates
coords_list = df.iloc[:,4:5]

# Initialize fitness function object using coords_list
#fitness_coords = mlrose.TravellingSales(coords = coords_list)
483/16: coords_list
483/17:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
#fitness_coords = mlrose.TravellingSales(coords = coords_list)
483/18: coords_list
483/19:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
489/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
from folium import plugins
489/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
from folium import plugins
489/3: from Book_extraction_single import search_for_file_path
491/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
491/2: import mlrose
491/3: path_file, base=search_for_file_path ()
491/4:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
491/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
491/6:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
491/7: df
491/8:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
491/9: coords_list
491/10: len(coords_list)
491/11: size(coords_list)
491/12: size.coords_list
491/13: coords_list.size
491/14: coords_list.size()
491/15: coords_list
491/16:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_no_fit = mlrose.TSPOpt(length = length(coords_list), coords = coords_list, maximize=False)
491/17:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/18: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/19:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/20: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/21: len(coords_list)
491/22:
# Create list of city coordinates
coords_list = df.iloc[:,3:5]
coords_list=coords_list[0:2,:]

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/23: coords_list[0:2,:]
491/24: coords_list.iloc[0:2,:]
491/25: coords_list.iloc[0:8,:]
491/26: pd.Series(df, name='lugares').unique()
491/27:
df_unicos=df.drop_duplicates(subset ="lugares", 
                     keep = False, inplace = True)
491/28: df_unicos
491/29: df_unicos
491/30:
df_unicos=df.drop_duplicates(subset ="lugares", 
                     keep = False, inplace = True)
491/31: type(df_unicos)
491/32: df
491/33:
df.drop_duplicates(subset ="lugares", 
                     keep = False, inplace = True)
491/34: df.drop_duplicates(subset ="lugares")
491/35: df_unicos=df.drop_duplicates(subset ="lugares")
491/36:
# Create list of city coordinates
coords_list = df_unicos.iloc[:,3:5]


# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/37: coords_list
491/38: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/39:
# Create list of city coordinates
coords_list = df_unicos.iloc[0:7,3:5]


# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/40: coords_list
491/41: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/42: coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), (1, 5), (2, 3)]
491/43: coords_list
491/44:
# Create list of city coordinates
coords_list = df_unicos.iloc[0:7,3:5]


# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/45: #df_unicos=df.drop_duplicates(subset ="lugares")
491/46: coords_list
491/47: coords_list[0]
491/48: coords_list.iloc[0]
491/49: coords_list.iloc[0][0]
491/50: coords_list.iloc[0][0], coords_list.iloc[0][1]
491/51: coords_list.iloc[0][0], coords_list.iloc[0][1], coords_list.iloc[1][0], coords_list.iloc[1][1]
491/52: type(coords_list)
491/53: coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), (1, 5), (2, 3)]
491/54: type(coords_list)
491/55:
# Create list of city coordinates
coords_list = df_unicos.iloc[0:7,3:5]


# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_list)
problem_fit = mlrose.TSPOpt(length = len(coords_list), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/56: coords_list.iloc[0][0], coords_list.iloc[0][1], coords_list.iloc[1][0], coords_list.iloc[1][1]
491/57: coords_list.values.tolist()
491/58: coords_LT=coords_list.values.tolist()
491/59:
# Create list of city coordinates
coords_list = df_unicos.iloc[0:7,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/60: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/61: best_state
491/62:
# Create list of city coordinates
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/63: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/64: best_state
491/65: len(coords_list)
491/66: best_fitness
491/67:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/68: len(coords_list)
491/69: path_file, base=search_for_file_path ()
491/70:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
491/71:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
491/72: df
491/73: #df_unicos=df.drop_duplicates(subset ="lugares")
491/74:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/75: len(coords_list)
491/76:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
491/77:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
491/78:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
491/79: df
491/80: #df_unicos=df.drop_duplicates(subset ="lugares")
491/81:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
#problem_no_fit = mlrose.TSPOpt(length = len(coords_list), coords = coords_list, maximize=False)
491/82: len(coords_list)
491/83: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/84: best_state
491/85: pd.DataFrame(best_state)
491/86: df['new_order']=best_state
491/87: df.sort_values(by=['new_order'])
491/88: df_new=df.sort_values(by=['new_order'])
491/89:
plt.plot(df_new['LON_google'], df_new['LAT_google'], '-o')
plt.show()
491/90: import matplotlib.pyplot as plt
491/91:
plt.plot(df_new['LON_google'], df_new['LAT_google'], '-o')
plt.show()
491/92: df.head(10)
491/93: df_new
491/94: df_new.head(20)
491/95: df_new=df.sort_values(by=df.index)
491/96: best_state[0]
491/97: best_state[:]
491/98: df.shape[0]
491/99: df.index[0]
491/100: df.index[1]
491/101:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[n] == df['new_order'][m]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[m][3], 'LON': dat.iloc[m][4]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/102:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[n] == df['new_order'][m]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[m][3], 'LON': df.iloc[m][4]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/103: dat1
491/104:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/105:
plt.plot(dat1['LON'], dat1['LAT'], 'o')
plt.show()
491/106:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/107: #
491/108: df['new_order'][1]
491/109: df['new_order'][0]
491/110:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[m] == df['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[m][3], 'LON': df.iloc[m][4]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/111: dat1
491/112:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/113:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[m] == df['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[n][3], 'LON': df.iloc[n][4]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/114: dat1
491/115:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/116:
plt.plot(dat1['LAT'], dat1['LON'], '-o')
plt.show()
491/117:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/118: dat1[0:20,:]
491/119: dat1.iloc[0:20,:]
491/120:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[m] == df['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[n][3], 'LON': df.iloc[n][4], 'order': df.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/121: dat1.iloc[0:20,:]
491/122: #
491/123:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/124:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                              max_attempts = 100, random_state = 2)
491/125: len(coords_list)
491/126: best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
491/127: best_state[:]
491/128: best_fitness
491/129: pd.DataFrame(best_state)
491/130: df['new_order']=best_state
491/131: df.index[0]
491/132: df['new_order'][0]
491/133:
dat1 = pd.DataFrame([])
for n in range(df.shape[0]):
    for m in range(df.shape[0]):
        if df.index[m] == df['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df.iloc[n][3], 'LON': df.iloc[n][4], 'order': df.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/134: dat1.iloc[0:20,:]
491/135: #
491/136:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/137: df_unicos=df.drop_duplicates(subset ="lugares")
491/138:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                              max_attempts = 100, random_state = 2)
491/139:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                              max_attempts = 100, random_state = 2)
491/140:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
491/141:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
491/142:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
491/143: df.head(10)
491/144: df_unicos=df.drop_duplicates(subset ="lugares")
491/145:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                              max_attempts = 100, random_state = 2)
491/146:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/147: df_unicos
491/148: df_unicos=df.drop_duplicates(subset ="lugares")
491/149: df_unicos
491/150:
# Create list of city coordinates
df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/151: df_unicos=df.drop_duplicates(subset ="lugares")
491/152: df_unicos
491/153:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/154:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/155: coords_LT
491/156: type(coords_LT)
491/157:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/158:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/159: coords_LT
491/160:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = 8, fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/161:
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/162:
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/163: len(coords_LT)
491/164: df_unicos=df.drop_duplicates(subset ="lugares")
491/165: len(df_unicos)
491/166:
len(df_unicos)
df_unicos
491/167: len(df_unicos)
491/168:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = 8, fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/169:
len(df_unicos)
len(coords_LT)
491/170:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/171:
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/172: best_state[:]
491/173: best_fitness
491/174: pd.DataFrame(best_state)
491/175: df_unicos['new_order']=best_state
491/176: df_unicos.index[0]
491/177: df_unicos['new_order'][0]
491/178:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/179: df_unicos
491/180: df_unicos.head(5)
491/181: df_unicos.head(32)
491/182: df_unicos.head(5)
491/183:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'][n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/184: #pd.DataFrame(best_state)
491/185: df_unicos['new_order'][:]
491/186: df_unicos.index[m] == df_unicos['new_order'][n]
491/187: df_unicos.index[0] == df_unicos['new_order'][0]
491/188: df_unicos.index[0] == df_unicos['new_order'][24]
491/189: df_unicos['new_order'][24]
491/190: df_unicos['new_order'][22]
491/191: df_unicos['new_order'][2]
491/192: df_unicos['new_order'][0]
491/193: df_unicos['new_order'][1]
491/194: df_unicos['new_order'][2]
491/195: df_unicos['new_order']
491/196: df_unicos['new_order'].iloc[2]
491/197:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/198: dat1.iloc[0:20,:]
491/199:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/200: dat1
491/201:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/202:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/203: dat1
491/204: #
491/205:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/206:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/207: dat1
491/208: df_unicos['new_order'].iloc[3]
491/209: df_unicos['new_order'].iloc[30]
491/210: df_unicos.index[1]
491/211: df_unicos.index[2]
491/212: df_unicos.head(10)
491/213:
df_unicos['new_order']=best_state
df_unicos.reset_index
491/214:
df_unicos['new_order']=best_state
df_unicos.reset_index()
491/215: df_unicos.index[2]
491/216:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/217: df_unicos.index[2]
491/218: df_unicos['new_order'].iloc[30]
491/219: df_unicos.head(10)
491/220:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][3], 'LON': df_unicos.iloc[n][4], 'order': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/221: dat1
491/222:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/223: dat1
491/224:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/225:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 100, random_state = 2)
491/226: best_state[:]
491/227: best_fitness
491/228: #pd.DataFrame(best_state)
491/229:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/230: df_unicos.index[2]
491/231: df_unicos['new_order'].iloc[30]
491/232: df_unicos.head(10)
491/233:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/234: dat1
491/235: #
491/236:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/237: df_unicos=df.drop_duplicates(subset ="lugares")
491/238:
len(df_unicos)
len(coords_LT)
491/239:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/240:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 100, random_state = 2)
491/241: best_state[:]
491/242: best_fitness
491/243: #pd.DataFrame(best_state)
491/244:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/245:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/246: dat1
491/247: #
491/248:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/249: dat1.to_csv("lllllll.csv")
491/250: df_unicos=df.drop_duplicates(subset ="lugares")
491/251:
len(df_unicos)
len(coords_LT)
491/252:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/253:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.02,
                                               max_attempts = 10, random_state = 2)
491/254: best_state[:]
491/255: best_fitness
491/256:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/257:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/258: dat1.to_csv("lllllll.csv")
491/259:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/260:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/261: #dat1.to_csv("lllllll.csv")
491/262: #
491/263:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/264: df_unicos=df.drop_duplicates(subset ="lugares")
491/265:
len(df_unicos)
len(coords_LT)
491/266:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/267:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.002,
                                               max_attempts = 10, random_state = 2)
491/268: best_state[:]
491/269: best_fitness
491/270: #pd.DataFrame(best_state)
491/271:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/272:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/273: #dat1.to_csv("lllllll.csv")
491/274: #
491/275:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/276: df_unicos=df.drop_duplicates(subset ="lugares")
491/277:
len(df_unicos)
len(coords_LT)
491/278:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/279:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.002,
                                               max_attempts = 100, random_state = 2)
491/280: best_state[:]
491/281: best_fitness
491/282: #pd.DataFrame(best_state)
491/283:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/284:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[n][4], 'LON': df_unicos.iloc[n][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/285: #dat1.to_csv("lllllll.csv")
491/286: #
491/287:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/288: dat1
491/289: df_unicos
491/290: df_unicos.head(5)
491/291: df_unicos.head(10)
491/292: df_unicos.head(20)
491/293: df_unicos.head(30)
491/294: df_unicos.head(10)
491/295: df_unicos.head(13)
491/296: df_unicos.head(14)
491/297:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[m][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/298: #dat1.to_csv("lllllll.csv")
491/299: dat1
491/300:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/301:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/302: #dat1.to_csv("lllllll.csv")
491/303: dat1
491/304:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 100, random_state = 2)
491/305: best_state[:]
491/306: best_fitness
491/307: #pd.DataFrame(best_state)
491/308:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/309:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/310: #dat1.to_csv("lllllll.csv")
491/311: dat1
491/312:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/313: df_unicos=df.drop_duplicates(subset ="lugares")
491/314: df_unicos.head(14)
491/315:
len(df_unicos)
len(coords_LT)
491/316:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/317:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 100, random_state = 2)
491/318: best_state[:]
491/319: best_fitness
491/320: #pd.DataFrame(best_state)
491/321:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/322:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/323: #dat1.to_csv("lllllll.csv")
491/324: dat1
491/325:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/326: df_unicos=df.drop_duplicates(subset ="lugares")
491/327: df_unicos.head(14)
491/328:
len(df_unicos)
len(coords_LT)
491/329:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
491/330:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 1000, random_state = 2)
491/331: best_state[:]
491/332: best_fitness
491/333: #pd.DataFrame(best_state)
491/334:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
491/335:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
491/336: #dat1.to_csv("lllllll.csv")
491/337: dat1
491/338:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
491/339: from geopy.distance import geodesic
491/340: dat1[0]
491/341: dat1.iloc[0]
491/342: geodesic(dat1.iloc[0], dat1.iloc[1]).miles)
491/343: geodesic(dat1.iloc[0,0:1], dat1.iloc[1,0:1]).miles)
491/344: dat1.iloc[0,0:1]
491/345: geodesic(dat1.iloc[0,0:2], dat1.iloc[1,0:2]).miles)
491/346: dat1.iloc[0,0:]
491/347: dat1.iloc[0,0:2]
491/348: dat1.iloc[0]
491/349: dat1.iloc[0][0]
491/350: dat1.iloc[0][0:1]
491/351: dat1.iloc[0][:]
491/352: dat1.iloc[0][0:1]
491/353: dat1.iloc[0][0:1,0]
491/354: dat1.iloc[0][1]
491/355: dat1.iloc[0]
491/356: dat1.iloc[0].values.tolist
491/357: dat1.iloc[0].values.tolist()
491/358: dat1.iloc[0,0:2].values.tolist()
491/359: dat1=dat1.values.tolist()
491/360: dat1
491/361: dat1[0]
491/362: dat1[0:2,:]
491/363: dat1[0:2]
491/364: dat1[0,0:2]
491/365: dat1[0]
491/366: dat1[0][0:2]
491/367:
dat1=dat1.values.tolist()
geodesic(dat1[0][0:2], dat1[1][0:2]).miles)
491/368: type(dat1[0][0:2])
491/369: (dat1[0][0:2])
491/370: (dat1[1][0:2])
491/371:
dat1=dat1.values.tolist()
geodesic(dat1[0][0:2], dat1[1][0:2]).miles)
491/372:
#dat1=dat1.values.tolist()
geodesic(dat1[0][0:2], dat1[1][0:2]).miles)
498/1: path_file, base=search_for_file_path ()
498/2:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
498/3: import mlrose
498/4: path_file, base=search_for_file_path ()
498/5:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
498/6:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
498/7:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
498/8: df.head(10)
498/9: df_unicos=df.drop_duplicates(subset ="lugares")
498/10: df_unicos.head(14)
498/11:
len(df_unicos)
len(coords_LT)
498/12:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
498/13:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 1000, random_state = 2)
498/14: len(df_unicos)
498/15:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
498/16: len(df_unicos)
498/17:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
498/18:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
                                               max_attempts = 1000, random_state = 2)
498/19:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.002,
                                               max_attempts = 10, random_state = 2)
498/20:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.00002,
                                               max_attempts = 10, random_state = 2)
498/21: best_state[:]
498/22: best_fitness
498/23: #pd.DataFrame(best_state)
498/24:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
498/25:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
498/26: #dat1.to_csv("lllllll.csv")
498/27: dat1
498/28:
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
498/29: from geopy.distance import geodesic
498/30:
plt.plot(dat1['LON'], dat1['LAT'], 'o')
plt.show()
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
498/31: from geopy.distance import geodesic
498/32:
#dat1=dat1.values.tolist()
geodesic(dat1[0][0:2], dat1[1][0:2]).miles)
498/33: dat1[0][0:2]
498/34:
dat1=dat1.values.tolist()
geodesic(dat1[0][0:2], dat1[1][0:2]).miles)
498/35: dat1[0][0:2]
498/36: dat1
498/37:
dat2=dat1.values.tolist()
geodesic(dat2[0][0:2], dat2[1][0:2]).miles)
498/38: dat2.iloc[0,0:2].values.tolist()
498/39: dat2[0,0:2]
498/40: dat2
498/41: dat2=dat1.values.tolist()
498/42:
dat2=dat1.values.tolist()
dat2
498/43:
#dat2=dat1.values.tolist()
geodesic(dat2[0][0:2], dat2[1][0:2]).miles)
498/44: dat2[0,0:2]
498/45: dat2
498/46: dat2[1]
498/47: dat2[0][0:2]
498/48:
#dat2=dat1.values.tolist()
geodesic(41.8932529, 12.484504, 41.8902102, 12.4922309).miles)
498/49:
#dat2=dat1.values.tolist()
geodesic((41.8932529, 12.484504), (41.8902102, 12.4922309)).miles)
498/50:
newport_ri = (41.49008, -71.312796)
cleveland_oh = (41.499498, -81.695391)
print(geodesic(newport_ri, cleveland_oh).miles)
498/51:
#dat2=dat1.values.tolist()
geodesic((41.8932529, 12.484504), (41.8902102, 12.4922309)).miles
498/52:
#dat2=dat1.values.tolist()
geodesic(dat2[0][0:2], dat2[1][0:2]).miles
498/53:
#dat2=dat1.values.tolist()
geodesic(dat2[1][0:2], dat2[2][0:2]).miles
498/54:
#dat2=dat1.values.tolist()
geodesic(dat2[1][0:2], dat2[2][0:2]).kilometers
498/55:
#dat2=dat1.values.tolist()
geodesic(dat2[0][0:2], dat2[1][0:2]).kilometers
498/56:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
498/57:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
498/58:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)

best_state, best_fitness
498/59:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)

#best_state, best_fitness
498/60: best_state, best_fitness
498/61: coords_list
498/62:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
best_state, best_fitness = mlrose.genetic_alg(problem_no_fit, random_state = 2)

#best_state, best_fitness
498/63: best_state, best_fitness
498/64:
from pyproj import Transformer
transformer = Transformer.from_crs('epsg:4269','epsg:4326',always_xy=True)
points = list(zip(wellLoc.dd_lon,wellLoc.dd_lat))
coordsWgs = np.array(list(transformer.itransform(points)))
coordsWgs[:5,:]
498/65:
from pyproj import Transformer
transformer = Transformer.from_crs(41.8991633, 12.4730742,always_xy=True)
points = list(zip(wellLoc.dd_lon,wellLoc.dd_lat))
coordsWgs = np.array(list(transformer.itransform(points)))
coordsWgs[:5,:]
498/66:
coords_list = [(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), 
               (1, 5), (2, 3)]
problem_no_fit = mlrose.TSPOpt(length = 8, coords = coords_list,
                               maximize=False)
best_state, best_fitness = mlrose.genetic_alg(problem_no_fit, random_state = 2)

#best_state, best_fitness
498/67:
from pyproj import Transformer
transformer = Transformer.from_crs('epsg:4269','epsg:4326',always_xy=True)
points = list(zip(wellLoc.dd_lon,wellLoc.dd_lat))
coordsWgs = np.array(list(transformer.itransform(points)))
coordsWgs[:5,:]
498/68:
from pyproj import Transformer
transformer = Transformer.from_crs('epsg:4269','epsg:4326',always_xy=True)
transformer
498/69:
from pyproj import Transformer
transformer = Transformer.from_crs(41.8991633, 12.4730742,always_xy=True)
transformer
498/70:
from pyproj import Transformer
transformer = Transformer.from_crs('epsg:4269','epsg:4326',always_xy=True)
points = list(zip(df.LON_google,df.LAT_google))
coordsWgs = np.array(list(transformer.itransform(points)))
coordsWgs[:5,:]
498/71:
figure = plt.figure(figsize=(10,12))
plt.scatter(wellLoc.lonWgs, wellLoc.latWgs, s=15, c='goldenrod')
plt.show()
498/72:
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
498/73: df_unicos=df.drop_duplicates(subset ="lugares")
498/74: df_unicos.head(14)
498/75: len(df_unicos)
498/76:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
498/77:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
498/78:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.00002,
                                               max_attempts = 10, random_state = 2)
498/79:
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
498/80:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.00002,
                                               max_attempts = 20, random_state = 3)
498/81: best_state[:]
498/82: best_fitness
498/83:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.00002,
                                               max_attempts = 100, random_state = 1)
498/84: best_state[:]
498/85: best_fitness
498/86: from python_tsp.distances import great_circle_distance_matrix
498/87: from python_tsp.distances import great_circle_distance_matrix
498/88: from python_tsp.distances import great_circle_distance_matrix
498/89:
breakpoint()
distance_matrix = great_circle_distance_matrix(sources)
498/90:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
498/91: distance_matrix
498/92: permutation, distance = solve_tsp_dynamic_programming(distance_matrix)
498/93:
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
498/94: permutation, distance = solve_tsp_dynamic_programming(distance_matrix)
500/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
500/2: import mlrose
500/3: path_file, base=search_for_file_path ()
500/4:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
500/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
500/6:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
500/7: df.head(10)
500/8: df_unicos=df.drop_duplicates(subset ="lugares")
500/9:
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
500/10:
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
500/11:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
500/12: distance_matrix
500/13: len(distance_matrix)
500/14: distance_matrix.size
500/15: distance_matrix.size()
500/16: distance_matrix.size
500/17: pd.DataFrame(distance_matrix)
500/18: #pd.DataFrame(distance_matrix)
500/19: permutation, distance = solve_tsp_dynamic_programming(distance_matrix)
500/20:
Matriz_dist=pd.DataFrame(distance_matrix)
Matriz_dist.to_csv("matriz_dist.csv")
500/21: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:5,0:5])
500/22: permutation
500/23: distance
500/24: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:10,0:10])
500/25: permutation
500/26: distance
500/27: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:38,0:38])
502/27: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:25,0:25])
502/28:
Matriz_dist=pd.DataFrame(distance_matrix)
Matriz_dist.to_csv("matriz_dist.csv")
502/29: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:10,0:10])
502/30: permutation
502/31: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:15,0:15])
502/32: permutation
502/33: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:20,0:20])
502/34: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:16,0:16])
502/35: permutation
502/36: distance
502/37: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[16:28,16:28])
502/38: permutation
502/39: permutation+16
502/40: permutation
502/41: permutation[:+16,:]
502/42: permutation
502/43: distance
502/44: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:15,0:15])
502/45: permutation
502/46: distance
502/47:
df_unicos_2=df_unicos.iloc(0:10,:)
df_unicos_2['new_order']=permutation
df_unicos_2=df_unicos_2.reset_index()
502/48:
df_unicos_2=df_unicos.iloc[0:10,:]
df_unicos_2['new_order']=permutation
df_unicos_2=df_unicos_2.reset_index()
502/49:
df_unicos_2=df_unicos.iloc[0:15,:]
df_unicos_2['new_order']=permutation
df_unicos_2=df_unicos_2.reset_index()
502/50:
dat1 = pd.DataFrame([])
for n in range(df_unicos_2.shape[0]):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/51:
dat1 = pd.DataFrame([])
for n in range(df_unicos_2.shape[0]):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/52: df_unicos_2
502/53:
df_unicos_2
df_unicos_2.shape[0]
502/54:
df_unicos_2
#df_unicos_2.shape[0]
502/55:
dat1 = pd.DataFrame([])
for n in range(
    
):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/56:
dat1 = pd.DataFrame([])
for n in range(df_unicos_2.shape[0]):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/57:
df_unicos_2
#df_unicos_2.shape[0]
df_unicos_2.iloc[n][10]
502/58:
df_unicos_2
#df_unicos_2.shape[0]
df_unicos_2.iloc[2][10]
502/59:
df_unicos_2
#df_unicos_2.shape[0]
df_unicos_2.iloc[2][5]
502/60:
df_unicos_2
#df_unicos_2.shape[0]
df_unicos_2.iloc[2][4]
502/61:
dat1 = pd.DataFrame([])
for n in range(df_unicos_2.shape[0]):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/62:
dat1 = pd.DataFrame([])
for n in range(df_unicos_2.shape[0]):
    for m in range(df_unicos_2.shape[0]):
        if df_unicos_2.index[m] == df_unicos_2['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[m][4], 'LON': df_unicos_2.iloc[m][5], 'order': df_unicos_2.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/63:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
502/64:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1
502/65:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]))
502/66:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]})
502/67:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]}))
502/68:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]}), index=[0]), ignore_index=True))
502/69:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]}), index=[0]), ignore_index=True)
502/70:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]}, index=[0], ignore_index=True)
502/71:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': df_unicos_2.iloc[1][4]}, index=[0]), ignore_index=True)
502/72:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT': 4}, index=[0]), ignore_index=True)
502/73:
pd.show_versions()

permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:15,0:15])
502/74: pd.show_versions()
502/75: #pd.show_versions()
502/76:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT_o': 4}, index=[0]), ignore_index=True)
502/77:
df_unicos_2
#df_unicos_2.shape[0]
dat1 = pd.DataFrame([])
dat1=dat1.append(pd.DataFrame({'LAT_o': 4}, index=[0]), ignore_index=True)
502/78:
# Create list of city coordinates
#df_unicos=df
coords_list = df_unicos.iloc[:,3:5]
coords_LT=coords_list.values.tolist() 

# Initialize fitness function object using coords_list
fitness_coords = mlrose.TravellingSales(coords = coords_LT)
problem_fit = mlrose.TSPOpt(length = len(coords_LT), fitness_fn = fitness_coords, maximize=False)
# problem_fit = mlrose.genetic_alg(problem_fit, mutation_prob = 0.2,
#                                               max_attempts = 100, random_state = 2)
502/79:
#best_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state = 2)
best_state, best_fitness = mlrose.genetic_alg(problem_fit, mutation_prob = 0.00002,
                                               max_attempts = 100, random_state = 1)
502/80: best_state[:]
502/81: best_fitness
502/82:
df_unicos['new_order']=best_state
df_unicos=df_unicos.reset_index()
502/83:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/84:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/85:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == df_unicos['new_order'].iloc[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
# print(dat1)
#df = pd.concat([dat, dat1], axis=1)
#print(df)
502/86:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
502/87: import mlrose
502/88: path_file, base=search_for_file_path ()
502/89:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
502/90:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
502/91:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
502/92: df.head(10)
502/93: df_unicos=df.drop_duplicates(subset ="LON_google")
502/94: df_unicos.head(14)
502/95:
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
502/96:
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
502/97:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
502/98:
Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
502/99:
Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
len(Matriz_dist)
502/100: permutation, distance = solve_tsp_dynamic_programming(distance_matrix[0:23,0:23])
503/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
503/2: import mlrose
503/3: path_file, base=search_for_file_path ()
503/4:
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')

# display world map
#world_map
503/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
503/6:
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
503/7: df.head(10)
503/8: df_unicos=df.drop_duplicates(subset ="LON_google")
503/9: df_unicos.head(14)
503/10:
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
503/11:
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
503/12: #df.head(10)
503/13:
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
503/14:
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
503/15: #df_unicos.head(14)
503/16:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
503/17:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
503/18:
import mlrose
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
503/19:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
503/20:
def print_solution(manager, routing, solution):
    """Prints solution on console."""
    print('Objective: {} miles'.format(solution.ObjectiveValue()))
    index = routing.Start(0)
    plan_output = 'Route for vehicle 0:\n'
    route_distance = 0
    while not routing.IsEnd(index):
        plan_output += ' {} ->'.format(manager.IndexToNode(index))
        previous_index = index
        index = solution.Value(routing.NextVar(index))
        route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)
    plan_output += ' {}\n'.format(manager.IndexToNode(index))
    print(plan_output)
    plan_output += 'Route distance: {}miles\n'.format(route_distance)
503/21:
manager = pywrapcp.RoutingIndexManager(len(distance_matrix),
                                           1, 0)
503/22: manager
503/23: routing = pywrapcp.RoutingModel(manager)
503/24: transit_callback_index = routing.RegisterTransitCallback(distance_matrix)
503/25: transit_callback_index
503/26: routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)
503/27:
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)
search_parameters = pywrapcp.DefaultRoutingSearchParameters()
    search_parameters.first_solution_strategy = (
        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)
503/28:
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)
search_parameters = pywrapcp.DefaultRoutingSearchParameters()
search_parameters.first_solution_strategy = (routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)
503/29:
# Solve the problem.
    solution = routing.SolveWithParameters(search_parameters)

    # Print solution on console.
    if solution:
        print_solution(manager, routing, solution)
503/30:
# Solve the problem.
solution = routing.SolveWithParameters(search_parameters)

    # Print solution on console.
if solution:
    print_solution(manager, routing, solution)
503/31: solution
503/32: routing
503/33: search_parameters
503/34:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')

for i in range(df_unicos.shape[0]):
    plt.text(s=df.index[i], fontdict=dict(color=’red’,size=10), bbox=dict(facecolor=’yellow’,alpha=0.5))


plt.show()
503/35:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')

for i in range(df_unicos.shape[0]):
    plt.text(s=df.index[i])


plt.show()
503/36:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')

for i in range(df_unicos.shape[0]):
    plt.text(x=df_unicos.LON_google.iloc[i], y=df_unicos.LAT_google.iloc[i], s=df.index[i])


plt.show()
503/37:
manager = pywrapcp.RoutingIndexManager(len(distance_matrix),
                                           1, 0)
503/38: routing = pywrapcp.RoutingModel(manager)
503/39: distance_matrix
503/40: type(distance_matrix)
503/41:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
Matriz_dist.to_csv("matriz_dist.csv")
len(Matriz_dist)
503/42: distance_matrix[0,0]
503/43: distance_matrix[0,1]
503/44: Matriz_dist
503/45: Matriz_dist.iloc[0]
503/46: Matriz_dist.iloc[0].sort_values
503/47: Matriz_dist.iloc[0].sort_values()
503/48: Matriz_dist[0].sort_values()
503/49: Matriz_dist.0.sort_values()
503/50: Matriz_dist.sort_values()
503/51: Matriz_dist["0"].sort_values()
503/52: Matriz_dist[0].sort_values()
503/53: Matriz_dist[0].sort_values().index()
503/54: Matriz_dist[0].sort_values()ff=
503/55: ff=Matriz_dist[0].sort_values()
503/56:
ff=Matriz_dist[0].sort_values()
ff.index()
503/57:
ff=Matriz_dist[0].sort_values()
ff
503/58:
ff=Matriz_dist[0].sort_values()
ff[0]
503/59:
ff=Matriz_dist[0].sort_values()
ff[1]
503/60:
ff=Matriz_dist[0].sort_values()
ff.index
503/61:
ff=Matriz_dist[0].sort_values()
ff.index[1]
503/62:
ff=Matriz_dist[0].sort_values()
ff.index[1]
ff
503/63:
ff=Matriz_dist.sort_values(0)
ff.index[1]
ff
503/64:
ff=Matriz_dist.sort_values(0)
ff.index[1]
#ff
503/65:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index
503/66:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index()
503/67:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[1]
503/68:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[_:]
503/69:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[:]
503/70:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index
503/71:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist[index]
503/72:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist
503/73:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index(0)
503/74:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[0:44]
503/75:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[0:4]
503/76:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[1]
503/77:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[2]
503/78:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Matriz_dist.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Sorted_matrix.index[1])
    Bridge=Bridge.drop(index=0)
503/79:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Matriz_dist.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(index=0)
503/80:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Matriz_dist.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(axis=0)
503/81:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Matriz_dist.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/82:
ff=Matriz_dist.sort_values(0)
ff.index[1]
Matriz_dist.index[1]
503/83:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1]
ff
503/84:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1]
503/85:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[0]
503/86:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1]
503/87:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[1]
503/88:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[2]
503/89:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Matriz_dist.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/90:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[1]
503/91:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
503/92:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Bridge.index[1]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/93:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
Matriz_dist.index[1]
503/94:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
Matriz_dist.index[0]
503/95:
new_order=[]
Bridge=Matriz_dist
for i in range(len(distance_matrix)):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/96:
new_order=[]
Bridge=Matriz_dist
for i in range(2):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/97: new_order
503/98:
new_order=[]
Bridge=Matriz_dist
for i in range(3):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/99: new_order
503/100:
new_order=0
Bridge=Matriz_dist
for i in range(3):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/101: new_order
503/102:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/103: new_order
503/104: Bridge
503/105:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/106:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[index])
503/107: new_order
503/108:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/109: new_order
503/110:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    index=Bridge.index[i]
    Bridge=Bridge.sort_values(index)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/111: new_order
503/112:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/113: new_order
503/114:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/115: new_order
503/116:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[new_order])
503/117:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[i])
503/118: new_order
503/119:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/120: new_order
503/121:
new_order=[0]
Bridge=Matriz_dist
for i in range(10):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/122: new_order
503/123:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/124: new_order
503/125: Bridge
503/126:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/127: new_order
503/128: Bridge
503/129:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    Bridge=Bridge.drop(Bridge.index[0])
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
503/130: new_order
503/131:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    Bridge=Bridge.drop(Bridge.index[0])
    new_order.append(Bridge.index[1])
503/132: new_order
503/133: Bridge
503/134:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/135: new_order
503/136: Bridge
503/137:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/138: new_order
503/139: Bridge
503/140:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/141: new_order
503/142: Bridge
503/143:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/144: new_order
503/145: Bridge
503/146:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/147: new_order
503/148: Bridge
503/149:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[0])
    Bridge=Bridge.drop(Bridge.index[0])
503/150: new_order
503/151:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/152: new_order
503/153: Bridge.sort_values(5)
503/154: Bridge.sort_values(5).index(0)
503/155: fff=Bridge.sort_values(5)
503/156:
fff=Bridge.sort_values(5)
fff
503/157:
fff=Bridge.sort_values(5)
fff.index(1)
503/158:
fff=Bridge.sort_values(5)
fff.index[1]
503/159:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/160: new_order
503/161:
fff=Bridge.sort_values(5)
fff.index[2]
503/162:
fff=Bridge.sort_values(5)
fff
503/163:
fff=Bridge.sort_values(5)
fff.index[0]
503/164:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[0])
    Bridge=Bridge.drop(Bridge.index[0])
503/165: new_order
503/166:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/167: new_order
503/168:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/169: new_order
503/170:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[1])
503/171: new_order
503/172:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[0])
    Bridge=Bridge.drop(Bridge.index[1])
503/173: new_order
503/174:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[1])
503/175: new_order
503/176:
new_order=[0]
Bridge=Matriz_dist
for i in range(5):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[1])
503/177: new_order
503/178:
new_order=[0]
Bridge=Matriz_dist
for i in range(5):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
503/179: new_order
503/180:
new_order=[0]
Bridge=Matriz_dist
for i in range(5):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order)
503/181:
new_order=[0]
Bridge=Matriz_dist
for i in range(5):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/182:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/183:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/184: new_order
503/185:
fff=Bridge.sort_values(5)
fff.index[0]
503/186:
fff=Bridge.sort_values(5)
fff
503/187:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/188: new_order
503/189:
fff=Bridge.sort_values(5)
fff
503/190:
fff=Bridge.sort_values(3)
fff
503/191:
fff=Bridge.sort_values(3)
fff
Bridge.drop(Bridge.index[0])
503/192:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/193: new_order
503/194:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    
    new_order.append(Bridge.index[1])
    Bridge=Bridge.sort_values(new_order)
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/195:
new_order=[0]
Bridge=Matriz_dist
for i in range(4):
    #index=Bridge.index[i]
    
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/196:
new_order=[0]
Bridge=Matriz_dist
for i in range(20):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/197:
new_order=[0]
Bridge=Matriz_dist
for i in range(1):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/198: new_order
503/199:
fff=Bridge.sort_values(3)
fff
Bridge.drop(Bridge.index[0])
503/200:
fff=Bridge.sort_values(3)
fff
503/201:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order)
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/202: new_order
503/203:
fff=Bridge.sort_values(5)
fff
503/204:
fff=Bridge.sort_values(5)
fff.index[1]
503/205:
new_order=[0]
Bridge=Matriz_dist
for i in range(2):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/206:
new_order=[0]
Bridge=Matriz_dist
for i in range(3):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/207:
new_order=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/208: new_order
503/209:
new_order=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/210: new_order
503/211:
new_order=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    distance.append(Bridge[new_order[i]][1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/212:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    Bridge=Bridge.sort_values(new_order[i])
    new_order.append(Bridge.index[1])
    distance.append(Bridge[new_order[i]][1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/213:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge[pos][1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/214:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
Matriz_dist[0]
503/215:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
Matriz_dist[0][1]
503/216:
ff=Matriz_dist.sort_values(0)
Matriz_dist.index[1], ff.index[0]
Matriz_dist[0][0]
503/217:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[pos,1])
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/218:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/219:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    #distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/220:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    #distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
503/221:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
503/222:
fff=Bridge.sort_values(5)
fff.index[1]
df_unicos.shape[0]
503/223:

df_unicos.shape[0]
503/224: len(new_order)
503/225:

df_unicos
503/226:
df_unicos['new_order']=new_order
df_unicos=df_unicos.reset_index()
df_unicos
503/227:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10]}, index=[0]), ignore_index=True)
503/228: dat1
503/229:
plt.plot(dat1['LON'], dat1['LAT'], 'o')
plt.show()
plt.plot(dat1['LON'], dat1['LAT'], '-o')
plt.show()
503/230:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    #distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
503/231: len(new_order)
503/232:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, df_unicos.LAT, s=15, '-o')

for i in range(df_unicos.shape[0]):
    plt.text(x=df_unicos.LON_google.iloc[i], y=df_unicos.LAT_google.iloc[i], s=df.index[i])


plt.show()
503/233:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, df_unicos.LAT, '-o')

for i in range(df_unicos.shape[0]):
    plt.text(x=df_unicos.LON_google.iloc[i], y=df_unicos.LAT_google.iloc[i], s=df.index[i])


plt.show()
503/234:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, dat1.LAT, '-o')

for i in range(df_unicos.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/235:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, dat1.LAT, '-o')

for i in range(dat1.LAT.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/236:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, dat1.LAT, '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/237: dat1.LON
503/238: dat1.LAT
503/239:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1.LON, dat1.LAT)

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/240:
figure = plt.figure(figsize=(10,12))
plt.scatter(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/241:
figure = plt.figure(figsize=(10,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/242:
figure = plt.figure(figsize=(10,15))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/243:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df.index[i])


plt.show()
503/244:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df_unicos.index[i])


plt.show()
503/245: df
503/246:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][7],'quotes': df_unicos.iloc[n][9]}, index=[0]), ignore_index=True)
503/247: dat1
503/248:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
503/249: dat1
503/250:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Psition book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
503/251: dat1
503/252:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
503/253: dat1
507/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
507/2: sqlite3 history.sqlite
507/3:
TABLE history
    (session integer, line integer, source text, source_raw text,
    PRIMARY KEY (session, line));
507/4:
TABLE history
    (session integer, line integer, source text, source_raw text,
    PRIMARY KEY (session, line));
513/1: %history -g -f my_notebook_code.txt
515/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
import mlrose
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
515/2: path_file, base=search_for_file_path ()
515/3:
pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
515/4:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
515/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
515/6: df_unicos=df.drop_duplicates(subset ="LON_google")
515/7:
plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
515/8:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
515/9:
figure = plt.figure(figsize=(10,12))
plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
plt.show()
515/10:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
515/11:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
515/12:
Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
515/13:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    #distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    print(new_order, len(Bridge))
515/14:
pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/15:
dat1=pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/16:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/17: new_order
515/18: #new_order
515/19:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[m][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/20: df_unicos
515/21: df_unicos.size
515/22: df_unicos.shape
515/23:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': new_order[n],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/24: dat1
515/25:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/26: dat1
515/27: df_unicos
515/28:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/29: dat1
515/30: df_unicos.shape[0]
515/31:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
515/32:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
515/33:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
515/34:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
515/35:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
515/36:
Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
515/37: ff=Matriz_dist[0].sort_values()
515/38:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    #distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
515/39: df_unicos.shape[0]
515/40:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/41: dat1
515/42:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[n],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/43: dat1
515/44:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/45: dat1
515/46: dat1.size
515/47: dat1.shape
515/48: new_order
515/49: new_order.shape
515/50: len(new_order)
515/51:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/52: dat1
515/53:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[n] == new_order[m]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/54: dat1
515/55:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/56:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[n][2],'quotes': df_unicos.iloc[n][6], 'Position book': df_unicos.iloc[n][7]}, index=[0]), ignore_index=True)
515/57: dat1
515/58:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[m][2],'quotes': df_unicos.iloc[m][6], 'Position book': df_unicos.iloc[m][7]}, index=[0]), ignore_index=True)
515/59: dat1
515/60:
dat1 = pd.DataFrame([])
for m in range(df_unicos.shape[0]):
    for n in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][3], 'LON': df_unicos.iloc[m][4], 'order': new_order[m],
            'lugares': df_unicos.iloc[m][2],'quotes': df_unicos.iloc[m][6], 'Position book': df_unicos.iloc[m][7]}, index=[0]), ignore_index=True)
            print(n)
515/61: df_unicos
515/62:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df_unicos.index[i])
515/63:
df_unicos['new_order']=new_order
df_unicos=df_unicos.reset_index()
df_unicos
515/64:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/65: dat1
515/66:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df_unicos.index[i])
515/67:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df_unicos.index[i], df_unicos.iloc[i][3] )
515/68:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=df_unicos.iloc[i][3] )
515/69:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc(pos,1))
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
515/70:
ff=Matriz_dist[0].sort_values()
Matriz_dist
515/71:
ff=Matriz_dist[0].sort_values()
Matriz_dist.head(4)
515/72:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[pos][1])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
515/73:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
515/74: distance
515/75:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
#df_unicos
515/76:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
515/77:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10],
            'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/78: dat1
515/79:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/80: dat1
515/81:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
515/82:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
515/83:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
515/84:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
515/85:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)
515/86:
Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
515/87:
ff=Matriz_dist[0].sort_values()
Matriz_dist.head(4)
515/88:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
515/89: distance
515/90:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
515/91:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/92: dat1
515/93:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[n][3],'quotes': df_unicos.iloc[n][7], 'Position book': df_unicos.iloc[n][8]}, index=[0]), ignore_index=True)
515/94: dat1
515/95:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
515/96: dat1
515/97:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.Distance.iloc[1] )
515/98:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.Distance [m].iloc[1] )
515/99:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.Distance[m].iloc[1] )
515/100:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][3] )
515/101: dat1["Distance [m]"].round_up
515/102: dat1["Distance [m]"]
515/103: dat1["Distance [m]"].round(decimals=0)
515/104: dat1["Distance [m]"]=dat1["Distance [m]"].round(decimals=0)
515/105:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][3] )
515/106:
dat1["Distance [m]"]=dat1["Distance [m]"].round(decimals=0)
dat1["Distance [m]"].sum()
515/107:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
515/108:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )

world_map = folium.Map(location=[df['LAT_google'].median(axis=0), df['LON_google'].median()], zoom_start=11, tiles='OpenStreetMap')
515/109: display world map
515/110: world_map
515/111:
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/112:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"][0], lugares3["LON"][0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"][: ff],lugares3["LON"][: ff], lugares3[0][: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/113:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"].iloc[: ff],lugares3["LON"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/114: lugares3["LAT"].iloc[0]
515/115:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/116:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"],lugares3["LON"], lugare3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/117:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugare3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/118: lugare3["lugares"]
515/119:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# display map
Area
515/120:
# create map
map_plot_antroute = folium.Map(location=[20,50], zoom_start=4)
# added lat long to route
route_lats_longs = [[22.4707, 70.0577],    # Jamnagar
                    [26.9124, 75.7873]]    # Jaipur
# Ploting ant-route
plugins.AntPath(route_lats_longs).add_to(map_plot_antroute)
map_plot_antroute
515/121:

folium.raster_layers.TileLayer(‘Open Street Map’).add_to(map_type)
folium.raster_layers.TileLayer(‘Stamen Terrain’).add_to(map_type)
folium.raster_layers.TileLayer(‘Stamen Toner’).add_to(map_type)
folium.raster_layers.TileLayer(‘Stamen Watercolor’).add_to(map_type)
folium.raster_layers.TileLayer(‘CartoDB Positron’).add_to(map_type)
folium.raster_layers.TileLayer(‘CartoDB Dark_Matter’).add_to(map_type)
515/122:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

mini_map = plugins.MiniMap(toggle_display=True)
# add the mini map to the big map
Area.add_child(mini_map)
Area
# display map
#Area
515/123:
from folium import plugins, MiniMap
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

mini_map = plugins.MiniMap(toggle_display=True)
# add the mini map to the big map
Area.add_child(mini_map)
Area
# display map
#Area
515/124:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/125: from folium.plugins import AntPath
515/126: from folium.plugins import antpath
515/127: #
515/128: from ipyleaflet import Map, AntPath
515/129: from ipyleaflet import Map, AntPath
515/130: #from ipyleaflet import Map, AntPath
515/131:
popup = Popup(
    location=center,
    child=message1,
    close_button=False,
    auto_close=False,
    close_on_escape_key=False
)
515/132:
from ipywidgets import HTML

from ipyleaflet import Map, Marker, Popup

center = (52.204793, 360.121558)

m = Map(center=center, zoom=9, close_popup_on_click=False)

marker = Marker(location=(52.1, 359.9))
m.add_layer(marker)

message1 = HTML()
message2 = HTML()
message1.value = "Try clicking the marker!"
message2.value = "Hello <b>World</b>"
message2.placeholder = "Some HTML"
message2.description = "Some HTML"

# Popup with a given location on the map:
popup = Popup(
    location=center,
    child=message1,
    close_button=False,
    auto_close=False,
    close_on_escape_key=False
)
m.add_layer(popup)

# Popup associated to a layer
marker.popup = message2

m
515/133:
from ipywidgets import HTML

from ipyleaflet import Map, Marker, Popup

center = (52.204793, 360.121558)

m = Map(center=center, zoom=9, close_popup_on_click=False)

marker = Marker(location=(52.1, 359.9))
m.add_layer(marker)

message1 = HTML()
message2 = HTML()
message1.value = "Try clicking the marker!"
message2.value = "Hello <b>World</b>"
message2.placeholder = "Some HTML"
message2.description = "Some HTML"

# Popup with a given location on the map:
popup = Popup(
    location=center,
    child=message1,
    close_button=False,
    auto_close=False,
    close_on_escape_key=False
)
m.add_layer(popup)

# Popup associated to a layer
marker.popup = message2

m
515/134:
from ipywidgets import HTML

from ipyleaflet import Map, Marker, Popup

center = (52.204793, 360.121558)

m = Map(center=center, zoom=9, close_popup_on_click=False)

marker = Marker(location=(52.1, 359.9))
m.add_layer(marker)

message1 = HTML()
message2 = HTML()
message1.value = "Try clicking the marker!"
message2.value = "Hello <b>World</b>"
message2.placeholder = "Some HTML"
message2.description = "Some HTML"

# Popup with a given location on the map:
popup = Popup(
    location=center,
    child=message1,
    close_button=False,
    auto_close=False,
    close_on_escape_key=False
)
m.add_layer(popup)

# Popup associated to a layer
marker.popup = message2

m
515/135:
from ipywidgets import HTML

from ipyleaflet import Map, Marker, Popup

center = (52.204793, 360.121558)

m = Map(center=center, zoom=9, close_popup_on_click=False)

marker = Marker(location=(52.1, 359.9))
m.add_layer(marker)

message1 = HTML()
message2 = HTML()
message1.value = "Try clicking the marker!"
message2.value = "Hello <b>World</b>"
message2.placeholder = "Some HTML"
message2.description = "Some HTML"

# Popup with a given location on the map:
popup = Popup(
    location=center,
    child=message1,
    close_button=False,
    auto_close=False,
    close_on_escape_key=False
)
m.add_layer(popup)

# Popup associated to a layer
marker.popup = message2

m
515/136:
import folium

m = folium.Map(location=[40.720, -73.993],
              zoom_start=15)

loc = [(40.720, -73.993),
       (40.721, -73.996)]

folium.PolyLine(loc,
                color='red',
                weight=15,
                opacity=0.8).add_to(m)
m
515/137:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
    folium.PolyLine(loc,
                color='red',
                weight=15,
                opacity=0.8).add_to(m)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/138:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

folium.PolyLine(loc, color='red', weight=15, opacity=0.8).add_to(m)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/139:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

Area=folium.PolyLine(loc, color='red', weight=15, opacity=0.8).add_to(m)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/140:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

folium.PolyLine(loc, color='red', weight=15, opacity=0.8).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/141:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
    folium.PolyLine(lat, lng, color='red', weight=15, opacity=0.8).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/142:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)

folium.PolyLine(lugares3["LAT"], lugares3["LON"], color='red', weight=15, opacity=0.8).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/143: lugares3["LAT"], lugares3["LON"]
515/144: loc=lugares3["LAT"], lugares3["LON"]
515/145:
loc=lugares3["LAT"], lugares3["LON"]
loc
515/146:
loc=lugares3["LAT"], lugares3["LON"]
loc=zeros[len(dat1),2]
515/147:
loc=lugares3.iloc[2:4]
loc=zeros[len(dat1),2]
515/148:
loc=lugares3.iloc[2:4]
loc
515/149:
loc=lugares3.iloc[:][2:4]
loc
515/150:
loc=lugares3.iloc[:,2:4]
loc
515/151:
loc=lugares3.iloc[:,3:4]
loc
515/152:
loc=lugares3.iloc[:,5:6]
loc
515/153:
loc=lugares3.iloc[:,1:2]
loc
515/154:
loc=lugares3.iloc[:,1:3]
loc
515/155:
loc=lugares3.iloc[:,0:2]
loc
515/156:
loc=lugares3.iloc[:,0:2].tolist
loc
515/157:
loc=lugares3.iloc[:,0:2].tolist()
loc
515/158:
loc=lugares3.iloc[:,0:2].to_list()
loc
515/159:
loc=lugares3.iloc[:,0:2]
loc
515/160:
loc=lugares3.iloc[:,0:2]
loc.values.tolist()
515/161:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='red', weight=15, opacity=0.8).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/162:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=15, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/163:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=2)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/164:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=10)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/165:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=15)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
515/166:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
519/1: %history -g -f anyfilename
529/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
import mlrose
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import
529/2:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
import mlrose
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
529/3: path_file, base=search_for_file_path ()
529/4:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/6:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/7:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/8:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/9:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/10:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/11:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/12:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
529/13:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
533/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift
import seaborn as sns; sns.set()
import csv
from folium import plugins
533/2: from Book_extraction_single import search_for_file_path
533/3: from Book_extraction_single import search_for_file_path
533/4: path_file, base=search_for_file_path ()
533/5:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/6:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/7:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/8:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
529/14: path_file, base=search_for_file_path ()
529/15:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/16:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/17:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/18:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/19:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/20:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/21:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/22:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/23:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
529/24:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
533/9:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/10:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/11:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(4):

    min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/12:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/13:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(4):

    min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/14:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/15:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/16:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/17:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(3):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/18:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/19: path_file, base=search_for_file_path ()
533/20:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(4):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/21:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/22:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
529/25: path_file, base=search_for_file_path ()
529/26:
df=pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/27:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/28:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/29:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
533/23: path_file, base=search_for_file_path ()
533/24:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/25:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/26:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/27:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/28:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
533/29:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.05
533/30:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/31:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.05
533/32:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/33:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/34:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/35: df
533/36:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
f
533/37:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/38: min_value
533/39:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/40:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/41:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
for k in range(5):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/42:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/43: min_value
533/44:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        if k==limit
            cluster_keep.append(clusters_DF.index[0:z])
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/45:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/46:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        if k==limit
            cluster_keep.append(clusters_DF.index[0:z])
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/47:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()
        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        if k==limit:
            cluster_keep.append(clusters_DF.index[0:z])
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/48:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/49: df
533/50:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/51:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):

    min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    if k==limit:
            min_value=0.6
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/52:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/53: df
533/54:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/55:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/56:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/57:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/58:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/59:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/60:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
529/30: path_file_2, base=search_for_file_path ()
529/31:
df=pd.read_csv(path_file_2)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/32:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/33:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/34:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/35:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/36:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/37:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/38:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/39:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
533/61:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/62:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/63:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/64:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/65:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/66:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/67:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/68:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/69:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/70: path_file, base=search_for_file_path ()
533/71:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/72:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.6+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/73:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/74:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/75:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/76:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/77:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/78:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/79:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=6
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/80:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/81:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/82:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=6
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.5+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+min_value/5
533/83:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/84:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=6
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+min_value/5
533/85:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/86:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/87:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=5
for k in range(limit):
    if k==limit-1:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/88:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/89:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/90:
#kmeans = KMeans(n_clusters = 3, init ='k-means++')
f=0
limit=6
for k in range(limit):
    if k==limit-2:
            min_value=0.6
    else:
        min_value=0.4+f
    centers=[]
    labels=[]
    model = MeanShift()
    model.fit(df[df.columns[2:4]]) # Compute k-means clustering.
    df['cluster_label'] = model.fit_predict(df[df.columns[2:4]])
    centers = model.cluster_centers_ # Coordinates of cluster centers.
    labels = model.predict(df[df.columns[2:4]]) # Labels of each point
    df.plot.scatter(x = 'LON_google', y = 'LAT_google', c=labels, s=50, cmap='viridis')
    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    plt.title('min_value= ' + str(min_value))
    counts_clusters=df.groupby(['cluster_label']).size()
    #df=df.loc[(df['cluster_label'] !=label_cl)]
    clusters_DF=pd.DataFrame(counts_clusters)
    clusters_DF=clusters_DF.sort_values(0, ascending=[False])
    clusters_DF['normalized']=clusters_DF[0].div(clusters_DF[0].sum())
    clusters_DF["cluster_label"]=clusters_DF.index
    #print(clusters_DF.index[0])
    
    cluster_keep=[]
    z=0
    sum_val=0
    
    for i in clusters_DF['normalized']:
        sum_val=clusters_DF['normalized'][0:z].sum()

        if sum_val>min_value:
            cluster_keep.append(clusters_DF.index[0:z])
            break
        
        else:
            z+=1
    cluster_keep=pd.DataFrame(cluster_keep)
    range_1=cluster_keep.size
    df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
    #plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.2)
    f=f+0.1
533/91:
#df = df[df['cluster_label'].isin(clusters_DF['cluster_label'][0:range_1])]
clusters_DF, min_value
533/92:
ff=len(df)
import folium
from folium import plugins
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[df["LAT_google"].mean(), df["LON_google"].mean()], zoom_start=4)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, pos, label, label_2 in zip(df["LAT_google"].iloc[: ff],df["LON_google"].iloc[: ff], df["Position"].iloc[: ff], df["lugares"].iloc[: ff], df["Quotes"].iloc[: ff]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup= str(pos)+' '+label + ': "' +label_2 + '"',
    ).add_to(Dots)

# display map
Area
Area.save('Maps/Clean_maps/Geocode_'+ base+ '.html')
df.to_csv("Data/Clean_data/Geocode_" + base + ".csv")
533/93: Area
529/40:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
529/41: path_file_2, base_2=search_for_file_path ()
529/42:
df=pd.read_csv(path_file_2)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/43:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/44:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/45:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/46:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/47:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/48:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/49:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/50:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
529/51:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
529/52: %history -g -f "Ultimo_update.txt"
529/53: dat1
529/54:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
529/55: path_file_2, base_2=search_for_file_path ()
529/56:
df=pd.read_csv(path_file_2)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/57:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/58:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/59:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/60:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/61:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/62:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/63:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/64:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
529/65:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
529/66: dat1
529/67: %history -g -f "Ultimo_update.txt"
529/68:
from Analysis_data import cluster_dots, path_dots, plot_path
path_dots(df, base)
529/69:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/70:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/71:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/72:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/73:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/74:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/75:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
Area=plot_path(dat1, base)
529/76:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/77:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/78:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area
# display map
#Area
529/79:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area.save('Maps/Clean_maps/Maps_path/Map_path_' + base_2 +'.html')
#Area
# display map
#Area
529/80:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/81:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/82:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/83:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/84:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/85:
import folium
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
533/94:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/95:
#df = pd.read_csv('Data/Data_Book_Geocode.csv')
df = pd.read_csv(path_file)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)

#df=df.drop(['Unnamed: 0', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], axis=1)
df
ax = df.plot.hexbin(x='LON_google', y='LAT_google', gridsize=40)
533/96:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
529/86:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
533/97:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/98:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/99:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/100: base
533/101: df
533/102: #df
533/103:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/104:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
533/105:
base
#df
529/87:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
plot_path(dat1, base)
529/88:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/89: plot_path(dat1, base)
529/90:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/91:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/92:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return()
529/93: plot_path(dat1, base)
529/94: base
529/95: plot_path(dat1, base_2)
529/96:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return
529/97: base
529/98: plot_path(dat1, base_2)
529/99:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/100: plot_path(dat1, base_2)
529/101:
from Analysis_data import cluster_dots, path_dots, plot_path
dat1=path_dots(df, base)
529/102:
from Analysis_data import cluster_dots, path_dots, plot_path
plot_path(dat1, base_2)
533/106:
from Analysis_data import cluster_dots, path_dots, plot_path
Datos_clust=cluster_dots(df, base)
529/103:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
529/104: path_file_2, base_2=search_for_file_path ()
529/105:
df=pd.read_csv(path_file_2)
df.head(5)
df.dropna(axis=0,how='any',subset=['LAT_google','LON_google'],inplace=True)
len(df)
df.head(5)
529/106:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/107:
# figure = plt.figure(figsize=(10,12))
# plt.scatter(df.LON_google, df.LAT_google, s=15, c='goldenrod')
# plt.show()
529/108:
df_unicos=df.drop_duplicates(subset ="LON_google") 
len(df_unicos)
figure = plt.figure(figsize=(10,12))
plt.scatter(df_unicos.LON_google, df_unicos.LAT_google, s=15, c='goldenrod')
plt.show()
529/109:
sources=df_unicos.iloc[:,3:5].values.tolist()

distance_matrix = great_circle_distance_matrix(sources)

Matriz_dist=pd.DataFrame(distance_matrix)
#Matriz_dist.to_csv("matriz_dist.csv")
529/110:
new_order=[0]
distance=[0]
Bridge=Matriz_dist
for i in range(len(Matriz_dist)-1):
    #index=Bridge.index[i]
    pos=new_order[i]
    Bridge=Bridge.sort_values(pos)
    new_order.append(Bridge.index[1])
    distance.append(Bridge.iloc[1][pos])
    Bridge=Bridge.drop(Bridge.index[0])
    #print(new_order, len(Bridge))
529/111:
df_unicos['new_order']=new_order
df_unicos['distance']=distance
df_unicos=df_unicos.reset_index()
df_unicos
529/112:
dat1 = pd.DataFrame([])
for n in range(df_unicos.shape[0]):
    for m in range(df_unicos.shape[0]):
        if df_unicos.index[m] == new_order[n]:
            dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
529/113:
figure = plt.figure(figsize=(15,12))
plt.plot(dat1['LON'], dat1['LAT'], '-o')

for i in range(dat1.shape[0]):
    plt.text(x=dat1.LON.iloc[i], y=dat1.LAT.iloc[i], s=dat1.iloc[i][4] )
529/114:
from folium import plugins
lugares3=dat1
ff=len(lugares3)
# let's start again with a clean copy of the map of San Francisco
Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
    if type(lat)!=type(None):
        folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(Dots)
loc=lugares3.iloc[:,0:2]
loc=loc.values.tolist()
folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

# mini_map = plugins.MiniMap(toggle_display=True)
# # add the mini map to the big map
# Area.add_child(mini_map)
Area.save('Maps/Clean_maps/Maps_path/Map_path_' + base_2 +'.html')
#Area
# display map
#Area
529/115: dat1
529/116: dat1=path_dots(df, base_2)
529/117:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    print("YEAH")
    dat1.to_csv("Data/Cleaned_data/Path_" + Book_name + ".csv")
    return dat1
529/118: dat1=path_dots(df, base_2)
529/119: base_2
529/120:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Cleaned_data/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
529/121: dat1=path_dots(df, base_2)
529/122:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    #dat1.to_csv("Data/Cleaned_data/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
529/123: dat1=path_dots(df, base_2)
529/124:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return
529/125: plot_path(dat1, base_2)
529/126:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Cleaned_data/" + Book_name + ".csv")
    print("saved")
    return dat1
529/127: dat1=path_dots(df, base_2)
529/128:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
529/129: dat1=path_dots(df, base_2)
529/130:
df=pd.read_csv(path_file_2)
df.head(5)
529/131: places_result  = g_key.places_nearby(location=location_med, radius = 400000, open_now =False , type = 'tourist_attraction')
529/132:
#places on API
lat_med=df['LAT_google'].average(axis=0)
lon_med=df['LON_google'].average(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/133:
#places on API
lat_med=df['LAT_google'].avg(axis=0)
lon_med=df['LON_google'].avg(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/134:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
529/135: places_result  = g_key.places_nearby(location=location_med, radius = 100, open_now =False , type = 'lodging')
529/136: len(places_result)
529/137: places_result  = g_key.places_nearby(location=location_med, radius = 500, open_now =False , type = 'lodging')
529/138: len(places_result)
529/139: places_result  = g_key.places_nearby(location=location_med, radius = 1000, open_now =False , type = 'lodging')
529/140: len(places_result)
529/141: places_result  = g_key.places_nearby(location=location_med, radius = 1500, open_now =False , type = 'lodging')
529/142: len(places_result)
529/143: places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
529/144: len(places_result)
529/145: places_result  = g_key.places_nearby(location=location_med, radius = 3500, open_now =False , type = 'lodging')
529/146: len(places_result)
529/147: places_result  = g_key.places_nearby(location=location_med, radius = 4500, open_now =False , type = 'lodging')
529/148: len(places_result)
529/149: places_result  = g_key.places_nearby(location=location_med, radius = 41500, open_now =False , type = 'lodging')
529/150: len(places_result)
529/151: places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
529/152: len(places_result)
529/153: places_result
529/154: Data_POI=pd.DataFrame(places_result['results'])
529/155: Data_POI
529/156: len(Data_POI)
529/157: import time
529/158:
Data_POI=pd.DataFrame([])
for i in range(0,2):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    Data_POI.append(pd.DataFrame(places_result['results']))
    time.sleep(3)
529/159: len(Data_POI)
529/160:
Data_POI=pd.DataFrame([])
for i in range(0,2):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI.append(data)
    time.sleep(3)
529/161: len(Data_POI)
529/162: (Data_POI)
529/163:
Data_POI=pd.DataFrame([])
for i in range(0,2):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI=pd.concat(data)
    time.sleep(3)
529/164:
Data_POI=pd.DataFrame([])
for i in range(0,1):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI.append(data)
    time.sleep(3)
529/165: (Data_POI)
529/166:
(Data_POI)
data
529/167:
Data_POI=pd.DataFrame([])

for i in range(0,1):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI=Data_POI.append(data)
    time.sleep(3)
529/168: (Data_POI)
529/169: len(Data_POI)
529/170:
Data_POI=pd.DataFrame([])

for i in range(0,2):
    places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI=Data_POI.append(data)
    time.sleep(3)
529/171: len(Data_POI)
529/172:
Data_POI=pd.DataFrame([])

for i in range(0,3):
    dist_rad=250
    places_result  = g_key.places_nearby(location=location_med, radius = 500+dist_rad, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_POI=Data_POI.append(data)
    time.sleep(3)
529/173: len(Data_POI)
529/174:
#HOTELS
Data_Hotels=pd.DataFrame([])

for i in range(0,4):
    dist_rad=250
    places_result  = g_key.places_nearby(location=location_med, radius = 500+dist_rad, open_now =False , type = 'lodging')
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/175: len(Data_Hotels)
529/176: Data_Hotels
529/177: Data_Hotels.to_csv("prova_hotels.csv")
529/178:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 500, open_now =False , type = 'lodging')
#MAL---LO COPIA OTRA VEZ
for i in range(0,4):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/179:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
#MAL---LO COPIA OTRA VEZ
for i in range(0,4):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/180:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
#MAL---LO COPIA OTRA VEZ
for i in range(0,4):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/181:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,4):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/182:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,3):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/183:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,2):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/184: len(Data_Hotels)
529/185:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,3):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/186:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,2):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
529/187: len(Data_Hotels)
529/188: Data_Hotels.to_csv("prova_hotels.csv")
529/189: %history -g -f "Ultimo_update.txt"
541/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
541/2: path_file_2, base_2=search_for_file_path ()
541/3:
df=pd.read_csv(path_file_2)
df.head(5)
541/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/5:
lat_med=df['LAT'].median(axis=0)
lon_med=df['LON'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/6:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
541/7: dat1=path_dots(df, base_2)
541/8:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
541/9: dat1=path_dots(df, base_2)
541/10: path_file_2, base_2=search_for_file_path ()
541/11:
df=pd.read_csv(path_file_2)
df.head(5)
541/12:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/13:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Paths/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
541/14: dat1=path_dots(df, base_2)
541/15:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
541/16: dat1=path_dots(df, base_2)
541/17:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/18:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
plot_path(dat1, base_2)
541/19:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/20: import time
541/21:
#HOTELS
Data_Hotels=pd.DataFrame([])
places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
time.sleep(3)
#MAL---LO COPIA OTRA VEZ
for i in range(0,2):
    #dist_rad=250
    places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
    data=pd.DataFrame(places_result['results'])
    Data_Hotels=Data_Hotels.append(data)
    time.sleep(3)
541/22: len(Data_Hotels)
541/23: Data_Hotels
541/24:
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import requests
headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}
url = 'https://www.booking.com/searchresults.html?label=gen173nr-1FCAEoggI46AdIM1gEaGyIAQGYATG4AQfIAQzYAQHoAQH4AQKIAgGoAgO4AvTIm_IFwAIB;sid=7101b3fb6caa095b7b974488df1521d2;city=-2109472;from_idr=1&;dr_ps=IDR;ilp=1;d_dcp=1'
response=requests.get(url,headers=headers)
soup=BeautifulSoup(response.content,'lxml')
541/25:
import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import ssl
import json
import re
import sys
import warnings

if not sys.warnoptions:
    warnings.simplefilter("ignore")


#For ignoring SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

# url = input('Enter url - ' )
url=input("Enter Tripadvisor Hotel Url- ")
html = urllib.request.urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, 'html.parser')

html = soup.prettify("utf-8")
hotel_json = {}

for line in soup.find_all('script',attrs={"type" : "application/ld+json"}):
    details = line.text.strip()
    details = json.loads(details)
    hotel_json["name"] = details["name"]
    hotel_json["url"] = "https://www.tripadvisor.in"+details["url"]
    hotel_json["image"] = details["image"]
    details["priceRange"] = details["priceRange"].replace("₹ ","Rs ")
    details["priceRange"] = details["priceRange"].replace("₹","Rs ")
    hotel_json["priceRange"] = details["priceRange"]
    hotel_json["aggregateRating"]={}
    hotel_json["aggregateRating"]["ratingValue"]=details["aggregateRating"]["ratingValue"]
    hotel_json["aggregateRating"]["reviewCount"]=details["aggregateRating"]["reviewCount"]
    hotel_json["address"]={}
    hotel_json["address"]["Street"]=details["address"]["streetAddress"]
    hotel_json["address"]["Locality"]=details["address"]["addressLocality"]
    hotel_json["address"]["Region"]=details["address"]["addressRegion"]
    hotel_json["address"]["Zip"]=details["address"]["postalCode"]
    hotel_json["address"]["Country"]=details["address"]["addressCountry"]["name"]
    break
hotel_json["reviews"]=[]
for line in soup.find_all('p',attrs={"class" : "partial_entry"}):
    review = line.text.strip()
    if review != "":
        review = line.text.strip()
        if review.endswith("More"):
            review = review[:-4]
        if review.startswith("Dear"):
            continue
        review = review.replace('\r', ' ').replace('\n', ' ')
        review = ' '.join(review.split())
        hotel_json["reviews"].append(review)


with open(hotel_json["name"]+".html", "wb") as file:
    file.write(html)

with open(hotel_json["name"]+".json", 'w') as outfile:
    json.dump(hotel_json, outfile, indent=4)
541/26:
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
541/27:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
541/28:
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
541/29: get_nearby_places(location_med, 'lodging', '')
541/30:
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    #get_nearby_places(coordinates, business_type, next_page_token)
541/31: get_nearby_places(location_med, 'lodging', '')
541/32:
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)

        get_nearby_places(coordinates, business_type, next_page_token)


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/33:
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)



def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/34: get_nearby_places(location_med, 'lodging', '')
541/35:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)



def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/36: get_nearby_places(location_med, 'lodging', '')
541/37: Data_Hotels
541/38: Data_Hotels[0]
541/39: Data_Hotels.iloc[0]
541/40: Data_Hotels.iloc[0]["geometry"]
541/41: Data_Hotels.iloc[0]["geometry"]["location"]
541/42: Data_Hotels.iloc[0]["geometry"]["location"][0]
541/43: Data_Hotels.iloc[0]["geometry"]["location"]
541/44: Data_Hotels.iloc[0]["geometry"]["location"]["lat"]
541/45:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        place_lat=result["geometry"]["location"]["lat"]
        place_lon=result["geometry"]["location"]["lon"]
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)



def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/46: Data_Hotels.iloc[0]
541/47:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)



def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/48:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/49: get_nearby_places(location_med, 'lodging', '')
541/50: Data_Hotels.iloc["place_id"]
541/51: Data_Hotels.iloc[0["place_id"]
541/52: Data_Hotels.iloc[0]["place_id"]
541/53: data
541/54: data.iloc[i]["place_id"]
541/55: data.iloc[:]["place_id"]
541/56:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/57:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'website' in place_details:
            return place_details['website']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/58:
data_lala=[]
data_lala=get_nearby_places(location_med, 'lodging', '')
data_lala.append(data_lala)
541/59: data_lala
541/60:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'price_level' in place_details:
            return place_details['price_level']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/61:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/62:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'reviews[]' in place_details:
            return place_details['reviews[]']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/63:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/64:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'vicinity' in place_details:
            return place_details['vicinity']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/65:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/66:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&fields=price_level'+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'vicinity' in place_details:
            return place_details['vicinity']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/67:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&fields=price_level'+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'price_level' in place_details:
            return place_details['price_level']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/68:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/69:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&fields=price_level,formatted_phone_number'+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'price_level' in place_details:
            return place_details['price_level']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/70:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/71:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&fields=price_level,formatted_phone_number'+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'formatted_phone_number' in place_details:
            return place_details['formatted_phone_number']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/72:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/73:
total_results = []
def get_nearby_places(coordinates, business_type, next_page):
    URL = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location='
        +coordinates+'&radius=16093&key='+ api_key +'&type='
        +business_type+'&pagetoken='+next_page)
    r = requests.get(URL)
    response = r.text
    python_object = json.loads(response)
    results = python_object["results"]
    for result in results:
        place_name = result['name']
        place_id = result['place_id']
        # place_lat=result["geometry"]["location"]["lat"]
        # place_lon=result["geometry"]["location"]["lon"]
        #place_rating = result['rating']
        website = get_place_website(place_id)
        print([business_type, place_name, website])
        total_results.append([business_type, place_name, website])
    try:
        next_page_token = python_object["next_page_token"]
    except KeyError:
        #no next page
        return
    time.sleep(1)
    get_nearby_places(coordinates, business_type, next_page_token)
#return total_results


def get_place_website(place_id):
    reqURL = ('https://maps.googleapis.com/maps/api/place/details/json?placeid='
    +place_id+'&fields=price_level,formatted_phone_number'+'&key='+api_key)
    r = requests.get(reqURL)
    response = r.text
    python_object = json.loads(response)
    try:
        place_details = python_object["result"]
        if 'formatted_phone_number' in place_details:
            return place_details['price_level']
        else:
            return "no website listed in API"
    except:
        print("err getting place details")
541/74:
data_lala=[]
get_nearby_places(location_med, 'lodging', '')
#data_lala.append(data_lala)
541/75: api_key= GooglePlaces("AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
541/76:
import GooglePlaces
api_key= GooglePlaces("AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
541/77: GooglePlaces("AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
541/78:
from GooglePlaces import GooglePlaces
GooglePlaces("AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
541/79:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
541/80: api= GooglePlaces(api_key)
541/81: places = api.search_places_by_coordinate(location_med, radius = 2500, "lodging")
541/82: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
541/83: places
541/84: #places
541/85: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
541/86:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
541/87: details
541/88: details['price_level']
541/89: details['website']
541/90: details.iloc[0]
541/91: details.type()
541/92: type(details)
541/93: details
541/94: details["result"]
541/95: details["result"]["reviews"]
541/96:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    print("===================PLACE===================")
    print("Name:", name)
    print("Website:", website)
    print("Address:", address)
    print("Phone Number", phone_number)
    print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        profile_photo = review['profile_photo_url']
        print("Author Name:", author_name)
        print("Rating:", rating)
        print("Text:", text)
        print("Time:", time)
        print("Profile photo:", profile_photo)
        print("-----------------------------------------")
541/97: #details["result"]["reviews"]
541/98: places
541/99: #places
541/100: size(places)
541/101: places
541/102: pd.DataFrame(places)
541/103: pd.DataFrame(places)["geometry"]["location"]["lat"]
541/104: pd.DataFrame(places).iloc[0]["geometry"]["location"]
541/105: pd.DataFrame(places).iloc[0]["geometry"]["location"]["lat"]
541/106: places["locations"]
541/107: type(places)
541/108: places[0]
541/109: places[0][location]
541/110: places[0].location
541/111: places[0]["location"]
541/112: places[0]
541/113: places[0]["geometry"]
541/114: places[:]["geometry"]
541/115: places["geometry"]
541/116: places[0]["geometry"]
541/117: places[0:end]["geometry"]
541/118: places[0:1]["geometry"]
541/119: places[0]["geometry"]["location"]
541/120:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = details['result']['geometry']["location"]["lat"]
    except KeyError:
        lat = ""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    print("===================PLACE===================")
    print("Name:", name)
    print("Website:", website)
    print("Address:", address)
    print("Phone Number", phone_number)
    print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        profile_photo = review['profile_photo_url']
        print("Author Name:", author_name)
        print("Rating:", rating)
        print("Text:", text)
        print("Time:", time)
        print("Profile photo:", profile_photo)
        print("-----------------------------------------")
541/121:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = details['result']['geometry']["location"]["lat"]
    except KeyError:
        lat = ""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    print("===================PLACE===================")
    print("Name:", name)
    print("Website:", website)
    print("Address:", address)
    print("Phone Number", phone_number)
    print("lat", lat)
    print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        profile_photo = review['profile_photo_url']
        print("Author Name:", author_name)
        print("Rating:", rating)
        print("Text:", text)
        print("Time:", time)
        print("Profile photo:", profile_photo)
        print("-----------------------------------------")
541/122: details["result"]["reviews"]
541/123: details["result"]
541/124:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
    except KeyError:
        lat = ""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    print("===================PLACE===================")
    print("Name:", name)
    print("Website:", website)
    print("Address:", address)
    print("Phone Number", phone_number)
    print("lat", lat)
    print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        profile_photo = review['profile_photo_url']
        print("Author Name:", author_name)
        print("Rating:", rating)
        print("Text:", text)
        print("Time:", time)
        print("Profile photo:", profile_photo)
        print("-----------------------------------------")
541/125: #details["result"]
541/126: places[0]["rating"]
541/127: places[0]
541/128: places[0]["user_ratings_total"]
541/129:
Data_Hotels=pd.DataFrame([])
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=Data_Hotels.append(name)
    Data_Hotels["Website"]=Data_Hotels.append(website)
    Data_Hotels["Phone Number"]=Data_Hotels.append(phone_number)
    Data_Hotels["LON"]=Data_Hotels.append(lon)
    Data_Hotels["LAT"]=Data_Hotels.append(lat)
    Data_Hotels["rating"]=Data_Hotels.append(rating_total)
    Data_Hotels["Popularity"]=Data_Hotels.append(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=Data_Hotels.append(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/130:
Data_Hotels=pd.DataFrame([])
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=Data_Hotels.append(pd.DataFrame(name))
    Data_Hotels["Website"]=Data_Hotels.append(website)
    Data_Hotels["Phone Number"]=Data_Hotels.append(phone_number)
    Data_Hotels["LON"]=Data_Hotels.append(lon)
    Data_Hotels["LAT"]=Data_Hotels.append(lat)
    Data_Hotels["rating"]=Data_Hotels.append(rating_total)
    Data_Hotels["Popularity"]=Data_Hotels.append(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=Data_Hotels.append(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/131: name
541/132:
#Data_Hotels=pd.DataFrame([])
Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=Data_Hotels.append((name))
    Data_Hotels["Website"]=Data_Hotels.append(website)
    Data_Hotels["Phone Number"]=Data_Hotels.append(phone_number)
    Data_Hotels["LON"]=Data_Hotels.append(lon)
    Data_Hotels["LAT"]=Data_Hotels.append(lat)
    Data_Hotels["rating"]=Data_Hotels.append(rating_total)
    Data_Hotels["Popularity"]=Data_Hotels.append(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=Data_Hotels.append(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/133:
#Data_Hotels=pd.DataFrame([])
Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=Data_Hotels.append(website)
    Data_Hotels["Phone Number"]=Data_Hotels.append(phone_number)
    Data_Hotels["LON"]=Data_Hotels.append(lon)
    Data_Hotels["LAT"]=Data_Hotels.append(lat)
    Data_Hotels["rating"]=Data_Hotels.append(rating_total)
    Data_Hotels["Popularity"]=Data_Hotels.append(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=Data_Hotels.append(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/134:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=Data_Hotels.append(website)
    Data_Hotels["Phone Number"]=Data_Hotels.append(phone_number)
    Data_Hotels["LON"]=Data_Hotels.append(lon)
    Data_Hotels["LAT"]=Data_Hotels.append(lat)
    Data_Hotels["rating"]=Data_Hotels.append(rating_total)
    Data_Hotels["Popularity"]=Data_Hotels.append(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=Data_Hotels.append(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/135:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/136:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        Data_Hotels["Popularity"]=Data_Hotels.append(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/137:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review="Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/138:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=["Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text]
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/139:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: " + author_name + "; Rating: " + rating +"; When: " + time + "\n " + text)
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/140:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+author_name+"; Rating: "+rating+"; When: "+time+ "\n "+text)
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/141: Full_review=("Author: "+ author_name +"; Rating: "+ rating +"; When: "+time+ "\n "+text)
541/142: Full_review=("Author: "+ author_name)
541/143:
Full_review=("Author: "+ author_name)
Full_review
541/144:
Full_review=("Author: "+ author_name +"; Rating: "+ rating)
Full_review
541/145:
Full_review=("Author: "+ author_name + "; Rating: " + rating)
Full_review
541/146:
Full_review=("Author: "+ author_name + rating)
Full_review
541/147:
Full_review=("Author: "+ author_name + s(rating))
Full_review
541/148:
Full_review=("Author: "+ author_name + rating)
Full_review
541/149:
Full_review=("Author: "+ author_name + str(rating))
Full_review
541/150:
("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text)
Full_review
541/151:
("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
541/152:
Full_review("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
541/153:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
541/154:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels["Name"]=name
    Data_Hotels["Website"]=(website)
    Data_Hotels["Phone Number"]=(phone_number)
    Data_Hotels["LON"]=(lon)
    Data_Hotels["LAT"]=(lat)
    Data_Hotels["rating"]=(rating_total)
    Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text)
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/155: Data_Hotels
541/156: Data_Hotels["Last 5 reviews"]=(Full_review)
541/157:
Data_Hotels["Last 5 reviews"]=(Full_review)
Data_Hotels
541/158:
Data_Hotels["Last 5 reviews"]="lalala"
Data_Hotels
541/159:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    data_all= [{'Name': name, 'Website': website, 'Phone Number':
        phone_number}]
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text)
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/160:
Data_Hotels["Last 5 reviews"]="lalala"
data_all
541/161:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular}, index=[0]), ignore_index=True)
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text)
        Data_Hotels["Last 5 reviews"]=(Full_review)
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/162: Data_Hotels
541/163:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text)
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/164: Data_Hotels
541/165: Data_Hotels["Last 5 Reviews"]
541/166: Data_Hotels["Last 5 Reviews"].iloc[0]
541/167: lat
541/168: lon
541/169:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/170: Data_Hotels["Last 5 Reviews"].iloc[0]
541/171: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
541/172: pd.DataFrame(places).iloc[0]["geometry"]["location"]["lat"]
541/173:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/174: lon
541/175: Data_Hotels["Last 5 Reviews"].iloc[1]
541/176:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
        Full_review=append(Full_review)
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/177:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
        Full_review.append(Full_review)
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/178:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=Full_review + ("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/179:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=Full_review + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/180:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+text + "NEXT\n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/181: Data_Hotels["Last 5 Reviews"].iloc[1]
541/182:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]
        rating_total = place['rating']
        popular = place["user_ratings_total"]
    except KeyError:
        lat = ""
        lon= ""
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/183: Data_Hotels["Last 5 Reviews"].iloc[1]
541/184: place['geometry']["location"]["lat"]
541/185: popular = place["user_ratings_total"]
541/186: place["user_ratings_total"]
541/187: place['rating']
541/188:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]

     except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
     except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/190:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lon"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/191: lat
541/192:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    lat = place['geometry']["location"]["lat"]
    lon = place['geometry']["location"]["lon"]
    rating_total = place['rating']
    popular = place["user_ratings_total"]
    # try:
    #     lat = place['geometry']["location"]["lat"]
    #     lon = place['geometry']["location"]["lon"]

    # # except KeyError:
    # #     lat = ""
    # #     lon= ""

    # try:
    #     rating_total = place['rating']
    #     popular = place["user_ratings_total"]
   
    # except KeyError:
    #     rating_total=""
    #     popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/193: place['geometry']["location"]["lon"]
541/194: place['geometry']["location"]["lat"]
541/195: place['geometry']["location"]["lon"]
541/196: place['geometry']["location"]
541/197:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    # lat = place['geometry']["location"]["lat"]
    # lon = place['geometry']["location"]["lng"]
    # rating_total = place['rating']
    # popular = place["user_ratings_total"]
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    # except KeyError:
    #     lat = ""
    #     lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/198:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    # lat = place['geometry']["location"]["lat"]
    # lon = place['geometry']["location"]["lng"]
    # rating_total = place['rating']
    # popular = place["user_ratings_total"]
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + "NEXT \n ")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/199: lon
541/200: Data_Hotels
541/201: Data_Hotels.to_csv("Hoteles.csv")
541/202:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    # lat = place['geometry']["location"]["lat"]
    # lon = place['geometry']["location"]["lng"]
    # rating_total = place['rating']
    # popular = place["user_ratings_total"]
    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
    #print("===================PLACE===================")
    #print("Name:", name)
    
    # Data_Hotels["Name"]=name
    # Data_Hotels["Website"]=(website)
    # Data_Hotels["Phone Number"]=(phone_number)
    # Data_Hotels["LON"]=(lon)
    # Data_Hotels["LAT"]=(lat)
    # Data_Hotels["rating"]=(rating_total)
    # Data_Hotels["Popularity"]=(popular)
    # print("Website:", website)
    # print("Address:", address)
    # print("Phone Number", phone_number)
    #print("lat", lat)
    #print("==================REVIEWS==================")
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
        
        # print("Author Name:", author_name)
        # print("Rating:", rating)
        # print("Text:", text)
        # print("Time:", time)
        # print("Profile photo:", profile_photo)
        # print("-----------------------------------------")
541/203: Data_Hotels.to_csv("Hoteles.csv")
541/204: details
541/205:
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/206:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/207:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/208:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/209:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=str(label),
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/210:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/211: Area
541/212:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/213:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='home',
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/214:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/215:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            #popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/216:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='<i>Mt. Hood Meadows</i>',
            #popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/217:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='info-sign',
            #popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/218:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon='info-sign',
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/219:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/220:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], icon=DivIcon(
        icon_size=(150,36),
        icon_anchor=(7,20),
        html='<div style="font-size: 18pt; color : black">1</div>',
        )).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/221:
from folium import plugins
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], icon=DivIcon(icon_size=(150,36),
            icon_anchor=(7,20),
            html='<div style="font-size: 18pt; color : black">1</div>',
            )).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/222:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], icon=DivIcon(icon_size=(150,36),
            icon_anchor=(7,20),
            html='<div style="font-size: 18pt; color : black">1</div>',
            )).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/223:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], icon=DivIcon(icon_size=(50,36),
            icon_anchor=(7,20),
            html='<div style="font-size: 18pt; color : black">1</div>',
            )).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/224:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/225:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>Timberline Lodge</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/226:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + label + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/227:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + str(label) + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/228:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + str(label) + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/229:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + str(label) + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/230: label
541/231:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Rating"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + str(label) + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/232:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], popup='<b>' + str(label) + '</b>', icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/233:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/234:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=None,
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/235:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/236: Area
541/237:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/238:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/239:
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area
541/240: Area
541/241:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/242:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='red')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/243:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["LAT"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup='<b>' + str(label) + '</b>', 
            icon=folium.Icon(color='lightgray', icon='home', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/244:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon='home', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/245:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon='home', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/246: Area
541/247:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/248:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon='home', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/249: details
541/250: help(folium.Icon)
541/251:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon='<i class="fas fa-hotel"></i>', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/252:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="fas fa-hotel", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/253:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon_color="green", icon="fas fa-hotel", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/254:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="fas fa-hotel", color="green", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/255:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray',icon="fas fa-hotel", prefix='fa', color="green")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/256:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray',icon="fas fa-hotel", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/257:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="fas fa-hotel")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/258:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="fa-hotel"', prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/259:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="fa-hotel", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/260:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="hotel", prefix='fa')).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/261:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='lightgray', icon="hotel", prefix='fa', icon_color="green")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/262:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/263:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/264:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/265:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/266:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/267:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/268: Area
541/269:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/270:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/271: Area
541/272:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            #popup='<b>' + label + '</b>', 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/273: Data_Hotels["Name"]
541/274: Data_Hotels["Name"][0]
541/275: Data_Hotels["Name"]
541/276:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=label, 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/277: label
541/278:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/279:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    html = """
    <h1> This is a big popup</h1><br>
    With a few lines of code...
    <p>
    <code>
        from numpy import *<br>
        exp(-2*pi)
    </code>
    </p>
    """
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=html, 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/280:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    html = """
    <h1> This is a big popup</h1><br>
    With a few lines of code...
    <p>
    <code>
        from numpy import *<br>
        exp(-2*pi)
    </code>
    </p>
    """
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=html, 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/281:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    html = Data_Hotels["Name"].to_html(
    classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=html, 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/282:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    html = Data_Hotels.to_html(
    classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=html, 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/283:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=(label, parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/284: help(folium.Popup)
541/285:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=(str(label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/286:

Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(str(label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/287:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(str(label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/288:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>" + str(label) "</b>", parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/289:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>" + str(label) + "</b>", parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/290:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<br>" + str(label) + "<br>", parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/291:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>" + str(label) + "</b>", parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/292:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"]+ Data_Hotels["Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>" + str(label) + "</b>", parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/293:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label) + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/294:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label) + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/295:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/296:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/297:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/298:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( (label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/299: label2
541/300:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Last 5 Reviews"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( (label), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/301:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( (label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/302:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup( str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/303:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Rating: "+ str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/304:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/305:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "\n" + "Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/306:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "<br>Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/307:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "           Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/308:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
541/309:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
541/310: path_file_2, base_2=search_for_file_path ()
541/311:
df=pd.read_csv(path_file_2)
df.head(5)
541/312:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/313:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
541/314: dat1=path_dots(df, base_2)
541/315:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
541/316:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
541/317: Area
541/318:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
541/319: import time
541/320: api= GooglePlaces(api_key)
541/321: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
541/322: pd.DataFrame(places).iloc[0]["geometry"]["location"]["lat"]
541/323: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
541/324:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
541/325:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
541/326: Data_Hotels["Last 5 Reviews"].iloc[1]
541/327: Data_Hotels.to_csv("Hoteles.csv")
541/328:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
541/329:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/1: Data_Hotels["Last 5 Reviews"].iloc[1]
545/2:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
545/3: path_file_2, base_2=search_for_file_path ()
545/4:
df=pd.read_csv(path_file_2)
df.head(5)
545/5:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/6:
def path_dots(df, Book_name):
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/7: dat1=path_dots(df, base_2)
545/8:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["lugares"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/9:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/10: Area
545/11:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/12: import time
545/13:
# #HOTELS
# Data_Hotels=pd.DataFrame([])
# places_result  = g_key.places_nearby(location=location_med, radius = 2500, open_now =False , type = 'lodging')
# time.sleep(3)

# for i in range(0,2):
#     #dist_rad=250
#     places_result  = g_key.places_nearby(page_token=places_result['next_page_token'])
#     data=pd.DataFrame(places_result['results'])
#     Data_Hotels["id"]=Data_Hotels.append(data.iloc[i]["place_id"])
#     time.sleep(3)
545/14: api= GooglePlaces(api_key)
545/15: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
545/16: pd.DataFrame(places).iloc[0]["geometry"]["location"]["lat"]
545/17: places[0]["user_ratings_total"]
545/18: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/19: details
545/20:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/21:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/22: Data_Hotels["Last 5 Reviews"].iloc[1]
545/23: Data_Hotels.to_csv("Hoteles_"+ base2 + ".csv")
545/24: Data_Hotels.to_csv("Hoteles_"+ base_2 + ".csv")
545/25:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
545/26:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/27:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"]):
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/28:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/29: Area
545/30:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"]):
        label_2=lugares3["lugares"]
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label+label_2,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/31:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/32: Area
545/33:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label+label_2,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/34:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/35: Area
545/36:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=label+ "\n"+ label_2,
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/37:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/38: Area
545/39:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=(label+ "\n" + label_2),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/40:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/41: Area
545/42:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=(label+ "<br>" + label_2),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/43:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/44: Area
545/45:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=(label_2 + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/46:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/47: Area
545/48:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"<\b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/49:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/50: Area
545/51:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/52:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/53: Area
545/54:
df=pd.read_csv(path_file_2)
size(df)
545/55:
df=pd.read_csv(path_file_2)
size.df
545/56:
df=pd.read_csv(path_file_2)
len(df)
545/57:
df=pd.read_csv(path_file_2)
df
545/58:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ (label) + "_________Rating: " + (label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/59:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/60:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>Name: </b>"+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/61:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>Name: </b>"+ str(label) + "_________Rating: " + str(label2), parse_html=False), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/62:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("<b>Name: </b>"+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/63: label
545/64:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/65: hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
545/66:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh
545/67:
hh=df.groupby(['lugares','LAT_google' ,'LON_google']).agg(lambda col: '\n'.join(col))
hh
545/68:
hh=df.groupby(['LAT_google' ,'LON_google']).agg(lambda col: '\n'.join(col))
hh
545/69:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh
545/70:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh['Quotes']
545/71:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh['Quotes'][0]
545/72:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh['Quotes'].iloc[0]
545/73:
def path_dots(df, Book_name):
    hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos["Quotes"]=hh["Quotes"]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/74: dat1=path_dots(df, base_2)
545/75:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/76:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/77: Area
545/78: dat1
545/79:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
hh['Quotes']
545/80:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
df_unicos=df.drop_duplicates(subset ="LON_google") 
df_unicos["Quotes"]=hh["Quotes"]
545/81:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
df_unicos=df.drop_duplicates(subset ="LON_google") 
df_unicos["Quotes"]=hh["Quotes"]
df_unicos
545/82:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/83: dat1=path_dots(df, base_2)
545/84: dat1
545/85:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/86:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/87: Area
545/88: hh=df.groupby(['LON_google'])
545/89:
hh=df.groupby(['LON_google'])
hh
545/90:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','quotes_merged':lambda x : '\n'.join(set(x)),'text2':lambda x : ','.join(set(x))}).reset_index()
s
545/91:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum':lambda x : '\n'.join(set(x)),'quotes_merged':lambda x : ','.join(set(x))}).reset_index()
s
545/92:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',:lambda x : '\n'.join(set(x)),'quotes_merged':lambda x : ','.join(set(x))}).reset_index()
s
545/93:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','text1':lambda x : ','.join(set(x)),'text2':lambda x : ','.join(set(x))}).reset_index()
s
s
545/94:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','text1':lambda x : ','.join(set(x)),'text2':lambda x : ','.join(set(x))}).reset_index()
s
545/95:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : ','.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()
s
545/96:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : ','.join(set(x)),'Position':lambda x : ','.join(set(x))}).reset_index()
s
545/97:
hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'Position':lambda x : ','.join(set(x))}).reset_index()
s
545/98:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()
s
545/99:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()
len(s)
545/100:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()
len(s)
len(dat1)
545/101:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()
len(s)
len(dat1)
df_unicos=df.drop_duplicates(subset ="LON_google")
545/102:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '\n'.join(set(x)),'labels':lambda x : ','.join(set(x))}).reset_index()

df_unicos=df.drop_duplicates(subset ="LON_google") 
s
545/103: df_unicos
545/104: df_unicos["Quotes"].iloc[0]
545/105: s["Quotes"].iloc[0]
545/106:
for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes"].iloc[n]
545/107: df_unicos["Quotes"].iloc[0]
545/108: df_unicos["Quotes"]
545/109: df_unicos
545/110:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '<br>'.join(set(x)),'Position':lambda x : ','.join(set(x))}).reset_index()

df_unicos=df.drop_duplicates(subset ="LON_google") 
s
545/111:
#hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes':lambda x : '<br>'.join(set(x))}).reset_index()

df_unicos=df.drop_duplicates(subset ="LON_google") 
s
545/112: df.Position
545/113: type(df.Position.iloc[0)]
545/114: type(df.Position.iloc[0]
545/115: type(df.Position.iloc[0])
545/116: str(df.Position.iloc[0])
545/117: str(df.Position)
545/118: df['Position'].astype(str)
545/119: df["Quotes_total"]="<b>Pos. in book: </b>" + df['Position'].astype(str) + "<br>" + df['Quotes']
545/120:
df["Quotes_total"]="<b>Pos. in book: </b>" + df['Position'].astype(str) + "<br>" + df['Quotes']

df["Quotes_total"]
545/121:
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']

df["Quotes_total"]
545/122:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/123: dat1=path_dots(df, base_2)
545/124: dat1
545/125:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/126: dat1=path_dots(df, base_2)
545/127: dat1
545/128:
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']

df["Quotes_total"]
df
545/129:
df=pd.read_csv(path_file_2)
df
545/130:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/131:
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']

df
545/132:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/133: dat1=path_dots(df, base_2)
545/134: dat1
545/135:
df=pd.read_csv(path_file_2)
df
545/136:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/137: dat1=path_dots(df, base_2)
545/138: dat1
545/139:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/140:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/141: Area
545/142:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/143: import time
545/144: api= GooglePlaces(api_key)
545/145: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
545/146: pd.DataFrame(places).iloc[0]["geometry"]["location"]["lat"]
545/147: places[0]["user_ratings_total"]
545/148: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/149: details
545/150:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/151:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/152: Data_Hotels["Last 5 Reviews"].iloc[1]
545/153: Data_Hotels.to_csv("Hoteles_"+ base_2 + ".csv")
545/154:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
545/155:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/156: path_file_2, base_2=search_for_file_path ()
545/157: path_file_2, base_2=search_for_file_path ()
545/158:
df=pd.read_csv(path_file_2)
df
545/159:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/160:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
545/161: dat1=path_dots(df, base_2)
545/162: dat1
545/163:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
545/164:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
545/165: Area
545/166:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
545/167: import time
545/168: api= GooglePlaces(api_key)
545/169: places = api.search_places_by_coordinate(location_med, "2500", "lodging")
545/170: places[0]["user_ratings_total"]
545/171: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/172: details
545/173:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/174:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/175: Data_Hotels["Last 5 Reviews"].iloc[1]
545/176: Data_Hotels.to_csv("Hoteles_"+ base_2 + ".csv")
545/177:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
545/178:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)
    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)  
        Data_Hotels.to_csv("Hoteles_"+ base_2 + ".csv")

    return Data_Hotels
545/179:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)
    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)  
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
545/180: Places_API=GetPlaces(api_key, location_med, "restaurants")
545/181:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)
    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)  
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
545/182: Places_API=GetPlaces(api_key, location_med, "lodging")
545/183: Places_API, det=GetPlaces(api_key, location_med, "lodging")
545/184:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)  
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels, details
545/185: Places_API, det=GetPlaces(api_key, location_med, "lodging")
545/186: Places_API, det=GetPlaces(api_key, location_med, "restaurants")
545/187: det
545/188: places = api.search_places_by_coordinate(location_med, "2500", "restaurants")
545/189: places
545/190: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/191:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/192: details
545/193:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        #  Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
        #         'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
        #         'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
        
        #Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels, details
545/194: Places_API, det=GetPlaces(api_key, location_med, "restaurants")
545/195: details
545/196: det
545/197:
breakpoint()

# airport
# amusement_park
# aquarium
# art_gallery

# bakery
# bank
# bar
# beauty_salon
# bicycle_store
# book_store
# bowling_alley
# bus_station
# cafe
# campground
# car_dealer
# car_rental
# car_repair
# car_wash
# casino
# cemetery
# church
# city_hall
# clothing_store
# convenience_store
# courthouse
# dentist
# department_store
# doctor
# drugstore
# electrician
# electronics_store
# embassy
# fire_station
# florist
# funeral_home
# furniture_store
# gas_station
# gym
# hair_care
# hardware_store
# hindu_temple
# home_goods_store
# hospital
# insurance_agency
# jewelry_store
# laundry
# lawyer
# library
# light_rail_station
# liquor_store
# local_government_office
# locksmith
# lodging
# meal_delivery
# meal_takeaway
# mosque
# movie_rental
# movie_theater
# moving_company
# museum
# night_club
# painter
# park
# parking
# pet_store
# pharmacy
# physiotherapist
# plumber
# police
# post_office
# primary_school
# real_estate_agency
# restaurant
# roofing_contractor
# rv_park
# school
# secondary_school
# shoe_store
# shopping_mall
# spa
# stadium
# storage
# store
# subway_station
# supermarket
# synagogue
# taxi_stand
# tourist_attraction
# train_station
# transit_station
# travel_agency
# university
# veterinary_care
# zoo
545/198: Places_API
545/199: det
545/200: Places_API, det=GetPlaces(api_key, location_med, "art_gallery")
545/201: det
545/202:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
         Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
        
        #Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels, details
545/204: Places_API, det=GetPlaces(api_key, location_med, "art_gallery")
545/205:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
            popular = place["user_ratings_total"]
    
        except KeyError:
            rating_total=""
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
        
        #Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels, details
545/206: Places_API, det=GetPlaces(api_key, location_med, "art_gallery")
545/207: Places_API, det=GetPlaces(api_key, location_med, "lodging)
545/208: det
545/209: Places_API, det=GetPlaces(api_key, location_med, "lodging")
545/210: det
545/211: Places_API
545/212: Places_API, det=GetPlaces(api_key, location_med, "restaurants")
545/213: Places_API
545/214: places = api.search_places_by_coordinate(location_med, "2500", "restaurants")
545/215: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/216:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/217:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
        popular = place["user_ratings_total"]
   
    except KeyError:
        rating_total=""
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/218: rating
545/219: popular
545/220:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/221: popular
545/222: Full_review
545/223: text
545/224: time
545/225: rating
545/226: rating_total
545/227: place
545/228: places
545/229: popular
545/230: places
545/231: Places_API, det=GetPlaces(api_key, location_med, "tourist_attraction")
545/232: Places_API
545/233: Places_API.sort_values("Popularity")
545/234: Places_API.sort_values("Popularity", ascending=False)
545/235:
#"tourist_attraction"
#"lodging"
Places_API, det=GetPlaces(api_key, location_med, "restaurants")
Places_API.sort_values("Popularity", ascending=False)
545/236: places = api.search_places_by_coordinate(location_med, "2500", "restaurants")
545/237: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/238:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/239:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
545/240: details[0]
545/241: details.iloc[0]
545/242: details
545/243: places
545/244: places[0]
545/245: places[1]
545/246: pd.DataFrame(places)
545/247: places = api.search_places_by_coordinate(location_med, "2500", "Restaurants")
545/248: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/249:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/250: pd.DataFrame(places)
545/251: places = api.search_places_by_coordinate(location_med, "2500", "cafe")
545/252: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/253:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/254: pd.DataFrame(places)
545/255: places = api.search_places_by_coordinate(location_med, "2500", "food")
545/256: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/257:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/258: pd.DataFrame(places)
545/259: places = api.search_places_by_coordinate(location_med, "2500", "bar")
545/260: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
545/261:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
545/262: pd.DataFrame(places)
545/263: pd.DataFrame(places).sort_values("user_ratings_total", ascending=False)
545/264:
#"tourist_attraction"
#"lodging"
#"bar"
Places_API, det=GetPlaces(api_key, location_med, "bar")
Places_API.sort_values("Popularity", ascending=False)
545/265:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " <br> "+text + 
            "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
545/266:
from Tkinter import *
import Tkinter as ttk 
from ttk import *

root = Tk()
root.title("Age Selector")

mainframe = Frame(root)                                 
mainframe.grid(column=0,row=0, sticky=(N,W,E,S) )
mainframe.columnconfigure(0, weight = 1)
mainframe.rowconfigure(0, weight = 1)
mainframe.pack(pady = 10, padx = 10)

var = StringVar(root)

# Use dictionary to map names to ages.
choices = {
    'Bob': '35',
    'Garry': '45',
    'John': '32',
    'Hank': '64',
    'Tyrone': '21',
}

option = OptionMenu(mainframe, var, *choices)
var.set('Bob')

option.grid(row = 1, column =1)

Label(mainframe, text="Age").grid(row = 2, column = 1)

age = StringVar()
# Bind age instead of var
age_ent = Entry(mainframe, text=age, width = 15).grid(column = 2, row = 2)

# change_age is called on var change.
def change_age(*args):
    age_ = choices[var.get()]
    age.set(age_)
# trace the change of var
var.trace('w', change_age)

root.mainloop()
545/267:
from tkinter import *
import tkinter as ttk 
from ttk import *

root = Tk()
root.title("Age Selector")

mainframe = Frame(root)                                 
mainframe.grid(column=0,row=0, sticky=(N,W,E,S) )
mainframe.columnconfigure(0, weight = 1)
mainframe.rowconfigure(0, weight = 1)
mainframe.pack(pady = 10, padx = 10)

var = StringVar(root)

# Use dictionary to map names to ages.
choices = {
    'Bob': '35',
    'Garry': '45',
    'John': '32',
    'Hank': '64',
    'Tyrone': '21',
}

option = OptionMenu(mainframe, var, *choices)
var.set('Bob')

option.grid(row = 1, column =1)

Label(mainframe, text="Age").grid(row = 2, column = 1)

age = StringVar()
# Bind age instead of var
age_ent = Entry(mainframe, text=age, width = 15).grid(column = 2, row = 2)

# change_age is called on var change.
def change_age(*args):
    age_ = choices[var.get()]
    age.set(age_)
# trace the change of var
var.trace('w', change_age)

root.mainloop()
545/268:
from tkinter import *
import tkinter as ttk 
#from ttk import *

root = Tk()
root.title("Age Selector")

mainframe = Frame(root)                                 
mainframe.grid(column=0,row=0, sticky=(N,W,E,S) )
mainframe.columnconfigure(0, weight = 1)
mainframe.rowconfigure(0, weight = 1)
mainframe.pack(pady = 10, padx = 10)

var = StringVar(root)

# Use dictionary to map names to ages.
choices = {
    'Bob': '35',
    'Garry': '45',
    'John': '32',
    'Hank': '64',
    'Tyrone': '21',
}

option = OptionMenu(mainframe, var, *choices)
var.set('Bob')

option.grid(row = 1, column =1)

Label(mainframe, text="Age").grid(row = 2, column = 1)

age = StringVar()
# Bind age instead of var
age_ent = Entry(mainframe, text=age, width = 15).grid(column = 2, row = 2)

# change_age is called on var change.
def change_age(*args):
    age_ = choices[var.get()]
    age.set(age_)
# trace the change of var
var.trace('w', change_age)

root.mainloop()
545/269:
from tkinter import *
import tkinter as ttk 
#from ttk import *

root = Tk()
root.title("Age Selector")

mainframe = Frame(root)                                 
mainframe.grid(column=0,row=0, sticky=(N,W,E,S) )
mainframe.columnconfigure(0, weight = 1)
mainframe.rowconfigure(0, weight = 1)
mainframe.pack(pady = 10, padx = 10)

var = StringVar(root)

# Use dictionary to map names to ages.
choices = {
    'Bob': '35',
    'Garry': '45',
    'John': '32',
    'Hank': '64',
    'Tyrone': '21',
}

option = OptionMenu(mainframe, var, *choices)
var.set('Bob')

option.grid(row = 1, column =1)

Label(mainframe, text="Age").grid(row = 2, column = 1)

age = StringVar()
# Bind age instead of var
age_ent = Entry(mainframe, text=age, width = 15).grid(column = 2, row = 2)

# change_age is called on var change.
def change_age(*args):
    age_ = choices[var.get()]
    age.set(age_)
# trace the change of var
var.trace('w', change_age)

root.mainloop()
545/270:
from tkinter import *
import tkinter as ttk 
#from ttk import *

root = Tk()
root.title("Age Selector")

mainframe = Frame(root)                                 
mainframe.grid(column=0,row=0, sticky=(N,W,E,S) )
mainframe.columnconfigure(0, weight = 1)
mainframe.rowconfigure(0, weight = 1)
mainframe.pack(pady = 10, padx = 10)

var = StringVar(root)

# Use dictionary to map names to ages.
choices = {
    'Bob': '35',
    'Garry': '45',
    'John': '32',
    'Hank': '64',
    'Tyrone': '21',
}

option = OptionMenu(mainframe, var, *choices)
var.set('Bob')

option.grid(row = 1, column =1)

Label(mainframe, text="Age").grid(row = 2, column = 1)

age = StringVar()
# Bind age instead of var
age_ent = Entry(mainframe, text=age, width = 15).grid(column = 2, row = 2)

# change_age is called on var change.
def change_age(*args):
    age_ = choices[var.get()]
    age.set(age_)
# trace the change of var
var.trace('w', change_age)

root.mainloop()
545/271:
# python program demonstrating 
# Combobox widget using tkinter 
  
  
import tkinter as tk 
from tkinter import ttk 
  
# Creating tkinter window 
window = tk.Tk() 
window.title('Combobox') 
window.geometry('500x250') 
  
# label text for title 
ttk.Label(window, text = "GFG Combobox Widget",  
          background = 'green', foreground ="white",  
          font = ("Times New Roman", 15)).grid(row = 0, column = 1) 
  
# label 
ttk.Label(window, text = "Select the Month :", 
          font = ("Times New Roman", 10)).grid(column = 0, 
          row = 5, padx = 10, pady = 25) 
  
# Combobox creation 
n = tk.StringVar() 
monthchoosen = ttk.Combobox(window, width = 27, textvariable = n) 
  
# Adding combobox drop down list 
monthchoosen['values'] = (' January',  
                          ' February', 
                          ' March', 
                          ' April', 
                          ' May', 
                          ' June', 
                          ' July', 
                          ' August', 
                          ' September', 
                          ' October', 
                          ' November', 
                          ' December') 
  
monthchoosen.grid(column = 1, row = 5) 
monthchoosen.current() 
window.mainloop()
545/272: monthchoosen
545/273:
# python program demonstrating 
# Combobox widget using tkinter 
  
  
import tkinter as tk 
from tkinter import ttk 
  
# Creating tkinter window 
window = tk.Tk() 
window.title('Combobox') 
window.geometry('500x250') 
  
# label text for title 
ttk.Label(window, text = "GFG Combobox Widget",  
          background = 'green', foreground ="white",  
          font = ("Times New Roman", 15)).grid(row = 0, column = 1) 
  
# label 
ttk.Label(window, text = "Select the Month :", 
          font = ("Times New Roman", 10)).grid(column = 0, 
          row = 5, padx = 10, pady = 25) 
  
# Combobox creation 
n = tk.StringVar() 
monthchoosen = ttk.Combobox(window, width = 27, textvariable = n) 
  
# Adding combobox drop down list 
monthchoosen['values'] = (' January',  
                          ' February', 
                          ' March', 
                          ' April', 
                          ' May', 
                          ' June', 
                          ' July', 
                          ' August', 
                          ' September', 
                          ' October', 
                          ' November', 
                          ' December') 
  
monthchoosen.grid(column = 1, row = 5) 
monthchoosen.current() 
window.mainloop()
545/274: monthchoosen
545/275:
# python program demonstrating 
# Combobox widget using tkinter 
  
  
import tkinter as tk 
from tkinter import ttk 
  
# Creating tkinter window 
window = tk.Tk() 
window.title('Combobox') 
window.geometry('500x250') 
  
# label text for title 
ttk.Label(window, text = "GFG Combobox Widget",  
          background = 'green', foreground ="white",  
          font = ("Times New Roman", 15)).grid(row = 0, column = 1) 
  
# label 
ttk.Label(window, text = "Select the Month :", 
          font = ("Times New Roman", 10)).grid(column = 0, 
          row = 5, padx = 10, pady = 25) 
  
# Combobox creation 
n = tk.StringVar() 
monthchoosen = ttk.Combobox(window, width = 27, textvariable = n) 
  
# Adding combobox drop down list 
monthchoosen['values'] = (' January',  
                          ' February', 
                          ' March', 
                          ' April', 
                          ' May', 
                          ' June', 
                          ' July', 
                          ' August', 
                          ' September', 
                          ' October', 
                          ' November', 
                          ' December') 
  
monthchoosen.grid(column = 1, row = 5) 
monthchoosen.current() 
window.mainloop()
545/276:
import dash
import dash_html_components as html
import dash_core_components as dcc

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div([
    dcc.Dropdown(
        id='demo-dropdown',
        options=[
            {'label': 'New York City', 'value': 'NYC'},
            {'label': 'Montreal', 'value': 'MTL'},
            {'label': 'San Francisco', 'value': 'SF'}
        ],
        value='NYC'
    ),
    html.Div(id='dd-output-container')
])


@app.callback(
    dash.dependencies.Output('dd-output-container', 'children'),
    [dash.dependencies.Input('demo-dropdown', 'value')])
def update_output(value):
    return 'You have selected "{}"'.format(value)


if __name__ == '__main__':
    app.run_server(debug=True)
545/277:
import dash
import dash_html_components as html
import dash_core_components as dcc

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div([
    dcc.Dropdown(
        id='demo-dropdown',
        options=[
            {'label': 'New York City', 'value': 'NYC'},
            {'label': 'Montreal', 'value': 'MTL'},
            {'label': 'San Francisco', 'value': 'SF'}
        ],
        value='NYC'
    ),
    html.Div(id='dd-output-container')
])


@app.callback(
    dash.dependencies.Output('dd-output-container', 'children'),
    [dash.dependencies.Input('demo-dropdown', 'value')])
def update_output(value):
    return 'You have selected "{}"'.format(value)


if __name__ == '__main__':
    app.run_server(debug=True)
545/278:
import dash
import dash_html_components as html
import dash_core_components as dcc

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div([
    dcc.Dropdown(
        id='demo-dropdown',
        options=[
            {'label': 'New York City', 'value': 'NYC'},
            {'label': 'Montreal', 'value': 'MTL'},
            {'label': 'San Francisco', 'value': 'SF'}
        ],
        value='NYC'
    ),
    html.Div(id='dd-output-container')
])


@app.callback(
    dash.dependencies.Output('dd-output-container', 'children'),
    [dash.dependencies.Input('demo-dropdown', 'value')])
def update_output(value):
    return 'You have selected "{}"'.format(value)


if __name__ == '__main__':
    app.run_server(debug=True)
545/279:
import dash
import dash_html_components as html
import dash_core_components as dcc

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div([
    dcc.Dropdown(
        id='demo-dropdown',
        options=[
            {'label': 'New York City', 'value': 'NYC'},
            {'label': 'Montreal', 'value': 'MTL'},
            {'label': 'San Francisco', 'value': 'SF'}
        ],
        value='NYC'
    ),
    html.Div(id='dd-output-container')
])


@app.callback(
    dash.dependencies.Output('dd-output-container', 'children'),
    [dash.dependencies.Input('demo-dropdown', 'value')])
def update_output(value):
    return 'You have selected "{}"'.format(value)


if __name__ == '__main__':
    app.run_server(debug=True)
545/280:
from tkinter import *

REPORTS = [
"Alle Mitarbeiter",
"Alle Projekte",
"Alle Skills"
]


ReportSelection_Win = Tk()

variable = StringVar(ReportSelection_Win)
variable.set(REPORTS[0]) # default value

Lbl_Headline = Label(ReportSelection_Win, text = "Bitte wählen Sie einen Report")#Create Label
Lbl_Headline.grid(column=0, row=0, padx=10, pady=10) #Show Label

Drop_Reports = OptionMenu(ReportSelection_Win, variable, *REPORTS)
Drop_Reports.grid(column=0, row=1, padx=10, pady=0)

def Select_Report():
    global Selected_Report
    Selected_Report = variable.get()
    ReportSelection_Win.destroy()

Btt_Confirm_2 = Button(ReportSelection_Win, text="Auswählen", command=Select_Report)
Btt_Confirm_2.grid(column=0, row=2, padx=10, pady=10)



ReportSelection_Win.mainloop()


#----------------------------Selected Report-----------------------------------

print(Selected_Report)
545/281:
from tkinter import *

REPORTS = [
"Alle Mitarbeiter",
"Alle Projekte",
"Alle Skills"
]


ReportSelection_Win = Tk()

variable = StringVar(ReportSelection_Win)
variable.set(REPORTS[0]) # default value

Lbl_Headline = Label(ReportSelection_Win, text = "Bitte wählen Sie einen Report")#Create Label
Lbl_Headline.grid(column=0, row=0, padx=10, pady=10) #Show Label

Drop_Reports = OptionMenu(ReportSelection_Win, variable, *REPORTS)
Drop_Reports.grid(column=0, row=1, padx=10, pady=0)

def Select_Report():
    global Selected_Report
    Selected_Report = variable.get()
    ReportSelection_Win.destroy()

Btt_Confirm_2 = Button(ReportSelection_Win, text="Auswählen", command=Select_Report)
Btt_Confirm_2.grid(column=0, row=2, padx=10, pady=10)



ReportSelection_Win.mainloop()


#----------------------------Selected Report-----------------------------------

print(Selected_Report)
545/282:
from tkinter import *

REPORTS = [
"Alle Mitarbeiter",
"Alle Projekte",
"Alle Skills"
]


ReportSelection_Win = Tk()

variable = StringVar(ReportSelection_Win)
variable.set(REPORTS[0]) # default value

Lbl_Headline = Label(ReportSelection_Win, text = "Bitte wählen Sie einen Report")#Create Label
Lbl_Headline.grid(column=0, row=0, padx=10, pady=10) #Show Label

Drop_Reports = OptionMenu(ReportSelection_Win, variable, *REPORTS)
Drop_Reports.grid(column=0, row=1, padx=10, pady=0)

def Select_Report():
    global Selected_Report
    Selected_Report = variable.get()
    ReportSelection_Win.destroy()

Btt_Confirm_2 = Button(ReportSelection_Win, text="Auswählen", command=Select_Report)
Btt_Confirm_2.grid(column=0, row=2, padx=10, pady=10)



ReportSelection_Win.mainloop()


#----------------------------Selected Report-----------------------------------

print(Selected_Report)
545/283:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "bar"):
545/284: import time
545/285:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " <br> "+text + 
            "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
545/286:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "bar"):
545/287:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "bar")
545/288: API_values
545/289: API_values.head(10)
545/290:
from tkinter import *

REPORTS = [
"Alle Mitarbeiter",
"Alle Projekte",
"Alle Skills"
]


ReportSelection_Win = Tk()

variable = StringVar(ReportSelection_Win)
variable.set(REPORTS[0]) # default value

Lbl_Headline = Label(ReportSelection_Win, text = "Bitte wählen Sie einen Report")#Create Label
Lbl_Headline.grid(column=0, row=0, padx=10, pady=10) #Show Label

Drop_Reports = OptionMenu(ReportSelection_Win, variable, *REPORTS)
Drop_Reports.grid(column=0, row=1, padx=10, pady=0)

def Select_Report():
    global Selected_Report
    Selected_Report = variable.get()
    ReportSelection_Win.destroy()

Btt_Confirm_2 = Button(ReportSelection_Win, text="Auswählen", command=Select_Report)
Btt_Confirm_2.grid(column=0, row=2, padx=10, pady=10)



ReportSelection_Win.mainloop()


#----------------------------Selected Report-----------------------------------

print(Selected_Report)
545/291:
from tkinter import *
from tkinter import ttk
root=Tk()
def new_window():
    t3 = Toplevel(root)
    t3.geometry('240x100+20+20')
    t3.title("...")
    Label(t3,text="I hope to help you").pack()
    Button(t3,text="destroy() in t3 ",command=t3.destroy).pack()
canvas_c=Canvas(root, width=400, height=400)
canvas_c.pack()
canvas_c.config(bg="blue")
Label(canvas_c,text="info").place(x=100,y=250)

ba=Button(root,text="new_window",command=new_window).pack()
bb=Button(root,text="destroy() in canvas",command=canvas_c.destroy).pack()
root.mainloop()
545/292:
from tkinter import *
from tkinter import ttk
root=Tk()
def new_window():
    t3 = Toplevel(root)
    t3.geometry('240x100+20+20')
    t3.title("...")
    Label(t3,text="I hope to help you").pack()
    Button(t3,text="destroy() in t3 ",command=t3.destroy).pack()
canvas_c=Canvas(root, width=400, height=400)
canvas_c.pack()
canvas_c.config(bg="blue")
Label(canvas_c,text="info").place(x=100,y=250)

ba=Button(root,text="new_window",command=new_window).pack()
bb=Button(root,text="destroy() in canvas",command=canvas_c.destroy).pack()
root.mainloop()
545/293:
from tkinter import *
from tkinter import ttk
root=Tk()
def new_window():
    t3 = Toplevel(root)
    t3.geometry('240x100+20+20')
    t3.title("...")
    Label(t3,text="I hope to help you").pack()
    Button(t3,text="destroy() in t3 ",command=t3.destroy).pack()
canvas_c=Canvas(root, width=400, height=400)
canvas_c.pack()
canvas_c.config(bg="blue")
Label(canvas_c,text="info").place(x=100,y=250)

ba=Button(root,text="new_window",command=new_window).pack()
bb=Button(root,text="destroy() in canvas",command=canvas_c.destroy).pack()
root.mainloop()
545/294: #
545/295: #
545/296: API_values.head(10)
545/297: API_values.head(10)
545/298: #
545/299:
from tkinter import *
from tkinter import ttk
root=Tk()
def new_window():
    t3 = Toplevel(root)
    t3.geometry('240x100+20+20')
    t3.title("...")
    Label(t3,text="I hope to help you").pack()
    Button(t3,text="destroy() in t3 ",command=t3.destroy).pack()
canvas_c=Canvas(root, width=400, height=400)
canvas_c.pack()
canvas_c.config(bg="blue")
Label(canvas_c,text="info").place(x=100,y=250)

ba=Button(root,text="new_window",command=new_window).pack()
bb=Button(root,text="destroy() in canvas",command=canvas_c.destroy).pack()
root.mainloop()
545/300:
from Tkinter import *

master = Tk()
Label(master, text="First Name").grid(row=0)
Label(master, text="Last Name").grid(row=1)

e1 = Entry(master)
e2 = Entry(master)

e1.grid(row=0, column=1)
e2.grid(row=1, column=1)

mainloop( )
545/301:
from tkinter import *

master = Tk()
Label(master, text="First Name").grid(row=0)
Label(master, text="Last Name").grid(row=1)

e1 = Entry(master)
e2 = Entry(master)

e1.grid(row=0, column=1)
e2.grid(row=1, column=1)

mainloop( )
545/302:
from Tkinter import *

def printData(firstName, lastName):
    print(firstName)
    print(lastName)
    root.destroy()

def get_input():

    firstName = entry1.get()
    lastName = entry2.get()
    printData(firstName, lastName)


root = Tk()
#Label 1
label1 = Label(root,text = 'First Name')
label1.pack()
label1.config(justify = CENTER)

entry1 = Entry(root, width = 30)
entry1.pack()

label3 = Label(root, text="Last Name")
label3.pack()
label1.config(justify = CENTER)

entry2 = Entry(root, width = 30)
entry2.pack()

button1 = Button(root, text = 'submit')
button1.pack() 
button1.config(command = get_input)

root.mainloop()
545/303:
from tkinter import *

def printData(firstName, lastName):
    print(firstName)
    print(lastName)
    root.destroy()

def get_input():

    firstName = entry1.get()
    lastName = entry2.get()
    printData(firstName, lastName)


root = Tk()
#Label 1
label1 = Label(root,text = 'First Name')
label1.pack()
label1.config(justify = CENTER)

entry1 = Entry(root, width = 30)
entry1.pack()

label3 = Label(root, text="Last Name")
label3.pack()
label1.config(justify = CENTER)

entry2 = Entry(root, width = 30)
entry2.pack()

button1 = Button(root, text = 'submit')
button1.pack() 
button1.config(command = get_input)

root.mainloop()
549/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
549/2: path_file_2, base_2=search_for_file_path ()
549/3:
df=pd.read_csv(path_file_2)
df
549/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
549/5:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
549/6: dat1=path_dots(df, base_2)
549/7: dat1
549/8:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/9:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/10: Area
549/11:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " <br> "+text + 
            "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
549/12:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "Hotels")
549/13:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "lodging")
549/14: API_values.head(10)
549/15: API_values.iloc[0]
549/16:
dat_dummy=dat1
dat_dummy["Type"]="LIB"
549/17: dat_dummy
549/18: np.array_split(dat_dummy, 3)
549/19: days=np.array_split(dat_dummy, 3)
549/20:
days=np.array_split(dat_dummy, 3)
days
549/21:
days=np.array_split(dat_dummy, 3)
pd.DataFrame(days)
549/22:
days=np.array_split(dat_dummy, 3)
type(days)
549/23:
days=np.array_split(dat_dummy, 3)
days.shape
549/24:
days=np.array_split(dat_dummy, 3)
days[0]
549/25:
days=np.array_split(dat_dummy, 3)
days[1]
549/26:
days=np.array_split(dat_dummy, 3)
days[2]
549/27:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
549/28:
days=np.array_split(dat_dummy, 3)
days[2]["LON"]
549/29:
days=np.array_split(dat_dummy, 3)
Area=plot_path(days[0], base_2)
549/30:
days=np.array_split(dat_dummy, 3)
Area=plot_path(days[0], base_2)
Area
549/31:
days=np.array_split(dat_dummy, 3)
Area=plot_path(days[1], base_2)
Area
549/32:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy)
centroids = kmeans.cluster_centers_
print(centroids)
549/33:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy.iloc[:,1:3])
centroids = kmeans.cluster_centers_
print(centroids)
549/34:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy.iloc[:,0:32)
centroids = kmeans.cluster_centers_
print(centroids)
549/35:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy.iloc[:,0:2)
centroids = kmeans.cluster_centers_
print(centroids)
549/36:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
549/37:
plt.scatter(at_dummy['LON'], at_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/38:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/39:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
549/40:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/41:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/42:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
549/43:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/44:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
549/45:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/46:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
549/47:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/48:
plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.show()
549/49:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax.show()
549/50:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax.show()
549/51:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
549/52:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
549/53:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
549/54: #places
549/55:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width: int), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
549/56:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width: 500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
549/57:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
549/58:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=("<b>" + label_2 +"</b>" + "<br>" + label, , max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/59:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/60:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/61:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/62: Area
549/63:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_height=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/64:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/65: Area
549/66:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_height=20),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/67:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/68:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_weight=200),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/69:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/70: Area
549/71:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_weight=80),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/72:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/73:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=80),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)

    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/74:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/75: Area
549/76:
folium.RegularPolygonMarker(
    [47.3489, -124.708],
    fill_color='#43d9de',
    radius=12,
    popup=folium.Popup(max_width=450).add_child(
        folium.Vega(json.load(open('vis1.json')), width=450, height=250))
    ).add_to(buoy_map)
549/77:
folium.RegularPolygonMarker(
    [47.3489, -124.708],
    fill_color='#43d9de',
    radius=12,
    popup=folium.Popup(max_width=450).add_child(
        folium.Vega(), width=450, height=250))
    ).add_to(buoy_map)
549/78:
folium.RegularPolygonMarker(
    [47.3489, -124.708],
    fill_color='#43d9de',
    radius=12,
    popup=folium.Popup(max_width=450).add_child(width=450, height=250).add_to(buoy_map)
549/79:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <h3 align="center" style="font-size:20px"><b>Titly title</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/80:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/81: Area
549/82:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <h3 align="center" style="font-size:20px"><b>Map Path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/83:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/84: Area
549/85:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:20px"><b>Titly title</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/86:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/87: Area
549/88:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup("<b>" + label_2 +"</b>" + "<br>" + label, max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/89:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/90: Area
549/91:
import folium

m = folium.Map(location=[43.775, 11.254],
               zoom_start=5)

html = '''1st line<br>
2nd line<br>
3rd line'''

iframe = folium.IFrame(html,
                       width=100,
                       height=100)

popup = folium.Popup(iframe,
                     max_width=100)

marker = folium.Marker([43.775, 11.254],
                       popup=popup).add_to(m)
m
549/92:
import folium

m = folium.Map(location=[43.775, 11.254],
               zoom_start=5)

html = '''1st line<br>
2nd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>
3rd line<br>'''

iframe = folium.IFrame(html,
                       width=100,
                       height=50)

popup = folium.Popup(iframe,
                     max_width=100)

marker = folium.Marker([43.775, 11.254],
                       popup=popup).add_to(m)
m
549/93:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
    html="<b>" + label_2 +"</b>" + "<br>" + label
    iframe = folium.IFrame(html,
                       width=100,
                       height=50)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=100),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/94:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=100,
                       height=50)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=100),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/95:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/96: Area
549/97:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=100,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=300),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/98:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/99: Area
549/100:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
549/101:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
549/102: Area
553/1:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/2:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
553/3:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
553/4:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
553/5: path_file_2, base_2=search_for_file_path ()
553/6:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
553/7:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
553/8: dat1=path_dots(df, base_2)
553/9: dat1
553/10:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
553/11:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
553/12: Area
553/13:
df=pd.read_csv(path_file_2)
df
553/14:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
553/15:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
553/16: dat1=path_dots(df, base_2)
553/17: dat1
553/18:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
553/19:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
553/20: Area
553/21:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
553/22: import time
553/23:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " <br> "+text + 
            "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
553/24:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "lodging")
553/25: API_values.iloc[0]
553/26:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
553/27:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
553/28:
dat_dummy=dat1
dat_dummy["Type"]="LIB"
553/29:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
553/30:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
553/31:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/32:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
553/33: api= GooglePlaces(api_key)
553/34: places = api.search_places_by_coordinate(location_med, "2500", "bar")
553/35: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
553/36:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
553/37:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
553/38: #places
553/39:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
553/40:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/41:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="restaurant", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/42:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="food", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/43:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="bar", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/44:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/45:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="utensils", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/46:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="fa-utensils", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/47:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="fa-utensils", prefix='fas', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/48:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="house", prefix='fas', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/49:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="home", prefix='fas', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/50:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="hotel", prefix='fas', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/51:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="home", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/52: places = api.search_places_by_coordinate(location_med, "2500", "night_club")
553/53: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
553/54:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
553/55:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
553/56: #places
553/57: Data_Hotels.to_csv("Hoteles_"+ base_2 + ".csv")
553/58:
Full_review=("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ "\n "+ text)
Full_review
553/59:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='purple', icon="home", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
553/60: places
553/61: details
555/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
555/2: path_file_2, base_2=search_for_file_path ()
555/3:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
555/4:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
555/5: dat1=path_dots(df, base_2)
555/6: dat1
555/7:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
555/8:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
555/9: Area
555/10:
df=pd.read_csv(path_file_2)
df
555/11:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
555/12:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
555/13: dat1=path_dots(df, base_2)
555/14: dat1
555/15:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
555/16:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
555/17: Area
555/18:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
555/19:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " <br> "+text + 
            "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
555/20:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "lodging")
555/21: API_values.iloc[0]
555/22:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
555/23:

Data_Hotels=API_values
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
555/24:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
555/25:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
555/26:
dat_dummy=dat1
dat_dummy["Type"]="LIB"
555/27:
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
555/28:
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
557/1: a="LALAA"
567/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
567/2: path_file_2, base_2=search_for_file_path ()
567/3:
df=pd.read_csv(path_file_2)
df
567/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
567/5:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/6: dat1=path_dots(df, base_2)
567/7: dat1
567/8:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
567/9:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
567/10: Area
567/11:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
567/12: import time
567/13: api= GooglePlaces(api_key)
567/14: places = api.search_places_by_coordinate(location_med, "2500", "bar")
567/15: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
567/16:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
567/17:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/18: size.details
567/19: len(details)
567/20: places = api.search_places_by_coordinate(location_med, "2500", "restaurant")
567/21: places = api.search_places_by_coordinate(location_med, "2500", "restaurants")
567/22: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
567/23:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
567/24: len(details)
567/25: details
567/26:
#night_club
places = api.search_places_by_coordinate(location_med, "2500", "night_club")
567/27: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
567/28:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
567/29: details
567/30: len(details)
567/31: len(details[0])
567/32: (details[0])
567/33: pd.DataFrame(details)
567/34: type(details)
567/35: details.values
567/36: details.values.tolist
567/37: details
567/38: details.iloc[0]
567/39: getsizeof(details)
567/40: places
567/41: len(places)
567/42: len(details)
567/43:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = []
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/44: review
567/45: len(review)
567/46: rating
567/47: Full_review
567/48: reviews
567/49: review
567/50:
Data_Hotels=pd.DataFrame([])
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = ""
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/51: phone_number
567/52: Full_review
567/53: review['author_name']
567/54:
Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
567/55: Full_review
567/56: website
567/57: lat
567/58:
latpd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/59:
pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/60:
pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0])
567/61: places[0]
567/62: popular
567/63:
popular = place["user_ratings_total"]
popular
567/64: places[0]["user_ratings_total"]
567/65:
#popular = place["user_ratings_total"]
place
567/66: places[0]
567/67: places[60]
567/68: places[59]
567/69:
Data_Hotels=pd.DataFrame([])
i=0
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
    except KeyError:
        reviews = ""
   
    Full_review=[]
    for review in reviews:
        author_name = review['author_name']
        rating = review['rating']
        text = review['text']
        time = review['relative_time_description']
        #profile_photo = review['profile_photo_url']
        #Data_Hotels["Popularity"]=(popular)
        Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
        "\n NEXT \n \n")
    print(i)
    i=i+1
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/70: Data_Hotels
567/71: places[19]
567/72: places[18]
567/73: places[18].place_id
567/74: places[18]["place_id"]
567/75: places[19]["place_id"]
567/76: details2 = api.get_place_details(place[19]['place_id'], fields)
567/77: details2 = api.get_place_details(places[19]['place_id'], fields)
567/78: details2
567/79: details2 = api.get_place_details(places[20]['place_id'], fields)
567/80: details2
567/81: places[20]["place_id"]
567/82: len(details2)
567/83: details2 = api.get_place_details(places[19]['place_id'], fields)
567/84: len(details2)
567/85: import sys
567/86:
import sys
sys.getsizeof(details2)
567/87: places[29]["place_id"]
567/88: details2 = api.get_place_details(places[19]['place_id'], fields)
567/89:
import sys
sys.getsizeof(details2)
567/90: details2 = api.get_place_details(places[20]['place_id'], fields)
567/91:
import sys
sys.getsizeof(details2)
567/92: places[20]["place_id"]
567/93: (details2)
567/94: details2 = api.get_place_details(places[19]['place_id'], fields)
567/95:
import sys
sys.getsizeof(details2)
567/96: (details2)
567/97: details2 = api.get_place_details(places[20]['place_id'], fields)
567/98:
import sys
sys.getsizeof(details2)
567/99: details2
567/100: places[20]
567/101:
Data_Hotels=pd.DataFrame([])
i=0
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
        Full_review=[]
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
    except KeyError:
        reviews = ""
   
    
    print(i)
    i=i+1
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/102: Data_Hotels
567/103:

Data_Hotels=API_values
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
567/104:

#Data_Hotels=API_values
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
567/105:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ 
            " <br> "+text+ "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
                .sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
567/106:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ 
            " <br> "+text+ "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), 
                ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
567/107:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "lodging")
567/108: API_values
567/109:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "restaurants")
567/110: API_values
567/111:
#"bar"
#"lodging"
#"tourist_attraction"
API_values=GetPlaces(api_key, location_med, "restaurant")
567/112: API_values
567/113: #Data_Hotels
567/114:
days=np.array_split(dat_dummy, 3)
Area=plot_path(days[1], base_2)
#Area
567/115:
number_of_days=5
API_values.head(number_of_days*2)
567/116: Points=["bar","lodging","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum"]
567/117: Points[0]
567/118: Points[2]
567/119: Points=["bar","lodging","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/120:

API_values=GetPlaces(api_key, location_med, Points[8])
567/121:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ 
            " <br> "+text+ "<br> NEXT <br> <br>")
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), 
                ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
567/122:

API_values=GetPlaces(api_key, location_med, Points[8])
567/123:
#night_club
places = api.search_places_by_coordinate(location_med, "2500", "church")
567/124: len(places)
567/125: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
567/126:
for place in places:
    details = api.get_place_details(place['place_id'], fields)
567/127: details
567/128: places
567/129: #places
567/130: fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
567/131: places[20]
567/132:
Data_Hotels=pd.DataFrame([])
i=0
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
        Full_review=[]
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
    except KeyError:
        reviews = ""
   
    i=i+1
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/133: Data_Hotels
567/134:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ 
                " <br> "+text+ "<br> NEXT <br> <br>")
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), 
                ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
567/135: Points=["bar","lodging","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/136:

API_values=GetPlaces(api_key, location_med, Points[8])
567/137:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
#Extract data from places dataframe
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
        
    
        except KeyError:
            rating_total=""      
          
        
        try:
            popular = place["user_ratings_total"]
    
        except KeyError:    
            popular=""
    
        try:
            reviews = details['result']['reviews']
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ 
                " <br> "+text+ "<br> NEXT <br> <br>")
        except KeyError:
            reviews = []
    
        Full_review=[]
        #Extract reviews per location
        
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), 
                ignore_index=True).sort_values("Popularity", ascending=False)
        
        Data_Hotels.to_csv(type_loc + "_" + base_2 + ".csv")

    return Data_Hotels
567/138: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/139: Points[2]
567/140:

API_values=GetPlaces(api_key, location_med, Points[8])
567/141:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   

    return Data_Hotels
567/142:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    for place in places:
        #Access to details
        details = api.get_place_details(place['place_id'], fields)

    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   

    return Data_Hotels
567/143: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/144:

API_values=GetPlaces(api_key, location_med, Points[8])
567/145: Points[8]
567/146:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   

    return Data_Hotels
567/147: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/148: Points[8]
567/149:

API_values=GetPlaces(api_key, location_med, Points[8])
567/150:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        Full_review=[]
        try:
            reviews = details['result']['reviews']
            
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   

    return Data_Hotels
567/151: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/152: Points[8]
567/153:

API_values=GetPlaces(api_key, location_med, Points[8])
567/154:
Data_Hotels=pd.DataFrame([])
i=0
#Data_Hotels=[]
for place in places:
    details = api.get_place_details(place['place_id'], fields)
    try:
        website = details['result']['website']
    except KeyError:
        website = ""
 
    try:
        name = details['result']['name']
    except KeyError:
        name = ""
 
    try:
        address = details['result']['formatted_address']
    except KeyError:
        address = ""
 
    try:
        phone_number = details['result']['international_phone_number']
    except KeyError:
        phone_number = ""
    

    try:
        lat = place['geometry']["location"]["lat"]
        lon = place['geometry']["location"]["lng"]

    except KeyError:
        lat = ""
        lon= ""

    try:
        rating_total = place['rating']
   
    except KeyError:
        rating_total=""

    try:
        popular = place["user_ratings_total"]
   
    except KeyError:
        popular=""
 
    try:
        reviews = details['result']['reviews']
        Full_review=[]
        for review in reviews:
            author_name = review['author_name']
            rating = review['rating']
            text = review['text']
            time = review['relative_time_description']
            #profile_photo = review['profile_photo_url']
            #Data_Hotels["Popularity"]=(popular)
            Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
            "\n NEXT \n \n")
    except KeyError:
        reviews = ""
   
    i=i+1
    Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)
567/155: Data_Hotels
567/156:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
        

    return Data_Hotels
567/157: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/158: Points[8]
567/159:

API_values=GetPlaces(api_key, location_med, Points[8])
567/160:

API_values=GetPlaces(api_key, location_med, Points[0])
567/161:
number_of_days=5
API_values.head(number_of_days*2)
567/162:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_Hotels=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=[]
    
        i=i+1
        Data_Hotels= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
        

    return Data_Hotels
567/163: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
567/164: Points[8]
567/165:

API_values=GetPlaces(api_key, location_med, Points[8])
567/166: #Data_Hotels
567/167:
 #Clusterdays
dat_dummy=dat1
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/168:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
567/169:
 #Clusterdays
dat_dummy=dat1
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/170:
 #Clusterdays
dat_dummy=dat1
kmeans = KMeans(n_clusters=4).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/171:
 #Clusterdays
days=4
dat_dummy=dat1
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/172:
 #Clusterdays
days=3
dat_dummy=dat1
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/173: details2
567/174: dat_dummy
567/175: dat_dummy["day"]
567/176:
 #Clusterdays
days=3
dat_dummy=dat1
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/177: dat_dummy["day"]
567/178:
test=np.array_split(dat_dummy, 3)
Area=plot_path(days[1], base_2)
#Area
567/179:
test=np.array_split(dat_dummy, 3)
#Area=plot_path(days[1], base_2)
#Area
567/180: test
567/181: test[0]
567/182: shape(test)
567/183: test.shape
567/184: test.shape()
567/185: len(test)
567/186:
test=np.array_split(dat_dummy["day"])
#Area=plot_path(days[1], base_2)
#Area
567/187:
test=np.array_split(dat_dummy.groupby["day"])
#Area=plot_path(days[1], base_2)
#Area
567/188:
test=np.array_split(dat_dummy.groupby("day", axis="columns"))
#Area=plot_path(days[1], base_2)
#Area
567/189:
test=np.array_split(dat_dummy.groupby("day", axis="columns"),days)
#Area=plot_path(days[1], base_2)
#Area
567/190: test[0]
567/191: test[1]
567/192:
test=np.array_split(dat_dummy.groupby("day", axis="columns"))
#Area=plot_path(days[1], base_2)
#Area
567/193:
dat_dummy.groupby("day", axis="columns")
#Area=plot_path(days[1], base_2)
#Area
567/194:
test=np.array_split(dat_dummy("day"), days)
#Area=plot_path(days[1], base_2)
#Area
567/195:
test=np.array_split(dat_dummy, days)
#Area=plot_path(days[1], base_2)
#Area
567/196: test
567/197: test[0]
567/198: test[1]
567/199: test[2]
567/200: test2=dat_dummy.groupby("days")
567/201: df_by_day = df.groupby('day')
567/202: df_by_day = dat_dummy.groupby('day')
567/203:
df_by_day = dat_dummy.groupby('day')
df_by_day
567/204:
df_by_day = dat_dummy.groupby('day')
df_by_day.describe()
567/205:
df_by_day = dat_dummy.groupby('day')
list(df_by_day)[10]
567/206:
df_by_day = dat_dummy.groupby('day')
list(df_by_day)
567/207:
df_by_day = dat_dummy.groupby('day')
Schedule_day=list(df_by_day)
567/208:
df_by_day = dat_dummy.groupby('day')
Schedule_day=list(df_by_day)

Schedule_day[0]
567/209:
df_by_day = dat_dummy.groupby('day')
Schedule_day=list(df_by_day)

Schedule_day[1]
567/210:
df_by_day = dat_dummy.groupby('day')
Schedule_day=pd.DataFrame(list(df_by_day))

Schedule_day[1]
567/211:
df_by_day = dat_dummy.groupby('day')
Schedule_day=pd.DataFrame(list(df_by_day))

Schedule_day
567/212:
df_by_day = dat_dummy.groupby('day')
Schedule_day=pd.DataFrame(list(df_by_day))

Schedule_day.iloc[0]
567/213:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))

Schedule_day
567/214:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df=df.sort_values("Position", ascending=True)
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/215: dat1=path_dots(df, base_2)
567/216: dat1
567/217:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
567/218:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
567/219: Area
567/220:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/221: dat1=path_dots(df, base_2)
567/222: dat1
567/223:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
567/224:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
567/225: Area
567/226:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/227: dat1=path_dots(df, base_2)
567/228: dat1
567/229:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
567/230:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
567/231: Area
567/232:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))

Schedule_day.LAT
567/233:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))

Schedule_day["LAT"]
567/234:
import numpy_indexed as npi
C = npi.group_by(dat_dummy['day']).split()
567/235:
import numpy_indexed as npi
C = npi.group_by(dat_dummy['day']).split()
567/236:
import numpy_indexed as npi
C = npi.group_by(dat_dummy.day).split()
567/237:
import numpy_indexed as npi
C = npi.group_by(dat_dummy.day).split()
567/238: import numpy_indexed as npi
567/239: C = npi.group_by(dat_dummy.day).split()
567/240: C = npi.group_by(dat_dummy.day).split(dat_dummy)
567/241:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C
567/242:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0]
567/243:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
len(C)
567/244:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0]
567/245:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0][0]
567/246:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0].shape()
567/247:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0].shape
567/248:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C.shape
567/249:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0].shape
567/250:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0]["LAT"]
567/251:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[0]
567/252:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[1]
567/253:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[2]
567/254:
C = npi.group_by(dat_dummy.day).split(dat_dummy)
C[2].LAT
567/255:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
567/256:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
pd.DataFrame(Schedule_day)
567/257:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
pd.DataFrame(Schedule_day[1])
567/258:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Schedule_day
567/259:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
np.Schedule_day
567/260: df_by_day = df_by_day.groups["day"]
567/261: df.to_numpy()
567/262: aa=df.to_numpy()
567/263:
aa=df.to_numpy()
aa[]
567/264:
aa=df.to_numpy()
aa[]
567/265:
aa=df.to_numpy()
aa[0]
567/266:
aa=df.to_numpy()
aa[1]
567/267:
aa=df.to_numpy()
shape.aa
567/268:
aa=df.to_numpy()
len(aa)
567/269: np.array_split.help
567/270: array_split.help
567/271:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(pd.DataFrame(df_by_day))
Schedule_day
567/272:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(pd.DataFrame(df_by_day))
Schedule_day[0]
567/273:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(pd.DataFrame(df_by_day))
Schedule_day[1]
567/274:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(pd.DataFrame(df_by_day))
Schedule_day[2]
567/275:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Scheduled_Day
567/276:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Scheduled_day
567/277:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Schedule_day
567/278:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
len(Schedule_day)
567/279:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0])
567/280:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[1])
567/281:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2])
567/282:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
pd.DataFrame(Schedule_day[2])
567/283:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2])
567/284:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][1])
567/285:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][0])
567/286:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][1])
567/287:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][2])
567/288:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][1])
567/289:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][1])
567/290:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][0])
567/291:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][1])
567/292:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][2])
567/293:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][1])
567/294:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[1][1])
567/295:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[2][1])
567/296:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[1][1])
567/297:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
(Schedule_day[0][1])
567/298:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Day_1=(Schedule_day[0][1])
567/299:
 #Clusterdays
days=3
dat_dummy=df
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/300:
 #Clusterdays
days=3
dat_dummy=df
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/301:
 #Clusterdays
days=3
dat_dummy=dat1
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON'], dat_dummy['LAT'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/302:
 #Clusterdays
days=3
dat_dummy=df
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,0:2])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,0:2])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/303:
 #Clusterdays
days=3
dat_dummy=df
kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
centroids = kmeans.cluster_centers_
print(centroids)
ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
#ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
ax
567/304:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Day_1=(Schedule_day[0][1])
567/305: Day_1
567/306: path_dots(Day_1, "Day_1_Angels")
567/307: data_day1=path_dots(Day_1, "Day_1_Angels")
567/308: Area=plot_path(data_day1, base_2)
567/309:
Area=plot_path(data_day1, base_2)
Area
567/310:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Day_1=(Schedule_day[1][1])
567/311: data_day1=path_dots(Day_1, "Day_1_Angels")
567/312:
Area=plot_path(data_day1, base_2)
Area
567/313:
df_by_day = dat_dummy.groupby('day')
Schedule_day=(list(df_by_day))
Day_1=(Schedule_day[2][1])
567/314: data_day1=path_dots(Day_1, "Day_1_Angels")
567/315:
Area=plot_path(data_day1, base_2)
Area
567/316:
 #Clusterdays
def divide_days(df,days)

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
567/317:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
567/318: divide_days(df,2)
567/319: Schedule_day=divide_days(df,2)
567/320:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[2][1])
567/321:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[1][1])
567/322: data_day1=path_dots(Day_1, "Day_1_Angels")
567/323:
Area=plot_path(data_day1, base_2)
Area
567/324:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[2][1])
567/325:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
567/326: data_day1=path_dots(Day_1, "Day_1_Angels")
567/327:
Area=plot_path(data_day1, base_2)
Area
567/328:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[1][1])
567/329: data_day1=path_dots(Day_1, "Day_1_Angels")
567/330:
Area=plot_path(data_day1, base_2)
Area
567/331:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
567/332: data_day1=path_dots(Day_1, "Day_1_Angels")
567/333:
Area=plot_path(data_day1, base_2)
Area
567/334: len(Day_1)
567/335: data_day1
567/336:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/337: dat1=path_dots(df, base_2)
567/338: dat1
567/339: df
567/340:
df=pd.read_csv(path_file_2)
df
567/341:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
567/342: dat1=path_dots(df, base_2)
567/343: dat1
567/344: Data_Hotels
569/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
569/2: path_file_2, base_2=search_for_file_path ()
569/3:
df=pd.read_csv(path_file_2)
df
569/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
569/5:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
569/6:
df=pd.read_csv(path_file_2)
df
569/7: #Area
569/8:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
569/9: #data_day1
569/10:
#places on API
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
569/11: import time
569/12:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=[]
    
        i=i+1
        
        Data_places= Data_Hotels.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Data_places
569/13: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
569/14: Points[8]
569/15:

API_values=GetPlaces(api_key, location_med, Points[8])
569/16:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=[]
    
        i=i+1
        
        Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Data_places
569/17: Points=["lodging","bar","tourist_attraction", "restaurant", "lodging", "night_club", "art", "museum", "church"]
569/18: Points[8]
569/19:

API_values=GetPlaces(api_key, location_med, Points[8])
569/20:

API_values=GetPlaces(api_key, location_med, Points[7])
569/21:

API_values=GetPlaces(api_key, location_med, Points[1])
569/22:

API_values=GetPlaces(api_key, location_med, Points[2])
569/23:

API_values=GetPlaces(api_key, location_med, Points[3])
569/24:

API_values=GetPlaces(api_key, location_med, Points[5])
569/25:

API_values=GetPlaces(api_key, location_med, Points[6])
569/26: Points[4]
569/27: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/28: Points[4]
569/29:

API_values=GetPlaces(api_key, location_med, Points[4])
569/30:

API_values=GetPlaces(api_key, location_med, Points[3])
569/31:

API_values=GetPlaces(api_key, location_med, Points[5])
569/32:

API_values=GetPlaces(api_key, location_med, Points[6])
569/33:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=[]
    
        i=i+1
        
        Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Full_review
569/34: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/35: Points[4]
569/36:

API_values=GetPlaces(api_key, location_med, Points[6])
569/37:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=[]
    
        i=i+1
        
        # Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
        #             'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
        #             'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Full_review
569/38: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/39: Points[4]
569/40:

API_values=GetPlaces(api_key, location_med, Points[6])
569/41: API_values
569/42:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=""
    
        i=i+1
        
        # Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
        #             'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
        #             'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Full_review
569/43: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/44: Points[4]
569/45:

API_values=GetPlaces(api_key, location_med, Points[6])
569/46: API_values
569/47:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=""
    
        i=i+1
        
        Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Full_review
569/48: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/49: Points[4]
569/50:

API_values=GetPlaces(api_key, location_med, Points[6])
569/51: API_values
569/52:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=""
    
        i=i+1
        
        Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Data_places
569/53: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/54: Points[4]
569/55:

API_values=GetPlaces(api_key, location_med, Points[6])
569/56: API_values
569/57: #API_values
569/58: Points[5]
569/59:

API_values=GetPlaces(api_key, location_med, Points[6])
569/60:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/61:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/62: Hotels=GetPlaces(api_key,location_med, Points[0], base_2)
569/63:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/64: Hotels=GetPlaces(api_key,location_med, Points[0], base_2)
569/65:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/66: Hotels=GetPlaces(api_key,location_med, Points[0], base_2)
569/67: Hotels=GetPlaces(api_key, location_med, Points[0])
569/68:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/69: Hotels=GetPlaces(api_key, location_med, Points[0])
569/70: Hotels=GetPlaces(api_key, location_med, Points[0], "dd")
569/71:
def GetPlaces(api_key, location_med, type_loc):
    
    api= GooglePlaces(api_key)
    places = api.search_places_by_coordinate(location_med, "2500", type_loc)
    #Choose fields
    fields = ['name', 'formatted_address', 'international_phone_number', 'website', 'price_level', 'review']
    Data_places=pd.DataFrame([])
    i=0
    #Data_Hotels=[]
    for place in places:
        details = api.get_place_details(place['place_id'], fields)
        try:
            website = details['result']['website']
        except KeyError:
            website = ""
    
        try:
            name = details['result']['name']
        except KeyError:
            name = ""
    
        try:
            address = details['result']['formatted_address']
        except KeyError:
            address = ""
    
        try:
            phone_number = details['result']['international_phone_number']
        except KeyError:
            phone_number = ""
        

        try:
            lat = place['geometry']["location"]["lat"]
            lon = place['geometry']["location"]["lng"]

        except KeyError:
            lat = ""
            lon= ""

        try:
            rating_total = place['rating']
    
        except KeyError:
            rating_total=""

        try:
            popular = place["user_ratings_total"]
    
        except KeyError:
            popular=""
    
        try:
            reviews = details['result']['reviews']
            Full_review=[]
            for review in reviews:
                author_name = review['author_name']
                rating = review['rating']
                text = review['text']
                time = review['relative_time_description']
                #profile_photo = review['profile_photo_url']
                #Data_Hotels["Popularity"]=(popular)
                Full_review=str(Full_review) + str("Author: "+ author_name +"; Rating: "+ str(rating) +"; When: "+str(time)+ " \n "+text + 
                "\n NEXT \n \n")
        except KeyError:
            reviews = ""
            Full_review=""
    
        i=i+1
        
        Data_places= Data_places.append(pd.DataFrame({'Name': name, 'Website': website, 
                    'Phone Number': phone_number, 'LON': lon, 'LAT': lat,
                    'Rating': rating_total, 'Popularity': popular, 'Last 5 Reviews': Full_review}, index=[0]), ignore_index=True)   
            
     

    return Data_places
569/72: Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
569/73: Points[5]
569/74:

API_values=GetPlaces(api_key, location_med, Points[6])
569/75: API_values
569/76: #API_values
569/77:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/78: Hotels=GetPlaces(api_key, location_med, Points[0])
569/79: Hotels=GetPlaces(api_key, location_med, Points[0], "ss")
569/80: Hotels=GetPlaces(api_key, location_med, Points[0])
569/81:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
569/82: Hotels=GetPlaces(api_key, location_med, Points[0])
569/83: Hotels=GetPlaces(api_key, location_med, Points[0], "ff")
571/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
571/2: path_file_2, base_2=search_for_file_path ()
571/3:
df=pd.read_csv(path_file_2)
#df
571/4:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
571/5: Hotels=GetPlaces(api_key, location_med, Points[0], "ff")
571/6: Hotels=GetPlaces(api_key, location_med, Points[0])
571/7: Hotels
571/8: Hotels.head(days)
571/9:
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed)
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
571/10: Hotels.head(days)
571/11:
Hotels=Hotels.sort_values("Rating", ascending=False)
Hotels.head(days)
571/12:
Hotels=Hotels.sort_values("Popularity", ascending=False)
Hotels.head(days)
571/13:
Hotels=Hotels.sort_values(Hotels["Popularity"]*Hotels["Rating"], ascending=False)
Hotels.head(days)
571/14:
Hotels=Hotels.sort_values(Hotels["Popularity","Rating"], ascending=[False, False])
Hotels.head(days)
571/15:
Hotels=Hotels.sort_values(["Popularity","Rating"], ascending=[False, False])
Hotels.head(days)
571/16:
def GetHotels(api_key, location_med, Points[0], days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days)
    return Hotels
571/17:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days)
    return Hotels
571/18: Hotels=GetHotels(api_key, location_med, Points[0], days)
571/19: Hotels
571/20:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
571/21: Hotels=GetHotels(api_key, location_med, Points[0], days)
571/22: Hotels
571/23:
Area=plot_path(dat1, base_2)
Area2=Area
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/24:
# Area=plot_path(dat1, base_2)
Area2=[]
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/25:
# Area=plot_path(dat1, base_2)
Area2=[]
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster()
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/26:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster()
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/27:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Dots = plugins.MarkerCluster(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/28:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location=location_med, zoom_start=12)
Dots = plugins.MarkerCluster(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/29:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location_med, zoom_start=12)
Dots = plugins.MarkerCluster(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/30:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/31:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/32:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(width=100,height=100,location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/33:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(width=500,height=500,location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/34:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    width='50%', height='60%',
    control_scale=True,
    zoom_control=False,
    max_zoom=12, min_zoom=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/35:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], width='50%', height='60%',
control_scale=True,
zoom_control=False,
max_zoom=12, min_zoom=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/36:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=400, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], width='50%', height='60%',
control_scale=True, zoom_control=False, max_zoom=12, min_zoom=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/37:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=400, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], width='50%', height='60%',
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Area2
571/38:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=400, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]], width='50%', height='60%',
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
571/39:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=400, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
571/40:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
571/41: Hotels[3]
571/42: Hotels.iloc[3]
571/43: Hotels.iloc[3][0]
571/44: df
571/45:
#Choose n°3
Hotels.iloc[3]
571/46: Hotel_choosen=pd.DataFrame(Hotel[0])
571/47: Hotel_choosen=pd.DataFrame(Hotels[0])
571/48: Hotel_choosen=pd.DataFrame(Hotels.iloc[0])
571/49:
Hotel_choosen=pd.DataFrame(Hotels.iloc[0])
Hotel_choosen
573/1:
#Choose n°3
Hotels.iloc[3].LON #da aggiungere ad ogni cluster f(days)
573/2:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
573/3:
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed)
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
573/4:
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed)
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
573/5: path_file_2, base_2=search_for_file_path ()
573/6:
df=pd.read_csv(path_file_2)
#df
573/7:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
573/8:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
573/9: Hotels=GetHotels(api_key, location_med, Points[0], days)
573/10:
Hotels
#User should choose one or more hotels for the stay
573/11:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
573/12:
#Choose n°3
Hotels.iloc[3].LON #da aggiungere ad ogni cluster f(days)
573/13:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3].LON #da aggiungere ad ogni cluster f(days)
573/14:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3].LON #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
573/15:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
573/16: df.head(1)
573/17:
(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON, 
                'order': df_unicos.iloc[n][10], 'labels': "HOT", 'lugares': Hotel_Choosen.Name,
                'quotes': Hotel_Choosen.Last 5 Reviews, 'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
573/18:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON, 
                'order': df_unicos.iloc[n][10], 'labels': "HOT", 'lugares': Hotel_Choosen.Name,
                'quotes': Hotel_Choosen.Last 5 Reviews, 'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
573/19:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen.Name
573/20:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen.Last 5 reviews
573/21:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
573/22:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen[end]
573/23:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen[-1]
573/24:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON, 
                'order': df_unicos.iloc[n][10], 'labels': "HOT", 'lugares': Hotel_Choosen.Name,
                'quotes': Hotel_Choosen[-1], 'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
573/25:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': Hotel_Choosen.Name, 'quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
573/26:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
573/27:
df=pd.read_csv(path_file_2)
#df
573/28:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': Hotel_Choosen.Name + Hotel_Choosen.Rating + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2], 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
573/29:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/30:
(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/31:
pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
573/32:
Trial=df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/33:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
573/34:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" + str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
573/35:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" + str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
Trial[-1]
573/36:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br>" + str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
Trial.iloc[-1]
573/37:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
Trial.iloc[-1]
573/38:
Trial=pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0])
Trial.iloc[-1]
573/39:
df=pd.read_csv(path_file_2)
#df
573/40:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/41:
df=pd.read_csv(path_file_2)
#df
573/42:
df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/43: df
573/44:
df=df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)


#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/45:
df=df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)

np.roll(df['#1'], shift=1)
#  + Hotel_Choosen.Popularity + Hotel_Choosen.website + Hotel_Choosen[2]
573/46: np.roll(df['#1'], shift=1)
573/47: np.roll(df.index, shift=1)
573/48: gg=np.roll(df.index, shift=1)
573/49:
gg=np.roll(df.index, shift=1)
gg
573/50: res = df.iloc[np.arange(-1, len(df)-1)]
573/51:
res = df.iloc[np.arange(-1, len(df)-1)]
res
573/52: df = df.iloc[np.arange(-1, len(df)-1)]
573/53:
def path_dots(df, Book_name,start, finish, Hotels):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos['Type']="LIB"
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.iloc[m][9]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/54: dat1=path_dots(df, base_2)
573/55:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df=df.sort_values("Position", ascending=False)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos['Type']="LIB"
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.iloc[m][9]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/56: dat1=path_dots(df, base_2)
573/57: dat1
573/58:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
573/59:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
573/60: Area
573/61: df.iloc[:-2]
573/62: df.iloc[:-2].drop_duplicates(subset ="LON_google")
573/63:
df.iloc[:-2].drop_duplicates(subset ="LON_google") 
df=df.append(df.iloc[-1])
573/64:
df.iloc[:-2].drop_duplicates(subset ="LON_google") 
df=df.append(df.iloc[-1])
df
573/65:
df=df.sort_values("Position", ascending=False)
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
573/66:
df=df.sort_values("Position", ascending=False)
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
df
573/67:
df=df.sort_values("Position", ascending=False)
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
df["Quotes_total"][1]
573/68:
df=df.sort_values("Position", ascending=False)
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
df["Quotes_total"][0]
573/69:
df=df.sort_values("Position", ascending=False)
df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
df["Quotes_total"]
573/70:
def path_dots(df, Book_name):
    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.iloc[:-2].drop_duplicates(subset ="LON_google") 
    df_unicos=df_unicos.append(df.iloc[-1])
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos['Type']="LIB"
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.iloc[m][9]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/71: dat1=path_dots(df, base_2)
573/72: dat1
573/73:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
573/74:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
573/75: Area
573/76:
df=pd.read_csv(path_file_2)
#df
573/77:
def path_dots(df, Book_name):


    df=df.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
'labels': "HOT", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
"<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
"<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.iloc[:-2].drop_duplicates(subset ="LON_google") 
    df_unicos=df_unicos.append(df.iloc[-1])
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos['Type']="LIB"
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.iloc[m][9]}, index=[0]), ignore_index=True)
    
    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/78:
df.iloc[:-2].drop_duplicates(subset ="LON_google") 
df=df.append(df.iloc[-1])
df["labels"]="LIB"
573/79:
df.iloc[:-2].drop_duplicates(subset ="LON_google") 
df=df.append(df.iloc[-1])
df["labels"]="LIB"
df
573/80:
def path_dots(df, Book_name, Hotel_Choosen):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df)-1)]
    
    #df_unicos=df_unicos.append(df.iloc[-1])
    
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/81: dat1=path_dots(df, base_2, Hotel_Choosen)
573/82:
def path_dots(df, Book_name, Hotel_Choosen):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    
    #df_unicos=df_unicos.append(df.iloc[-1])
    
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
573/83: dat1=path_dots(df, base_2, Hotel_Choosen)
573/84: dat1
573/85:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
573/86:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
573/87: Area
573/88:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
573/89: Hotels=GetHotels(api_key, location_med, Points[0], days)
573/90:
Hotels
#User should choose one or more hotels for the stay
573/91:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
573/92:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
573/93: Hotels=GetHotels(api_key, location_med, Points[0], days)
573/94:
Hotels
#User should choose one or more hotels for the stay
573/95:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
573/96:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
573/97:
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed)
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
573/98:
df=pd.read_csv(path_file_2)
#df
573/99:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
573/100:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
573/101: Hotels=GetHotels(api_key, location_med, Points[0], days)
573/102:
Hotels
#User should choose one or more hotels for the stay
575/1:
df=pd.read_csv(path_file_2)
#df
575/2:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
575/3:
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed)
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/4: path_file_2, base_2=search_for_file_path ()
575/5:
df=pd.read_csv(path_file_2)
#df
575/6:
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
575/7:
def GetHotels(api_key, location_med, type, days):
    API_values=GetPlaces(api_key, location_med, Points[0])
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
575/8: Hotels=GetHotels(api_key, location_med, Points[0], days)
575/9:
Hotels
#User should choose one or more hotels for the stay
575/10:
# Area=plot_path(dat1, base_2)
Data_Hotels=Hotels
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)
#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Rating"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup("Name: "+ str(label) + "_________Rating: " + str(label2), parse_html=True, max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/11:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
575/12:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
575/13:
def path_dots(df, Book_name, Hotel_Choosen):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    
    #df_unicos=df_unicos.append(df.iloc[-1])
    
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/14: dat1=path_dots(df, base_2, Hotel_Choosen)
575/15: dat1
575/16:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)    
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=folium.Icon(color='red', icon="book", prefix='fa', icon_color="white"),
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/17:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/18: Area
575/19:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["labels"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/20:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/21: Area
575/22:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["Type"],
    lugares3["labels"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  

        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/23:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/24: Area
575/25:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3[""]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/26:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/27:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/28: Area
575/29:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=500, height=450)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], , control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area2)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/30:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=500, height=450)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area2)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/31:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/32: Area
575/33:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=500, height=450)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/34:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/35: Area
575/36:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=450, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/37:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/38: Area
575/39:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        else:
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/40:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/41: Area
575/42:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    loc=lugares3.iloc[:,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/43:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/44: Area
575/45:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
575/46:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
575/47: len(Day_1)
575/48: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/49: #data_day1
575/50:
Area=plot_path(data_day1, base_2)
Area
575/51:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[1][1])
575/52: len(Day_1)
575/53:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][0])
575/54: len(Day_1)
575/55:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][2])
575/56:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
575/57:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[1][1])
575/58: len(Day_1)
575/59:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[2][1])
575/60:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
575/61: Day_1
575/62: Schedule_day
575/63: Schedule_day[0]
575/64: Schedule_day[0][1]
575/65: Schedule_day[0][2]
575/66: Schedule_day[0][0]
575/67: Schedule_day[0][1]
575/68: Schedule_day[0][2]
575/69: Schedule_day[0][1]
575/70: Schedule_day[1][1]
575/71: Schedule_day[0][1]
575/72:
Schedule_day[2][1]
#[day][1]
575/73:
Schedule_day[0][1]
#[day][1]
575/74:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[0][1])
#[day][1]
575/75: len(Day_1)
575/76:
Schedule_day=divide_days(df,2)
Day_1=(Schedule_day[1][1])
#[day][1]
575/77: len(Day_1)
575/78: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/79: #data_day1
575/80:
Area=plot_path(data_day1, base_2)
Area
575/81:
loc=dat1.iloc[:,0:2]
    loc=loc.values.tolist()
575/82:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
575/83:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc[0]
575/84:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc.index
575/85:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc.index[0]
575/86:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc[0]
575/87:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc[0], loc[-1]
575/88:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:1,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[2:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_start.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/89:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/90: Area
575/91:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:1,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[2:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/92:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/93: Area
575/94:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[2:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/95:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/96: Area
575/97:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/98:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/99: Area
575/100:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-3:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/101:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/102: Area
575/103:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/104:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/105: Area
575/106:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-3:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/107:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/108: Area
575/109:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:0,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/110:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/111: Area
575/112:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/113:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/114: Area
575/115:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[0:-2,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/116:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/117: Area
575/118:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:0,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/119:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/120: Area
575/121:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/122:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/123: Area
575/124:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/125:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/126: Area
575/127:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:0,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/128:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/129: Area
575/130:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/131:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/132: Area
575/133:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/134:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/135: Area
575/136:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-4,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/137:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/138: Area
575/139:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-2,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-3:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/140:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/141: Area
575/142:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-3:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/143:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/144: Area
575/145:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1,0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/146:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/147: Area
575/148:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc[0]
575/149:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
loc[-1]
575/150:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala.append(loc[-2])
575/151:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala.append(loc[-2])
lala
575/152:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala=lala.append(loc[-2])
lala
575/153:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala=lala.append(loc[-2])
lala
575/154: lala
575/155:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala.append(loc[-2])
lala
575/156:
loc=dat1.iloc[:,0:2]
loc=loc.values.tolist()
lala=loc[-1]
lala.append(loc[-2])
loc
575/157:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3, 0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/158:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/159: Area
575/160:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3, 0:2]
    loc_end.append(lugares3.iloc[0, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/161:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/162: Area
575/163:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3, 0:2]
    loc_end.append(lugares3.iloc[0:2, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/164:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/165: Area
575/166:
loc=dat1.iloc[-1:-3, 0:2]
loc=loc.values.tolist()
loc
575/167:
loc=dat1.iloc[0:2, 0:2]
loc=loc.values.tolist()
loc
575/168:
loc=dat1.iloc[-1:-2, 0:2]
loc=loc.values.tolist()
loc
575/169:
loc=dat1.iloc[-1:-3, 0:2]
loc=loc.values.tolist()
loc
575/170:
loc=dat1.iloc[-2:-1, 0:2]
loc=loc.values.tolist()
loc
575/171:
loc=dat1.iloc[-3:-1, 0:2]
loc=loc.values.tolist()
loc
575/172:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3, 0:2]
    loc_end.append(lugares3.iloc[-3:-1, 0:2])
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/173:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/174: Area
575/175:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-1:-3, 0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/176:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/177: Area
575/178:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-3:-1, 0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/179:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/180: Area
575/181:
loc=dat1.iloc[-2:-1, 0:2]
loc=loc.values.tolist()
loc
575/182:
loc=dat1.iloc[-2:0, 0:2]
loc=loc.values.tolist()
loc
575/183:
loc=dat1.iloc[-2:-1, 0:2]
loc=loc.values.tolist()
loc
575/184:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/185:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/186: Area
575/187:
loc=dat1.iloc[-3:-1, 0:2]
loc=loc.values.tolist()
loc
575/188:
loc=dat1.iloc[0:-1, 0:2]
loc=loc.values.tolist()
loc
575/189:
loc=dat1.iloc[-2:-1, 0:2]
loc=loc.values.tolist()
loc
575/190:
loc=dat1.iloc[-2:-1, 0:2], dat1.iloc[0:1, 0:2]
loc=loc.values.tolist()
loc
575/191:
loc=dat1.iloc[-2:-1, 0:2]
loc=loc.append(dat1.iloc[0:1, 0:2])
loc=loc.values.tolist()
loc
575/192:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")  
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/193:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/194: Area
575/195:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
575/196:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[1][1])
#[day][1]
575/197: len(Day_1)
575/198:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[0][1])
#[day][1]
575/199: len(Day_1)
575/200:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[2][1])
#[day][1]
575/201: len(Day_1)
575/202:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[0][1])
#[day][1]
575/203: len(Day_1)
575/204: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/205:
Area=plot_path(data_day1, base_2)
Area
575/206:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[1][1])
#[day][1]
575/207: len(Day_1)
575/208: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/209: #data_day1
575/210:
Area=plot_path(data_day1, base_2)
Area
575/211:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[0][1])
#[day][1]
575/212: len(Day_1)
575/213:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[2][1])
#[day][1]
575/214: len(Day_1)
575/215:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[1][1])
#[day][1]
575/216: len(Day_1)
575/217: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/218: #data_day1
575/219:
Area=plot_path(data_day1, base_2)
Area
575/220:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[0][1])
#[day][1]
575/221: len(Day_1)
575/222: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/223: #data_day1
575/224:
Area=plot_path(data_day1, base_2)
Area
575/225:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[1][1])
#[day][1]
575/226: len(Day_1)
575/227: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/228: #data_day1
575/229:
Area=plot_path(data_day1, base_2)
Area
575/230:
Schedule_day=divide_days(df,3)
Day_1=(Schedule_day[2][1])
#[day][1]
575/231: len(Day_1)
575/232: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/233: #data_day1
575/234:
Area=plot_path(data_day1, base_2)
Area
575/235: Points[1:]
575/236:
def GetPOIs(api_key, location_med, type, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, Points[2])
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.head(days*SPEED*2)
    return POIs_ext
575/237:
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
575/238:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.head(days*SPEED*2)
    return POIs_ext
575/239: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED):
575/240: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
575/241: POIs_ext
575/242: #POIs_ext
575/243: len(POIs_ext)
575/244:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
     df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
 
    

    
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/245:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
 
    

    
    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/246: dat1=path_dots(df, base_2, Hotel_Choosen)
575/247: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/248: (POIs_ext)
575/249: (POIs_ext).head(10)
575/250:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/251: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/252:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/253: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/254:
df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/255:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/256:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/257: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/258:
df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/259:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext[-1],
'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/260: POIs_ext.Website
575/261: POIs_ext.Popularity
575/262: POIs_ext[-1]
575/263: POIs_ext.iloc[-1]
575/264: POIs_ext.iloc[:,-1]
575/265:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/266:
pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating}, index=[0])
575/267:
pd.DataFrame({'LAT_google': POIs_ext.LAT})

#  'LON_google': POIs_ext.LON,
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/268:
pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,})

#  'LON_google': POIs_ext.LON,
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/269:
pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1],'Position book': POIs_ext.Rating}, index=[0]})


# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/270:
pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1],'Position book': POIs_ext.Rating})


# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/271:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/272: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/273:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/274:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/275: Area
575/276: dat1
575/277:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1],'Position book': POIs_ext.Rating})


# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/278:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1],'Position book': POIs_ext.Rating})

result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/279: result
575/280:
  df= df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/281: df
575/282:
df=pd.read_csv(path_file_2)
#df
575/283:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT}, index=[0]), ignore_index=True)

# , 'LON_google': POIs_ext.LON,
#     'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
#     "<br><b>Website: </b>" + str(POIs_ext.Website) +
#     "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
#     'Position book': POIs_ext.Rating
575/284:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT[0], 'LON_google': POIs_ext.LON[0],
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/285:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT[0], 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/286:
df.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating}, index=[0]), ignore_index=True)
575/287: df.append(POIs_ext)
575/288:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating}, index=[0])
575/289:
#POIs_ext.rename(columns={"Name": "lugares", "LAT": "LAT_google", "LON": "LON_google", "Last 5 Reviews": "Quotes"})
df.append(RRR)
575/290: RRR
575/291:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
'Position book': POIs_ext.Rating})
575/292: RRR
575/293:
#POIs_ext.rename(columns={"Name": "lugares", "LAT": "LAT_google", "LON": "LON_google", "Last 5 Reviews": "Quotes"})
df.append(RRR)
575/294:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/295: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/296: dat1
575/297:
df=pd.read_csv(path_file_2)
#df
575/298:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/299: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/300: dat1
575/301: df
575/302:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) +
"<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]})
575/303: df
575/304:
#POIs_ext.rename(columns={"Name": "lugares", "LAT": "LAT_google", "LON": "LON_google", "Last 5 Reviews": "Quotes"})
df.append(RRR)
575/305:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/306: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/307: dat1
575/308:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': df_unicos.distance[n], 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/309: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/310: dat1
575/311:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/312: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/313: dat1
575/314:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/315: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/316: dat1
575/317:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][10], 'Distance [m]': df_unicos.iloc[n][11], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][7], 'Position book': df_unicos.iloc[m][8], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/318: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/319:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][11], 'Distance [m]': df_unicos.iloc[n][12], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][8], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][10], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/320: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/321: dat1
575/322:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[m][4], 'LON': df_unicos.iloc[m][5], 
                'order': df_unicos.iloc[n][11], 'Distance [m]': df_unicos.iloc[n][12], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][8], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][9], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/323: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/324: dat1
575/325:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.iloc[n][11], 'Distance [m]': df_unicos.distance[n], 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][8], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][9], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/326: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/327: dat1
575/328:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.iloc[n][11], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.iloc[m][3],
                'quotes': df_unicos.iloc[m][8], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][9], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/329: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/330: dat1
575/331:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.iloc[0][4], 'LON': df_unicos.iloc[0][5],
    'order': df_unicos.iloc[0][9], 'Distance [m]': df_unicos.iloc[0][11], 'lugares': df_unicos.iloc[0][3],
    'quotes': df_unicos.iloc[0][7], 'Position book': df_unicos.iloc[0][8], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/332: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/333: dat1
575/334:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position book': df_unicos.iloc[m][9], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position book': df_unicos.iloc[0][9], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/335: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/336:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1],
    'Position book': Hotel_Choosen.Rating}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
    'Position book': POIs_ext.Rating}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position book': df_unicos.iloc[m][10], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position book': df_unicos.iloc[0][10], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/337: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/338: dat1
575/339:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position book': df_unicos.iloc[m][10], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position book': df_unicos.iloc[0][10], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/340: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/341: dat1
575/342:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/343: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/344: dat1
575/345:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/346:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/347: Area
575/348:
# POIs_ext.rename(columns={"Name": "lugares", "LAT": "LAT_google", "LON": "LON_google", "Last 5 Reviews": "LAT_google"})
# df.append(POIs_ext)
575/349: POIs_ext
575/350:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})

#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/351:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/352:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/353:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/354:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + str(POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/355:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/356:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR.Name
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/357:
RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR.lugares
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/358:
POIs_ext = POIs_ext.applymap(str)

RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR.lugares
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/359:
#POIs_ext = POIs_ext.applymap(str)

RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + (POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR.lugares
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/360:
#POIs_ext = POIs_ext.applymap(str)

RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + (POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/361:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/362: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/363: dat1
575/364:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED*2)
    return POIs_ext
575/365: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
575/366: POIs_ext
575/367: POIs_ext.Rating
575/368: POIs_ext.Rating=POIs_ext.Rating.applymap(str)
575/369: =str(POIs_ext.Rating)
575/370: str(POIs_ext.Rating)
575/371: POIs_ext.Rating.apply(str)
575/372: POIs_ext.Rating=POIs_ext.Rating.apply(str)
575/373:
POIs_ext.Rating=POIs_ext.Rating.apply(str)
POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
575/374:
#POIs_ext = POIs_ext.applymap(str)

RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + (POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
RRR
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/375:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': str(Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + str(Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
    "<br><b>Website: </b>" + str(POIs_ext.Website) +
    "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/376: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/377: dat1
575/378:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/379:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/380: Area
575/381:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/382: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/383: dat1
575/384:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/385:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/386: Area
575/387:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/388: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/389: dat1
575/390:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/391:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/392: Area
575/393:
Schedule_day=divide_days(df,days)
Day_1=(Schedule_day[2][1])
#[day][1]
575/394: len(Day_1)
575/395:
Schedule_day=divide_days(df,days)
Day_1=(Schedule_day[0][1])
#[day][1]
575/396: len(Day_1)
575/397: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/398:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/399: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/400: dat1
575/401:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/402:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/403: Area
575/404:
#POIs_ext = POIs_ext.applymap(str)

RRR=pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
'labels': "TOUR",'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
"<br><b>Website: </b>" + (POIs_ext.Website) + "<br><b>Popularity: </b>" + (POIs_ext.Popularity), 
'Quotes': POIs_ext.iloc[:,-1]})
#result = pd.merge(df, RRR, on='LAT_google')
# 'labels': "TOUR", 'lugares': str(POIs_ext.Name) + "<br><b>Rating: </b>" + str(POIs_ext.Rating) +
# "<br><b>Website: </b>" + str(POIs_ext.Website) +
# "<br><b>Popularity: </b>" + str(POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1],
# 'Position book': POIs_ext.Rating}, index=[0]
575/405:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    return POIs_ext
575/406: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
575/407:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/408:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/409: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/410: dat1
575/411:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/412:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/413: Area
575/414:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/415: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
575/416: POIs_ext
575/417:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    print(df_unicos)
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/418: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/419:
def path_dots(df, Book_name, Hotel_Choosen, POIs_ext):

    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    
    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum','Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/420: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/421: dat1
575/422:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="hotel", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/423:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/424: Area
575/425:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="landmark", prefix='fa', icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/426:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/427: Area
575/428:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/429:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/430: Area
575/431:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': (POIs_ext.Name) + "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    "<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
575/432: df_unicos=add_POIs_df(df, POIs_ext)
575/433: df_unicos
575/434:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': (Hotel_Choosen.Name) + "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    "<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/435: dat1=path_dots(df, base_2, Hotel_Choosen, POIs_ext)
575/436: dat1
575/437:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/438:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/439: Area
575/440: dat1=path_dots(df_unicos, base_2, Hotel_Choosen, POIs_ext)
575/441: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
575/442: dat1
575/443:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/444:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/445: Area
575/446:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
575/447:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
575/448: len(Day_1)
575/449: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/450:
Area=plot_path(data_day1, base_2)
Area
575/451:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
575/452: len(Day_1)
575/453: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/454: #data_day1
575/455:
Area=plot_path(data_day1, base_2)
Area
575/456:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
575/457: len(Day_1)
575/458: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/459: #data_day1
575/460:
Area=plot_path(data_day1, base_2)
Area
575/461:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[3][1])
#[day][1]
575/462: len(Day_1)
575/463: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/464: #data_day1
575/465:
Area=plot_path(data_day1, base_2)
Area
575/466:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[4][1])
#[day][1]
575/467: len(Day_1)
575/468: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/469: #data_day1
575/470:
Area=plot_path(data_day1, base_2)
Area
575/471:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[5][1])
#[day][1]
575/472:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
575/473: len(Day_1)
575/474: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/475: #data_day1
575/476:
Area=plot_path(data_day1, base_2)
Area
575/477:
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/478:
Data_Hotels=pd.DataFrame({'LAT_google': Hotels.LAT, 'LON_google': Hotels.LON,
    'labels': "HOTEL", 'lugares': (Hotels.Name) + "<br><b>Rating: </b>" + (Hotels.Rating) +
    "<br><b>Website: </b>" + (Hotels.Website) +
    "<br><b>Popularity: </b>" + (Hotels.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
575/479:
Data_Hotels=pd.DataFrame({'LAT_google': Hotels.LAT, 'LON_google': Hotels.LON,
    'labels': "HOTEL", 'lugares': (Hotels.Name) + "<br><b>Rating: </b>" + (Hotels.Rating) +
    "<br><b>Website: </b>" + (Hotels.Website) +
    "<br><b>Popularity: </b>" + (Hotels.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0])
575/480:
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT_google': Hotels_AV.LAT, 'LON_google': Hotels_AV.LON,
    'labels': "HOTEL", 'lugares': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels_AV.iloc[-1]}, index=[0])
575/481:
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT_google': Hotels_AV.LAT, 'LON_google': Hotels_AV.LON,
    'labels': "HOTEL", 'lugares': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels_AV.iloc[-1]}, index=[0])

Data_Hotels
575/482:
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT_google': Hotels_AV.LAT, 'LON_google': Hotels_AV.LON,
    'labels': "HOTEL", 'lugares': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels_AV.iloc[-1]})
575/483: Data_Hotels
575/484:
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT_google': Hotels_AV.LAT, 'LON_google': Hotels_AV.LON,
    'labels': "HOTEL", 'lugares': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels_AV[-1]})
575/485: Data_Hotels
575/486: Hotels_AV[-1]
575/487: Hotels_AV.iloc[-1]
575/488: Hotels
575/489: Hotels["Last 5 Reviews"]
575/490:
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT_google': Hotels_AV.LAT, 'LON_google': Hotels_AV.LON,
    'labels': "HOTEL", 'lugares': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
575/491: Data_Hotels
575/492:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2, reviews in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label_2 +"</b>" + "<br>" + label + "<br>" + reviews 
    iframe = folium.IFrame(html, width=500, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/493:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label_2
    iframe = folium.IFrame(html, width=500, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/494:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=500, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/495:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=200, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/496:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
Dots = plugins.MarkerCluster().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/497:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup()

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/498:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/499:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"<b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/500:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/501:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': (Hotels_AV.Name) + "<br><b>Rating: </b><br>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/502:
import folium

m = folium.Map([51.5, -0.25], zoom_start=10)

test = folium.Html('<b>Hello world</b>', script=True)

popup = folium.Popup(test, max_width=2650)
folium.RegularPolygonMarker(
    location=[51.5, -0.25], popup=popup,
).add_to(m)
575/503:
import folium

m = folium.Map([51.5, -0.25], zoom_start=10)

test = folium.Html('<b>Hello world</b>', script=True)

popup = folium.Popup(test, max_width=2650)
folium.RegularPolygonMarker(
    location=[51.5, -0.25], popup=popup,
).add_to(m)
m
575/504:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
Figure.add_child(Area2)
Area2
575/505:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
Area2
575/506:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
     ''' 
Figure.add_child(Area)
Area.get_root().html.add_child(folium.Element(title_html))
Area2
575/507:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
     ''' 
Figure.add_child(Area)
Area2.get_root().html.add_child(folium.Element(title_html))
Area2
575/508:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    "<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
     ''' 
Figure.add_child(Area2)
Area2.get_root().html.add_child(folium.Element(title_html))
Area2
575/509:
# MAP for available HOTELS
Hotels_AV=Hotels
Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
    'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
    "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
    #"<br><b>Website: </b>" + (Hotels_AV.Website) +
    "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
Area2=[]
from folium import plugins
from folium.features import DivIcon
Figure=folium.Figure(width=500, height=450)
Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
control_scale=True, zoom_start=12)
# Dots = plugins.MarkerCluster().add_to(Area2)
Dots = folium.map.FeatureGroup().add_to(Area2)

#mini_map = plugins.MiniMap(toggle_display=True)
for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
    # html = Data_Hotels.to_html(
    # classes="table table-striped table-hover table-condensed table-responsive")
    html="<b>" + label +"</b>" + "<br>" + label2
    iframe = folium.IFrame(html, width=450, height=100)

    if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng], 
            popup=folium.Popup(iframe,max_width=500), 
            icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
    #loc=lugares3.iloc[:,0:2]
    #loc=loc.values.tolist()
    #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
     ''' 
Figure.add_child(Area2)
Area2.get_root().html.add_child(folium.Element(title_html))
Area2
575/510:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
575/511: df_unicos=add_POIs_df(df, POIs_ext)
575/512:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
575/513: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
575/514: dat1
575/515:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
575/516:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
575/517: Area
575/518: #Area
575/519:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
575/520: Show_hotels(Hotels)
575/521: #dat1
575/522:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
575/523:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
575/524:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
575/525: len(Day_1)
575/526: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
575/527: #data_day1
575/528:
Area=plot_path(data_day1, base_2)
Area
575/529:
#INPUTS
datetime.datetime.utcnow()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/530:
#INPUTS
datetime.datetime.utcnow()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/531:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime
575/532:
#INPUTS
datetime.datetime.utcnow()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/533: datetime.datetime.utcnow()
575/534: datetime.datetime.cetnow()
575/535: datetime.datetime.now()
575/536:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime
from tkcalendar import Calendar, DateEntry
575/537:
def example1():
    def print_sel():
        print(cal.selection_get())

    top = tk.Toplevel(root)

    cal = Calendar(top,
                   font="Arial 14", selectmode='day',
                   cursor="hand1", year=2018, month=2, day=5)
    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()
575/538: example1
575/539: example1()
575/540:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime
from tkcalendar import Calendar, DateEntry
import tkinter as tk
575/541:
def example1():
    def print_sel():
        print(cal.selection_get())

    top = tk.Toplevel(root)

    cal = Calendar(top,
                   font="Arial 14", selectmode='day',
                   cursor="hand1", year=2018, month=2, day=5)
    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()
575/542: example1()
575/543:
def example2():
    top = tk.Toplevel(root)

    ttk.Label(top, text='Choose date').pack(padx=10, pady=10)

    cal = DateEntry(top, width=12, background='darkblue',
                    foreground='white', borderwidth=2)
    cal.pack(padx=10, pady=10)
575/544: example2()
575/545:
def example2():
    top = tk.Toplevel(root)

    ttk.Label(top, text='Choose date').pack(padx=10, pady=10)

    cal = DateEntry(top, width=12, background='darkblue',
                    foreground='white', borderwidth=2)
    cal.pack(padx=10, pady=10)

root = tk.Tk()
s = ttk.Style(root)
s.theme_use('clam')

ttk.Button(root, text='Calendar', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='DateEntry', command=example2).pack(padx=10, pady=10)

root.mainloop()
575/546: example2()
575/547:
try:
    import tkinter as tk
    from tkinter import ttk
except ImportError:
    import Tkinter as tk
    import ttk

from tkcalendar import Calendar, DateEntry

def example1():
    def print_sel():
        print(cal.selection_get())

    top = tk.Toplevel(root)

    cal = Calendar(top, font="Arial 14", selectmode='day', locale='en_US',
                   cursor="hand1", year=2018, month=2, day=5)

    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()


def example2():

    top = tk.Toplevel(root)

    cal = Calendar(top, selectmode='none')
    date = cal.datetime.today() + cal.timedelta(days=2)
    cal.calevent_create(date, 'Hello World', 'message')
    cal.calevent_create(date, 'Reminder 2', 'reminder')
    cal.calevent_create(date + cal.timedelta(days=-2), 'Reminder 1', 'reminder')
    cal.calevent_create(date + cal.timedelta(days=3), 'Message', 'message')

    cal.tag_config('reminder', background='red', foreground='yellow')

    cal.pack(fill="both", expand=True)
    ttk.Label(top, text="Hover over the events.").pack()


def example3():
    top = tk.Toplevel(root)

    ttk.Label(top, text='Choose date').pack(padx=10, pady=10)

    cal = DateEntry(top, width=12, background='darkblue',
                    foreground='white', borderwidth=2, year=2010)
    cal.pack(padx=10, pady=10)
575/548:

root = tk.Tk()
ttk.Button(root, text='Calendar', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='Calendar with events', command=example2).pack(padx=10, pady=10)
ttk.Button(root, text='DateEntry', command=example3).pack(padx=10, pady=10)

root.mainloop()
575/549:

root = tk.Tk()
ttk.Button(root, text='Calendar', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='Calendar with events', command=example2).pack(padx=10, pady=10)
ttk.Button(root, text='DateEntry', command=example3).pack(padx=10, pady=10)

root.mainloop()
575/550:

root = tk.Tk()
ttk.Button(root, text='Calendar', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='Calendar with events', command=example2).pack(padx=10, pady=10)
ttk.Button(root, text='DateEntry', command=example3).pack(padx=10, pady=10)

root.mainloop()
575/551:
try:
    import tkinter as tk
    from tkinter import ttk
except ImportError:
    import Tkinter as tk
    import ttk

from tkcalendar import Calendar, DateEntry

def example1():
    def print_sel():
        print(cal.selection_get())

    top = tk.Toplevel(root)

    cal = Calendar(top, font="Arial 14", selectmode='day', locale='en_US',
                   cursor="hand1", year=2018, month=2, day=5)

    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()


def example2():

    top = tk.Toplevel(root)

    cal = Calendar(top, selectmode='none')
    date = cal.datetime.today() + cal.timedelta(days=2)
    cal.calevent_create(date, 'Hello World', 'message')
    cal.calevent_create(date, 'Reminder 2', 'reminder')
    cal.calevent_create(date + cal.timedelta(days=-2), 'Reminder 1', 'reminder')
    cal.calevent_create(date + cal.timedelta(days=3), 'Message', 'message')

    cal.tag_config('reminder', background='red', foreground='yellow')

    cal.pack(fill="both", expand=True)
    ttk.Label(top, text="Hover over the events.").pack()


def example3():
    top = tk.Toplevel(root)

    ttk.Label(top, text='Choose date').pack(padx=10, pady=10)

    cal = DateEntry(top, width=12, background='darkblue',
                    foreground='white', borderwidth=2, year=2010)
    cal.pack(padx=10, pady=10)
575/552:

root = tk.Tk()
ttk.Button(root, text='Calendar', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='Calendar with events', command=example2).pack(padx=10, pady=10)
ttk.Button(root, text='DateEntry', command=example3).pack(padx=10, pady=10)

root.mainloop()
575/553:
import tkinter as tk
from tkinter import ttk

from tkcalendar import Calendar

def example1():
    def print_sel():
        print('last_date="{}"'.format(cal.selection_get()))
    def quit1():
        top.destroy()

    top = tk.Toplevel(root)

    cal = Calendar(top,
                   font="Arial 14", selectmode='day',
                   cursor="hand1", year=2018, month=2, day=5)
    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()
    ttk.Button(top, text="exit", command=quit1).pack()

def example2():
    def print_sel():
        print('next_date="{}"'.format(cal.selection_get()))
    def quit1():
        top.destroy()

    top = tk.Toplevel(root)

    cal = Calendar(top,
                   font="Arial 14", selectmode='day',
                   cursor="hand1", year=2018, month=2, day=5)
    cal.pack(fill="both", expand=True)
    ttk.Button(top, text="ok", command=print_sel).pack()
    ttk.Button(top, text="exit", command=quit1).pack()

root = tk.Tk()
s = ttk.Style(root)
s.theme_use('clam')

ttk.Button(root, text='Last Date', command=example1).pack(padx=10, pady=10)
ttk.Button(root, text='Next Date', command=example2).pack(padx=10, pady=10)

root.mainloop()
575/554:
#INPUTS
start_time = date(year=2020, month=1, day=31)
finish_time = datetime.datetime.now()
difference = later_time - first_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
>>> divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/555:
#INPUTS
start_time = date(year=2020, month=1, day=31)
finish_time = datetime.datetime.now()
difference = later_time - first_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/556:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime import date, time, datetime
from tkcalendar import Calendar, DateEntry
import tkinter as tk
575/557:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
from tkcalendar import Calendar, DateEntry
import tkinter as tk
575/558:
#INPUTS
start_time = date(year=2020, month=1, day=31)
finish_time = datetime.datetime.now()
difference = later_time - first_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/559:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = later_time - first_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/560:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
days=5
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/561:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
(0, 8)      # 0 minutes, 8 seconds
datetime.datetime.now()
575/562:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
seconds_in_day = 24 * 60 * 60
datetime.timedelta(0, 8, 562000)
divmod(difference.days * seconds_in_day + difference.seconds, 60)
575/563:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
575/564:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
difference
575/565:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
difference.days
575/566:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.hours
575/567:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
575/568:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/569:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
575/570: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/571: POIs_ext
575/572:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/573: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/574: POIs_ext
575/575: POIs_ext.Rating[0]
575/576: POIs_ext.Rating
575/577: POIs_ext.Rating[1]
575/578:
Hotels.Rating[0]
#User should choose one or more hotels for the stay
575/579:
Hotels.Rating
#User should choose one or more hotels for the stay
575/580:
Hotels.Rating[11]
#User should choose one or more hotels for the stay
575/581: POIs_ext.Rating
575/582: POIs_ext.Rating[3]
575/583: type(POIs_ext.Rating[3])
575/584:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/585: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/586: POIs_ext
575/587: POIs_ext.Rating[3]
575/588:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/589: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/590: POIs_ext.Rating[3]
575/591: (POIs_ext.Rating)
575/592: POIs_ext
575/593:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] = ""].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/594:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/595:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == " "].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/596: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/597: POIs_ext.Rating[3]
575/598:
POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
#POIs_ext
575/599:
POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
#POIs_ext
575/600:
POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
POIs_ext
575/601:
#POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
POIs_ext
575/602:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/603: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/604:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/605: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/606:
#POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
POIs_ext
575/607:
POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] == ""].index, inplace = True)
#POIs_ext
575/608: POIs_ext
575/609:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext['Rating'] >= 2].index, inplace = True)
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/610: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/611:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext.drop(POIs_ext[POIs_ext["Rating"].str.contains("")==False])
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/612: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/613: POIs_ext
575/614:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/615: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/616: POIs_ext
575/617:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/618: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/619: POIs_ext
575/620:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/621: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/622: POIs_ext
575/623: POIs_ext[POIs_ext["Rating"].str.contains("")==False]
575/624: POIs_ext["Rating"]
575/625: POIs_ext.sort_values("Ratings", ascending=True)
575/626:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/627: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/628: POIs_ext
575/629: POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, False])
575/630: POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, True])
575/631: POIs_ext.sort_values(["Popularity","Rating"], ascending=[True, True])
575/632: POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, False])
575/633:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/634: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/635:
RR=POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, False])
RR
575/636:
#Select POIs
importlib.reload(mymodule)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/637:
#Select POIs
importlib.reload(mymodule)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/638:
#Select POIs
import importlib
importlib.reload(mymodule)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/639:
#Select POIs
import importlib
importlib.reload(GetPlaces)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/640:
#Select POIs
import importlib
importlib.reload(Analysis_data.py)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/641:
#Select POIs
import importlib
importlib.reload(Analysis_data)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/642:
#Select POIs
%load_ext autoreload
%autoreload
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/643: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
575/644:
RR=POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, False])
RR
575/645:
#Select POIs
import importlib
importlib.reload(Analysis_data.py)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/646:
#Select POIs
import importlib
importlib.reload(Analysis_data)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/647:
#Select POIs
import imp
imp.reload(Analysis_data)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/648:
#Select POIs
import imp
imp.reload(GetPlaces)
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/649:
#Select POIs
import imp
imp.reload(GetPlaces())
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/650:
#Select POIs
import imp
imp.reload(GetPlaces
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/651:
#Select POIs
import imp
imp.reload(GetPlaces

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/652:
#Select POIs
import imp
imp.reload(GetPlaces)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/653:
#Select POIs
import imp
imp.reload(Analysis_data)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/654:
#Select POIs
import importlib
importlib.reload(Analysis_data)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/655:
#Select POIs
import importlib
importlib.reload(Analysis_data)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/656:
#Select POIs
import importlib
importlib.reload(Analysis_data.GetPlaces)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/657:
#Select POIs
from importlib import reload
importlib.reload(Analysis_data)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
575/658:
#Select POIs
from importlib import reload
importlib.reload(Analysis_data)

def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
576/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
from tkcalendar import Calendar, DateEntry
import tkinter as tk
576/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
576/3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
576/4:
df=pd.read_csv(path_file_2)
lat_med=df['LAT_google'].median(axis=0)
lon_med=df['LON_google'].median(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
#define loc med from df selected
576/5:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    # POIs_ext.Rating=POIs_ext.Rating.apply(str)
    # POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
576/6: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
576/7:
RR=POIs_ext.sort_values(["Popularity","Rating"], ascending=[False, False])
RR
576/8:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
576/9: Hotels=GetHotels(api_key, location_med, Points[0], days)
576/10:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
576/11: Show_hotels(Hotels)
576/12:
df=pd.read_csv(path_file_2)
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
#define loc med from df selected
576/13: Hotels=GetHotels(api_key, location_med, Points[0], days)
576/14:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
576/15: Show_hotels(Hotels)
576/16:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
576/17:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
576/18: df_unicos=add_POIs_df(df, POIs_ext)
576/19:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
576/20: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
576/21:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
576/22: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
576/23:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
576/24: df_unicos=add_POIs_df(df, POIs_ext)
576/25:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
576/26: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
576/27:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
576/28:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
576/29: Area
576/30:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
576/31:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
576/32: len(Day_1)
576/33: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
576/34: #data_day1
576/35:
Area=plot_path(data_day1, base_2)
Area
578/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
from tkcalendar import Calendar, DateEntry
import tkinter as tk
578/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
NAT=1
REC=2
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
578/3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
578/4:
df=pd.read_csv(path_file_2)
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
#define loc med from df selected
578/5:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    return Hotels
578/6:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
578/7: Hotels=GetHotels(api_key, location_med, Points[0], days)
578/8: POIs_ext=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
578/9:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/10: Show_hotels(Hotels)
578/11:
#Choose n°3
Hotel_Choosen=Hotels.iloc[3] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
578/12:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
578/13: df_unicos=add_POIs_df(df, POIs_ext)
578/14:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
578/15: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
578/16:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
578/17:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
578/18: Area
578/19:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
578/20:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
578/21: len(Day_1)
578/22: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
578/23: #data_day1
578/24:
Area=plot_path(data_day1, base_2)
Area
578/25:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
578/26: len(Day_1)
578/27: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
578/28: #data_day1
578/29:
Area=plot_path(data_day1, base_2)
Area
578/30:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
578/31:
df=pd.read_csv(path_file_2)
lat_med=df['LAT_google'].mean(axis=0)
lon_med=df['LON_google'].mean(axis=0)
lat_me=str(lat_med)
lon_med=str(lon_med)
location_med= lat_me + "," + lon_med
location_med
#define loc med from df selected
578/32: df
578/33:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
578/34:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
578/35: len(Day_1)
578/36: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
578/37: #data_day1
578/38:
Area=plot_path(data_day1, base_2)
Area
578/39:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
578/40: len(Day_1)
578/41: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
578/42: #data_day1
578/43:
Area=plot_path(data_day1, base_2)
Area
578/44:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[4][1])
#[day][1]
578/45: len(Day_1)
578/46: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
578/47: #data_day1
578/48:
Area=plot_path(data_day1, base_2)
Area
578/49: POIs_ext=GetPOIs(api_key, location_med, Points[4], days, CLT, NAT, REC, SPEED)
578/50:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
578/51: df_unicos=add_POIs_df(df, POIs_ext)
578/52:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
578/53: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
578/54: #dat1
578/55:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
578/56:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
578/57: Area
578/58: Hotels
578/59: Hotels.index
578/60: Hotels.reset_index
578/61: Hotels.reset_index()
578/62:
dd=Hotels.reset_index()
dd.index()
578/63:
dd=Hotels.reset_index()
dd.index
578/64:
dd=Hotels.reset_index()
dd
578/65:
dd=Hotels.reset_index()
dd.index
578/66:
dd=Hotels.reset_index()
dd.index()
578/67:
dd=Hotels.reset_index()
dd.index
578/68:
dd=Hotels.reset_index()
dd.index[0]
578/69:
dd=Hotels.reset_index(drop=True)
dd.index[0]
578/70:
dd=Hotels.reset_index(drop=True)
dd
578/71:
dd=Hotels.reset_index(drop=True)
dd.index
578/72: dd
578/73: dd.index()
578/74: dd.index
578/75: df.index
578/76: dd.Index
578/77: dd.index = np.arange(1, len(dd))
578/78: dd.index = np.arange(1, len(dd)+1)
578/79:
dd.index = np.arange(1, len(dd)+1)
dd
578/80: Hotels.index
578/81:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
578/82: Hotels=GetHotels(api_key, location_med, Points[0], days)
578/83: Hotels.index
578/84: Hotels.index[0]
578/85: Hotels.index[1]
578/86:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + index +"</b>" + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/87: Show_hotels(Hotels)
578/88:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>" + str(index) +"</b>" + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/89: Show_hotels(Hotels)
578/90:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>index: " + str(index) +"</b>" + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/91: Show_hotels(Hotels)
578/92:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/93: Show_hotels(Hotels)
578/94:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
578/95: Show_hotels(Hotels)
578/96:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
578/97: len(Day_1)
580/1: from Main_path import Itinerary_creation
580/2:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
Title="Libro_prova"
CLT=3
NAT=1
REC=0
SPEED=3
BUDGET=2

Day_1=Itinerary_creation(Title, start_time, finish_time, CLT, NAT, REC, SPEED, BUDGET)
580/3:
from Main_path import Itinerary_creation
from datetime import date, time, datetime
580/4:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
Title="Libro_prova"
CLT=3
NAT=1
REC=0
SPEED=3
BUDGET=2

Day_1=Itinerary_creation(Title, start_time, finish_time, CLT, NAT, REC, SPEED, BUDGET)
586/1:
from Main_path import Itinerary_creation
from datetime import date, time, datetime
586/2:
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
Title="Libro_prova"
CLT=3
NAT=1
REC=0
SPEED=3
BUDGET=2

Day_1=Itinerary_creation(Title, start_time, finish_time, CLT, NAT, REC, SPEED, BUDGET)
588/1:
# dataset definition (it consists of 2D data points)

NumberOfDataPoints = 100
#test 1
x = np.linspace(0,40,NumberOfDataPoints)
y = x + np.random.randn(1,NumberOfDataPoints)*5
#test 2
#x = np.array([-5,-4.8,-4.7,-4.5,-4,-3.9,-3.5,-3,-2.7,-2.2,-1.6,-1,-0.5,0.3,0.8,1.3,  2,2.8,3 ])
#y = np.array([26,  25,  24,  20,18,  16,11.8,13,   9, 4.5, 2.5, 1, 0.5,  1,  1,  2,3.8,  7,7 ])
#test 3
#x = np.linspace(-10, 10, num=NumberOfDataPoints)
#y = x**3 + (np.random.rand(NumberOfDataPoints)*200-0.5)

# visualize data
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.scatter(x, y)
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.show()
588/2:
#import
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
588/3:
#import
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
588/4:
# dataset definition (it consists of 2D data points)

NumberOfDataPoints = 100
#test 1
x = np.linspace(0,40,NumberOfDataPoints)
y = x + np.random.randn(1,NumberOfDataPoints)*5
#test 2
#x = np.array([-5,-4.8,-4.7,-4.5,-4,-3.9,-3.5,-3,-2.7,-2.2,-1.6,-1,-0.5,0.3,0.8,1.3,  2,2.8,3 ])
#y = np.array([26,  25,  24,  20,18,  16,11.8,13,   9, 4.5, 2.5, 1, 0.5,  1,  1,  2,3.8,  7,7 ])
#test 3
#x = np.linspace(-10, 10, num=NumberOfDataPoints)
#y = x**3 + (np.random.rand(NumberOfDataPoints)*200-0.5)

# visualize data
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.scatter(x, y)
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.show()
588/5:
# linear regression
# Importing linear regression model
from sklearn.linear_model import LinearRegression

# fitting the lin reg to our dataset
LinRegModel = LinearRegression()
LinRegModel.fit(x.reshape(-1,1),y.reshape(-1,1))

y_LinReg = LinRegModel.predict(x.reshape(-1,1))

print('Intercept:', LinRegModel.intercept_)
print('Slope:', LinRegModel.coef_)
588/6:
# show the linear regression line

figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.plot(x, y_LinReg, 'r',label='Lin Reg')
plt.scatter(x, y, s=15, label='Data')
plt.xlim([np.min(x)-1,np.max(x)+1])
plt.ylim([np.min(y)-1,np.max(y)+1])
plt.legend()
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.show()
588/7:
from sklearn.metrics import mean_squared_error
print('RMSE for Linear Regression=>',np.sqrt(mean_squared_error(y.reshape(-1,1),y_LinReg)))
588/8:
# play with prediction
newValuex = np.array([0])
y_newLinReg = LinRegModel.predict(newValuex.reshape(-1,1)) 

print('Predicted output of Lin Reg model:', y_newLinReg)
588/9:
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.scatter(x, y,s=15, label='Data')
plt.scatter(newValuex, y_newLinReg, marker='s', color='black',  label='New Data')
plt.plot(x, y_LinReg, 'r',label='Lin Reg')

plt.xlim([np.min(x)-1,np.max(x)+1])
plt.ylim([np.min(y)-1,np.max(y)+1])
plt.legend()
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.show()
588/10:
# extension to polynomial regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# creating the poly regression model
polynomialOrder = 2
PolyRegression  = Pipeline( [('polynomial',PolynomialFeatures(degree=polynomialOrder)),('modal',LinearRegression()) ] )


#fitting to training data
PolyRegression.fit(x.reshape(-1,1),y.reshape(-1,1) )
588/11:
# fitting
y_PolyRegr = PolyRegression.predict(x.reshape(-1,1))

# new data
y_newPolyReg = PolyRegression.predict(newValuex.reshape(-1,1)) 



figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.scatter(x, y,s=15, label='Data')
plt.scatter(newValuex, y_newLinReg, marker='s', color='black',  label='New Data Lin Reg')
plt.scatter(newValuex, y_newPolyReg, marker='s', color='m',  label='New Data Poly Reg')
plt.plot(x, y_LinReg, 'r',label='Lin Reg')
plt.plot(x,y_PolyRegr,color='g',label='Polynomial Regression')
plt.xlim([np.min(x)-1,np.max(x)+1])
plt.ylim([np.min(y)-1,np.max(y)+1])
plt.legend()
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.show()
588/12:
# comparison of multiple polynomial

# linear
LinRegModel = LinearRegression()
LinRegModel.fit(x.reshape(-1,1),y.reshape(-1,1))
y_pred = LinRegModel.predict(x.reshape(-1,1))

plt.figure(figsize=(10,6))
plt.scatter(x,y,s=15,label='Data')
plt.plot(x,y_pred,color='r',label='Linear Regression')

#polynomial
for i in np.array([2,3]):
  PolyRegression  = Pipeline( [('polynomial',PolynomialFeatures(degree=i)),('modal',LinearRegression()) ] )
  PolyRegression.fit(x.reshape(-1,1), y.reshape(-1,1))
  y_plot = PolyRegression.predict(x.reshape(-1,1))
  plt.plot(x, y_plot, label='polynomial of order {}' .format(i))

  
plt.xlabel('x value',fontsize=16)
plt.ylabel('y value',fontsize=16)
plt.legend()
plt.show()
590/1:
# IMPORT THE TOOLS TO BE USED
import cv2
import numpy as np
import os
import sys
from keras.datasets import mnist
from keras.layers import Dense, Flatten
from keras.layers.convolutional import Conv2D
from keras.models import Sequential
from keras.utils import to_categorical
import matplotlib.pyplot as plt
592/1:
#import library and dataset (upload local dataset)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
# load dataset
pima = pd.read_csv("pima-indians-diabetes.csv", header=None, names=col_names)
592/2:
#show what the dataset contains
print('Dataset size:',pima.shape)
pima.head()

# EXPLAIN WHAT THE DATASET CONTAINS
#Pregnant: Number of times pregnant
#Glucose: Plasma glucose concentration at 2 hours in an oral glucose tolerance test
#bp - BloodPressure: Diastolic blood pressure (mm Hg)
#skin - SkinThickness: Triceps skin fold thickness (mm)
#insulin: 2-Hour serum insulin (mu U/ml)
#BMI: Body mass index (weight in kg/(height in m)^2)
#Pedigree: Diabetes pedigree function
#Age: Age (years)
#Label 1/0 means "has diabete or not"
592/3:
#some analyses on the dataset
pima.describe()
594/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
594/2:
#Loading the dataset
col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

diabetes_data = pd.read_csv("pima-indians-diabetes.csv", header=None, names=col_names)

diabetes_data.head()
594/3:
#scaling (it is always advisable to bring all the features to the same scale for applying distance based algorithms like KNN)
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(diabetes_data.drop(["Outcome"],axis = 1),),
        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])

X.head()
594/4:
#importing train_test_split
from sklearn.model_selection import train_test_split
y = diabetes_data.Outcome
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)

print('Size of complete dataset: {}' .format(y.size))
print('Size of training dataset: {}' .format(y_train.size))
print('Size of   test   dataset: {}' .format(y_test.size))
594/5:
# KNN classification
from sklearn.neighbors import KNeighborsClassifier
MAX_numberOfNearestNeighbor = 9

testing_error = []
training_error = []
train_accuracy =np.empty(MAX_numberOfNearestNeighbor)
test_accuracy = np.empty(MAX_numberOfNearestNeighbor)

# we check the impact of the number of nearest neighbors (K)
for i in range(1,MAX_numberOfNearestNeighbor):
   #set the model (KNN) 
   knn = KNeighborsClassifier(n_neighbors=i)
   #fit the model to training dataset
   knn.fit(X_train,y_train)

   #assess the model on testing dataset
   prediction_train = knn.predict(X_train)
   train_accuracy[i] = knn.score(X_train, y_train)
   training_error.append(np.mean(prediction_train != y_train))

   #assess the model on testing dataset
   prediction_test = knn.predict(X_test)
   testing_error.append(np.mean(prediction_test != y_test)) 
   test_accuracy[i] = knn.score(X_test, y_test)
594/6:
# plot the accuracy
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 10),  facecolor='w', edgecolor='k')
plt.subplot(211)
plt.plot(range(1,MAX_numberOfNearestNeighbor),training_error,marker='o', markersize=2, label='Training')
plt.plot(range(1,MAX_numberOfNearestNeighbor),testing_error,marker='o', markersize=2, label='Testing')
plt.xlabel('# of NN', fontsize=18)
plt.ylabel('Error', fontsize=18)
plt.legend()

plt.subplot(212)
plt.plot(range(1,MAX_numberOfNearestNeighbor),train_accuracy[1:MAX_numberOfNearestNeighbor], label='Training')
plt.plot(range(1,MAX_numberOfNearestNeighbor),test_accuracy[1:MAX_numberOfNearestNeighbor], label='Testing')
plt.xlabel('# of NN', fontsize=18)
plt.ylabel('Accuracy', fontsize=18)
plt.legend()
plt.show()
594/7:
# another example with one selected K
K = 10

knn = KNeighborsClassifier(n_neighbors=K)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)

# Confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn import metrics

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, fmt='g',  cmap="YlGnBu")
plt.title('Confusion matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()


TruePositive = cnf_matrix[1,1]
TrueNegative = cnf_matrix[0,0]
FalsePositive = cnf_matrix[0,1]
FalseNegative = cnf_matrix[1,0]
print('True Positive: {}' .format(TruePositive))
print('True Negative: {}' .format(TrueNegative))
print('False Positive: {}' .format(FalsePositive))
print('False Negative: {}' .format(FalseNegative))

#True Positive Rate = True Positive / ( True Positive + False Negative)      - also known as detection probability
#False Positive Rate = False Positive / ( False Positive + True Negative)    - also known as false alarm

print('TPR: {}' .format(TruePositive/(TruePositive+FalseNegative)))
print('FPR: {}' .format(FalsePositive/(FalsePositive+TrueNegative)))
594/8:
# other common metrics
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))
596/1:
# definition of a function to fit a probability distribution to a univariate data sample
def fit_distribution(data):
    # estimate parameters
    mu = np.mean(data)
    sigma = np.std(data)
    print(mu, sigma)
    # fit distribution
    dist = norm(mu, sigma)
    return dist
596/2:
# Check which points are class 0 and which points are class 1
print(y[1])
print(X[1])
print(y[2])
print(X[2])
596/3:
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
from matplotlib.pyplot import figure
from sklearn.datasets import make_blobs


# generate 2d classification dataset
X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) #generate blobs of points with a Gaussian distribution. 100 examples with two numerical input variables, each assigned one of two classes. 
#X[y == 0] = X[y == 0]-6 # we will uncomment this line later on for testing something 



# visualization
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.plot(X[:, 0], X[:, 1],marker='o', linestyle='none',markersize=5)
#plt.scatter(X[:, 0], X[:, 1],marker='o', c=y, s=20)
plt.xlabel('x value',fontsize=18)
plt.ylabel('y value',fontsize=18)
plt.show()
596/4:
# Check which points are class 0 and which points are class 1
print(y[1])
print(X[1])
print(y[2])
print(X[2])
596/5:
# definition of a function to fit a probability distribution to a univariate data sample
def fit_distribution(data):
    # estimate parameters
    mu = np.mean(data)
    sigma = np.std(data)
    print(mu, sigma)
    # fit distribution
    dist = norm(mu, sigma)
    return dist
596/6:
#SPLIT DATA
# sort data into classes
Class0 = X[y == 0]
Class1 = X[y == 1]
print(Class0.shape, Class1.shape)

figure(num=None, figsize=(8, 10),  facecolor='w', edgecolor='k')
plt.subplot(211)
plt.plot(Class0,marker='o', linestyle='none',markersize=2)
plt.xlabel('Samples')
plt.ylabel('Value')
plt.legend(['x coord.','y coord.'])
plt.title('Class 0')
plt.subplot(212)
plt.plot(Class1,marker='o', linestyle='none',markersize=2)
plt.xlabel('Samples')
plt.ylabel('Value')
plt.legend(['x coord.','y coord.'])
plt.title('Class 1')
plt.show()
596/7:
#Calculate the prior probabilities of the two classes
priory0 = len(Class0) / len(X)
priory1 = len(Class1) / len(X)
print('Probabilities of the two classes:')
print(priory0*100, priory1*100)
596/8:
# Fit distribution
print('mean ------------------ std')
# create PDFs for y==0
X1y0 = fit_distribution(Class0[:, 0])
X2y0 = fit_distribution(Class0[:, 1])
# create PDFs for y==1
X1y1 = fit_distribution(Class1[:, 0])
X2y1 = fit_distribution(Class1[:, 1])
596/9:
#function definition
def probability(X, prior, dist1, dist2):
    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])
596/10:
# classify one example
Xsample, ysample = X[0], y[0]
print('Input sample: ')
print(Xsample)
print('Probabilities')
py0 = probability(Xsample, priory0, X1y0, X2y0)
py1 = probability(Xsample, priory1, X1y1, X2y1)
print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))
print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))
print('Truth: y=%d' % ysample)

figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.plot(X[:, 0], X[:, 1],marker='o', linestyle='none',markersize=5)
plt.scatter(Xsample[0],Xsample[1],  marker='s', color='red',s=50)
plt.xlabel('x value',fontsize=18)
plt.ylabel('y value',fontsize=18)
plt.show()
596/11:
#some fun with new data
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.plot(X[:, 0], X[:, 1],marker='o', linestyle='none',markersize=5)

myXsample = ([-2,6])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='red',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 1:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))

myXsample = ([-10,-4])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='yellow',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 2:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))

myXsample = ([-8.5,-3])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='green',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 2:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))



plt.xlabel('x value',fontsize=18)
plt.ylabel('y value',fontsize=18)
plt.show()
596/12:
#some fun with new data
figure(num=None, figsize=(6, 6),  facecolor='w', edgecolor='k')
plt.plot(X[:, 0], X[:, 1],marker='o', linestyle='none',markersize=5)

myXsample = ([-2,6])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='red',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 1:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))

myXsample = ([-10,-4])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='yellow',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 2:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))

myXsample = ([-8.5,-13])
plt.scatter(myXsample[0],myXsample[1],  marker='s', color='green',s=50)
py0 = probability(myXsample, priory0, X1y0, X2y0)
py1 = probability(myXsample, priory1, X1y1, X2y1)
print('Sample 2:')
print('P(y=0 | %s) = %.3f' % (myXsample, py0*100))
print('P(y=1 | %s) = %.3f' % (myXsample, py1*100))



plt.xlabel('x value',fontsize=18)
plt.ylabel('y value',fontsize=18)
plt.show()
598/1:
# import
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
592/4:
#import library and dataset (upload local dataset)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

col_names =['Age', 'Sex', 'Chest_pain', 'trtbps',   'chol', 'fbs'   ,'restecg'  ,'thalachh' ,'exng',    'oldpeak',  'slp',  'caa',  'thall']

# load dataset
pima = pd.read_csv("heart.csv")
592/5:
#show what the dataset contains
print('Dataset size:',pima.shape)
pima.head()

# EXPLAIN WHAT THE DATASET CONTAINS
#Pregnant: Number of times pregnant
#Glucose: Plasma glucose concentration at 2 hours in an oral glucose tolerance test
#bp - BloodPressure: Diastolic blood pressure (mm Hg)
#skin - SkinThickness: Triceps skin fold thickness (mm)
#insulin: 2-Hour serum insulin (mu U/ml)
#BMI: Body mass index (weight in kg/(height in m)^2)
#Pedigree: Diabetes pedigree function
#Age: Age (years)
#Label 1/0 means "has diabete or not"
592/6:
#some analyses on the dataset
pima.describe()
592/7:
# how many with diabetes?
pima.groupby("output").size()
592/8:
#Explore the data
from matplotlib import pyplot 
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = [20, 10] # set the figure size 
pima.hist()
plt.show()

# Density plots for all attributes
pima.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
plt.show()
592/9:
#Check possible correlations among input parameters
import seaborn as sns

correlationMatrix = pima.corr()
print(correlationMatrix)
592/10:
# plot correlation matrix in a better way
mask = np.triu(np.ones_like(correlationMatrix, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(240, 10)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(correlationMatrix, annot=True, cmap=cmap, vmax=1, center=0, linewidths=.15)
602/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church", "park"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
602/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
602/3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
602/4:
df=pd.read_csv(path_file_2)

#define loc med from df selected
602/5:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
return location_med
602/6: df
602/7:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
602/8:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
602/9: Hotels=GetHotels(api_key, location_med, Points[0], days)
602/10:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
    return location_med
602/11: location_med=position_med(df)
602/12: df
602/13:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
602/14:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
602/15: Hotels=GetHotels(api_key, location_med, Points[0], days)
602/16:
#Choose n°3
Hotel_Choosen=Hotels.iloc[2] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
602/17: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
602/18:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
602/19: df_unicos=add_POIs_df(df, POIs_ext)
602/20:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
602/21: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
602/22:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
602/23:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
602/24: Area
602/25:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
602/26:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
602/27: len(Day_1)
602/28: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/29: #data_day1
602/30:
Area=plot_path(data_day1, base_2)
Area
602/31:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
602/32: len(Day_1)
602/33: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/34: #data_day1
602/35:
Area=plot_path(data_day1, base_2)
Area
602/36:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
602/37: len(Day_1)
602/38: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/39: #data_day1
602/40:
Area=plot_path(data_day1, base_2)
Area
602/41:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[3][1])
#[day][1]
602/42: len(Day_1)
602/43: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/44: #data_day1
602/45:
Area=plot_path(data_day1, base_2)
Area
602/46:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[4][1])
#[day][1]
602/47: len(Day_1)
602/48: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/49: #data_day1
602/50:
Area=plot_path(data_day1, base_2)
Area
602/51:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
602/52: len(Day_1)
602/53: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/54: #data_day1
602/55:
Area=plot_path(data_day1, base_2)
Area
602/56:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
602/57: len(Day_1)
602/58: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
602/59: #data_day1
602/60:
Area=plot_path(data_day1, base_2)
Area
602/61:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Hotels.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
602/62: Rest=GetRestaurants(api_key, location_med, Points[3], days)
602/63: Show_hotels(Rest)
602/64:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
602/65: Show_hotels(Rest)
602/66: Rest
602/67: Rest=GetRestaurants(api_key, location_med, Points[1], days)
602/68: Rest
602/69: Rest
602/70: Rest=GetRestaurants(api_key, location_med, Points[2], days)
602/71: Rest
602/72: Points[2]
602/73: Rest=GetRestaurants(api_key, location_med, Points[6], days)
602/74: Rest
604/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church", "park"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
604/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
604/3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
604/4:
df=pd.read_csv(path_file_2)

#define loc med from df selected
604/5:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
    return location_med
604/6: location_med=position_med(df)
604/7: df
604/8:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Hotels.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
604/9:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
604/10: Rest=GetRestaurants(api_key, location_med, Points[6], days)
604/11: Rest
604/12:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
604/13: Rest=GetRestaurants(api_key, location_med, Points[6], days)
604/14: Rest
604/15: Show_hotels(Rest)
604/16:
# MAP for available HOTELS
def Show_hotels(Hotels):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/17: Show_hotels(Rest)
604/18:
# MAP for available HOTELS
def Show_places(Hotels, type):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)
         if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/19:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)
         if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/20:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)
        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/21:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)
        
        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/22:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
604/23:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon.add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/24:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        if type(lat)!=type(None):
                folium.Marker(
                location=[lat, lng], 
                popup=folium.Popup(iframe,max_width=500), 
                icon=Icon.add_to(Dots))
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/25: Show_hotels(Rest)
604/26: Show_places(Rest, "REST")
604/27:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-food", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/28: Show_places(Rest, "REST")
604/29: Rest=GetRestaurants(api_key, location_med, Points[3], days)
604/30: Rest
604/31:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='red', icon="glyphicon-cutlery", icon_color="white")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/32: Show_places(Rest, "REST")
604/33:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    title_html = '''
        <head><style> html { overflow-y: hidden; } </style></head>
        <h3 align="center" style="font-size:18px"><b>Hotels</b></h3>
        ''' 
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/34: Show_places(Rest, "REST")
604/35:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/36: Show_places(Rest, "REST")
604/37:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=450, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/38: Show_places(Rest, "REST")
604/39: Show_places(Rest, "HOT")
604/40: Show_places(Rest, "HOTEL")
604/41: Show_places(Hotels, "HOTEL")
604/42: Hotels=GetHotels(api_key, location_med, Points[0], days)
604/43:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
604/44: Hotels=GetHotels(api_key, location_med, Points[0], days)
604/45: Show_places(Hotels, "HOTEL")
604/46: Show_places(Rest, "REST")
604/47:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=400, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/48: Show_places(Hotels, "HOTEL")
604/49:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=350, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
604/50: Show_places(Hotels, "HOTEL")
604/51: Show_places(Rest, "REST")
604/52:
#Choose n°3
Hotel_Choosen=Hotels.iloc[2] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
604/53:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
604/54: df_unicos=add_POIs_df(df, POIs_ext)
604/55:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
604/56: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
604/57: #dat1
604/58:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
604/59: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
604/60:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
604/61: df_unicos=add_POIs_df(df, POIs_ext)
604/62:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
604/63: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
604/64:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
604/65:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
604/66: Area
604/67:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
604/68:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
604/69: len(Day_1)
604/70: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
604/71: #data_day1
604/72:
Area=plot_path(data_day1, base_2)
Area
604/73:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[1][1])
#[day][1]
604/74: len(Day_1)
604/75: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
604/76: #data_day1
604/77:
Area=plot_path(data_day1, base_2)
Area
604/78:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[0][1])
#[day][1]
604/79: len(Day_1)
604/80: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
604/81: #data_day1
604/82:
Area=plot_path(data_day1, base_2)
Area
604/83:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
604/84: len(Day_1)
604/85: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
604/86: #data_day1
604/87:
Area=plot_path(data_day1, base_2)
Area
604/88:
location_med=position_med(df)
location_med
604/89: location_med="33.8688, 151.2093"
604/90: Hotels=GetHotels(api_key, location_med, Points[0], days)
604/91:
location_med=position_med(df)
type(location_med)
604/92:
location_med=position_med(df)
type(location_med)
604/93:
location_med=position_med(df)
(location_med)
604/94: location_med="33.8688 , 151.2093"
604/95: Hotels=GetHotels(api_key, location_med, Points[1], days)
604/96: location_med="52.9776 , 56.4504"
604/97: Hotels=GetHotels(api_key, location_med, Points[0], days)
604/98: Show_places(Hotels, "HOTEL")
604/99: location_med="48.8566 , 2.3522"
604/100: Hotels=GetHotels(api_key, location_med, Points[0], days)
604/101: Rest=GetRestaurants(api_key, location_med, Points[3], days)
604/102: Show_places(Hotels, "HOTEL")
604/103: Show_places(Rest, "REST")
604/104: location_med="28.2916, 16.6297"
604/105: Rest=GetRestaurants(api_key, location_med, Points[3], days)
604/106: Show_places(Rest, "REST")
604/107:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    try:
        Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    
    except KeyError:
        Rest=Rest
    
    Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
604/108: Rest=GetRestaurants(api_key, location_med, Points[3], days)
604/109:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
604/110: Rest=GetRestaurants(api_key, location_med, Points[3], days)
604/111: API_values=GetPlaces(api_key, location_med, "restaurants")
604/112: API_values
604/113: Rest=GetRestaurants(api_key, location_med, Points[1], days)
604/114: API_values=GetPlaces(api_key, location_med, "lodging")
604/115: API_values
604/116: location_med="28.2916, 16.6291"
604/117: API_values=GetPlaces(api_key, location_med, "lodging")
604/118: API_values
604/119:
location_med=position_med(df)
(location_med)
604/120: API_values=GetPlaces(api_key, location_med, "lodging")
604/121: API_values
604/122: location_med="28.2916, -16.6291"
604/123: API_values=GetPlaces(api_key, location_med, "lodging")
604/124: API_values
604/125: Rest=GetRestaurants(api_key, location_med, Points[1], days)
604/126: Show_places(Rest, "REST")
604/127: location_med="28.2916, -16.6291"
604/128: Rest=GetRestaurants(api_key, location_med, Points[1], days)
604/129: Rest
604/130: location_med="28.4636, 16.2518"
604/131: API_values=GetPlaces(api_key, location_med, "lodging")
604/132: API_values
604/133: location_med="28.4636, -16.2518"
604/134:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
604/135: Rest=GetRestaurants(api_key, location_med, Points[1], days)
604/136: Rest
604/137: Show_places(Rest, "REST")
604/138:

Area=Show_places(Rest, "REST")
Area.save('Maps/Clean_maps/Maps_path/FOOD_TENERIFE.html')
604/139:

Area=Show_places(Rest, "REST")
Area.save('back-end-python/src/MapsMaps/Clean_maps/Maps_path/FOOD_TENERIFE.html')
604/140:

Area=Show_places(Rest, "REST")
Area.save('back-end-python/src/Maps/Clean_maps/Maps_path/FOOD_TENERIFE.html')
604/141:

Area=Show_places(Rest, "REST")
Area.save('Maps/Clean_maps/Maps_path/FOOD_TENERIFE.html')
604/142:

Area=Show_places(Rest, "REST")
#Area.save('Maps/Clean_maps/Maps_path/FOOD_TENERIFE.html')
Area
608/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church", "park"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
608/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
608/3: location_med="28.1009, -15.4654"
608/4:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
608/5:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
608/6: Rest=GetRestaurants(api_key, location_med, Points[1], days)
608/7: Rest
608/8:

Area=Show_places(Rest, "REST")
Area.save('Maps/Clean_maps/Maps_path/LAS_PALMAS.html')
Area
608/9:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=350, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
608/10:
#Choose n°3
Hotel_Choosen=Hotels.iloc[2] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
608/11:

Area=Show_places(Rest, "REST")
Area.save('Maps/Clean_maps/Maps_path/LAS_PALMAS.html')
Area
608/12:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
608/13: Rest=GetRestaurants(api_key, location_med, Points[1], days)
608/14:

Area=Show_places(Rest, "REST")
Area.save('Maps/Clean_maps/Maps_path/LAS_PALMAS.html')
Area
610/1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church", "park"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
610/2:
#INPUTS
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
610/3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
610/4:
df=pd.read_csv(path_file_2)

#define loc med from df selected
610/5:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
    return location_med
610/6:
# location_med=position_med(df)
# (location_med)
610/7: location_med="28.1009, -15.4654"
610/8:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
    return location_med
610/9: #location_med="28.1009, -15.4654"
610/10: df
610/11:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
610/12:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
610/13:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
610/14: Hotels=GetHotels(api_key, location_med, Points[0], days)
610/15: Points[2]
610/16: API_values=GetPlaces(api_key, location_med, "lodging")
610/17: API_values
610/18:
def position_med(df):
    lat_med=df['LAT_google'].mean(axis=0)
    lon_med=df['LON_google'].mean(axis=0)
    lat_me=str(lat_med)
    lon_med=str(lon_med)
    location_med= lat_me + "," + lon_med
    return location_med
610/19:
location_med=position_med(df)
# (location_med)
610/20:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
610/21:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
610/22:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
610/23: Hotels=GetHotels(api_key, location_med, Points[0], days)
610/24: Rest=GetRestaurants(api_key, location_med, Points[1], days)
610/25: Rest
610/26: Points[2]
610/27: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
610/28:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=350, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
610/29: Show_places(Hotels, "HOTEL")
610/30:

Area=Show_places(Rest, "REST")
#Area.save('Maps/Clean_maps/Maps_path/LAS_PALMAS.html')
Area
610/31:
#HOTELS
def GetHotels(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Hotels=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    Hotels=Hotels.head(days*2)
    Hotels=Hotels.reset_index(drop=True)
    return Hotels
610/32:
def GetRestaurants(api_key, location_med, type_loc, days):
    API_values=GetPlaces(api_key, location_med, type_loc)
    Rest=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #Rest=Rest.head(days*3)
    Rest=Rest.reset_index(drop=True)
    return Rest
610/33:
#Select POIs


def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
    #POIs_ext = POIs_ext.applymap(str)
    POIs_ext=POIs_ext.head(days*SPEED)
    POIs_ext.Rating=POIs_ext.Rating.apply(str)
    POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
610/34: Hotels=GetHotels(api_key, location_med, Points[0], days)
610/35: Points[2]
610/36: Rest=GetRestaurants(api_key, location_med, Points[1], days)
610/37: Rest
610/38: POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
610/39: Hotels.index[1]
610/40:
# MAP for available HOTELS
def Show_places(Hotels, type_place):
    Hotels_AV=Hotels
    Hotels_AV.Rating=Hotels_AV.Rating.apply(str)
    Hotels_AV.Popularity=Hotels_AV.Popularity.apply(str)
    Data_Hotels=pd.DataFrame({'LAT': Hotels_AV.LAT, 'LON': Hotels_AV.LON,
        'labels': "HOTEL", 'Name': '<a href="'+ (Hotels_AV.Website)  +'"target="_blank"> ' + (Hotels_AV.Name) + ' </a>'+ 
        "<br><b>Rating: </b>" + (Hotels_AV.Rating) +
        #"<br><b>Website: </b>" + (Hotels_AV.Website) +
        "<br><b>Popularity: </b>" + (Hotels_AV.Popularity), 'Quotes': Hotels["Last 5 Reviews"]})
    Area2=[]
    from folium import plugins
    from folium.features import DivIcon
    Figure=folium.Figure(width=500, height=450)
    Area2=folium.Map(location=[Hotels["LAT"].iloc[0], Hotels["LON"].iloc[0]],
    control_scale=True, zoom_start=12)
    # Dots = plugins.MarkerCluster().add_to(Area2)
    Dots = folium.map.FeatureGroup().add_to(Area2)

    #mini_map = plugins.MiniMap(toggle_display=True)
    for lat, lng, index, label, label2 in zip(Data_Hotels["LAT"], Data_Hotels["LON"], 
    Data_Hotels.index, Data_Hotels["Name"], Data_Hotels["Quotes"]):
        # html = Data_Hotels.to_html(
        # classes="table table-striped table-hover table-condensed table-responsive")
        html="<b>Index: </b>" + str(index) + "<b><br>" + label +"</b>" + "<br>" + label2
        iframe = folium.IFrame(html, width=350, height=100)

        if type_place=="REST":
            Icon= folium.Icon(color='white', icon="glyphicon-cutlery", icon_color="blue")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to eat?</b></h3>
            ''' 
        elif type_place=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
            title_html = '''
            <head><style> html { overflow-y: hidden; } </style></head>
            <h3 align="center" style="font-size:18px"><b>Where to sleep?</b></h3>
            '''
        elif type_place=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")  
        
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
        #loc=lugares3.iloc[:,0:2]
        #loc=loc.values.tolist()
        #folium.PolyLine(loc, color='green', weight=10, opacity=0.7).add_to(Area)
    
    Figure.add_child(Area2)
    Area2.get_root().html.add_child(folium.Element(title_html))
    return(Area2)
610/41: Show_places(Hotels, "HOTEL")
610/42:

Area=Show_places(Rest, "REST")
#Area.save('Maps/Clean_maps/Maps_path/LAS_PALMAS.html')
Area
610/43:
#Choose n°3
Hotel_Choosen=Hotels.iloc[2] #da aggiungere ad ogni cluster f(days)
Hotel_Choosen
610/44:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': "TOUR", 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name) + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
610/45: df_unicos=add_POIs_df(df, POIs_ext)
610/46:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
610/47: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
610/48: #dat1
610/49:
def plot_path(dat1, Book_name):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>Map path</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/50:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2)
610/51: Area
610/52:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
610/53:
Schedule_day=divide_days(df_unicos,days)
Day_1=(Schedule_day[2][1])
#[day][1]
610/54: len(Day_1)
610/55:
Area=plot_path(data_day1, base_2)
Area
610/56: data_day1=path_dots(Day_1, "Day_1_Angels", Hotel_Choosen)
610/57: #data_day1
610/58:
Area=plot_path(data_day1, base_2)
Area
610/59: Schedule_day
610/60: Schedule_day[0[1]]
610/61: Schedule_day[0][1]
610/62: Schedule_day[0][2]
610/63: Schedule_day[0][1]
610/64: Schedule_day[0][0]
610/65: Schedule_day[0][1]
610/66: Schedule_day[1][1]
610/67: len(Schedule_day[1][1])
610/68: len(Schedule_day[:][1])
610/69: len(Schedule_day[0][1])
610/70: len(Schedule_day[1][1])
610/71: len(Schedule_day[2][1])
610/72: len(Schedule_day[3][1])
610/73: len(Schedule_day[4][1])
610/74: len(Schedule_day[5][1])
610/75: len(Schedule_day[0][1])
610/76: len(Schedule_day[:][1])
610/77: len(Schedule_day[:][:])
610/78: len(Schedule_day)
610/79: len(Schedule_day[:])
610/80: len(Schedule_day[1])
610/81: len(Schedule_day[0])
610/82: len(Schedule_day[0][0])
610/83: len(Schedule_day[1][0])
610/84: len(Schedule_day[0][1])
610/85: Day_IT = [[Schedule_day[0][1]], [Schedule_day[1][1]]]
610/86: Day_IT
610/87: Day_IT[1]
610/88: Day_IT[0]
610/89: Day_IT[0][0]
610/90: Day_IT[0][1]
610/91: Day_IT[0][0]
610/92: Day_IT[1][0]
610/93: Day_IT[2][0]
610/94: Day_IT[0][0]
610/95:
Day_IT[0][0]
Day_IT.append([Schedule_day[2][1]])
610/96:
#INPUTS
titolo_libro="Angels & Demons"
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
610/97:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Day_IT[d][0], "Day_" + (d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day1, base_2)
        Area.save('Maps/Clean_maps/Maps_path/Day_' + (d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/98: generate_days(Schedule_day)
610/99:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + (d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day1, base_2)
        Area.save('Maps/Clean_maps/Maps_path/Day_' + (d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/100: generate_days(Schedule_day)
610/101:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day1, base_2)
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/102: generate_days(Schedule_day)
610/103:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/104:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/105: Area
610/106:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day1, base_2, 'Day_' + str(d+1) + '_' + titolo_libro)
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/107: generate_days(Schedule_day)
610/108: Day_IT = [[Schedule_day[0][1]], [Schedule_day[1][1]]]
610/109:
Day_IT[0][0]
Day_IT.append([Schedule_day[2][1]])
Day_IT
610/110:
Day_IT[0][0]
Day_IT.append([Schedule_day[2][1]])
Day_IT[2][0]
610/111: xx=generate_days(Schedule_day)
610/112: xx
610/113: xx[1]
610/114: xx[1][1]
610/115: xx[1][0]
610/116: xx[0][0]
610/117: xx[1][0]
610/118: xx[2][0]
610/119: xx[3][0]
610/120: xx[4][0]
610/121:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro)
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/122: xx=generate_days(Schedule_day)
610/123:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT)+'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/124: xx=generate_days(Schedule_day)
610/125:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +'<br>Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/126: xx=generate_days(Schedule_day)
610/127:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro + '.html')
    return(Day_IT)
610/128: xx=generate_days(Schedule_day)
610/129:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + 'CLT: ' + str(CLT) +'_NAT: ' + str(NAT)+'_REC: ' + str(REC)+'_SPEED: ' + str(SPEED)
        + '.html')
    return(Day_IT)
610/130: xx=generate_days(Schedule_day)
610/131:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + 'CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED: ' + str(SPEED)
        + '.html')
    return(Day_IT)
610/132: xx=generate_days(Schedule_day)
610/133: xx=generate_days(Schedule_day)
610/134:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + 'CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)
        + '.html')
    return(Day_IT)
610/135:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])
        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))
        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + 'CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)
        + '.html')
    return(Day_IT)
610/136: xx=generate_days(Schedule_day)
610/137: #API_values
610/138: Points[2]
610/139:
#Select POIs
def GetPOIs(api_key, location_med, type_loc, days, CLT, NAT, REC, SPEED):
    API_values=[]
    API_values=GetPlaces(api_key, location_med, type_loc)
    POIs_ext=API_values.sort_values(["Popularity","Rating"], ascending=[False, False])
    if type_loc=="tourist_attraction":
        #POIs_ext=POIs_ext[POIs_ext["Rating"].str.contains("")==False]
        #POIs_ext = POIs_ext.applymap(str)
        POIs_ext=POIs_ext.head(days*SPEED)
        POIs_ext.Rating=POIs_ext.Rating.apply(str)
        POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    elif type_loc=="museum":
        POIs_ext=POIs_ext.head(days*SPEED*CLT)
        POIs_ext.Rating=POIs_ext.Rating.apply(str)
        POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    elif type_loc=="park":
        POIs_ext=POIs_ext.head(days*SPEED*NAT)
        POIs_ext.Rating=POIs_ext.Rating.apply(str)
        POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    elif type_loc=="night_club":
        POIs_ext=POIs_ext.head(days*REC)
        POIs_ext.Rating=POIs_ext.Rating.apply(str)
        POIs_ext.Popularity=POIs_ext.Popularity.apply(str)
    return POIs_ext
610/140:
#Tourists attraction
POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
610/141: POIs_ext
610/142:
#Tourists attraction
POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
POIS_ext["type"]="TOUR"
#night_clubs
POIs_ext_2=GetPOIs(api_key, location_med, Points[4], days, CLT, NAT, REC, SPEED)
POIS_ext_2["type"]="CLUB"
#museums
POIs_ext_3=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
POIS_ext_3["type"]="MUSE"
#parks
POIs_ext_4=GetPOIs(api_key, location_med, Points[8], days, CLT, NAT, REC, SPEED)
POIS_ext_4["type"]="MUSE"
610/143:
#Tourists attraction
POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
POIs_ext["type"]="TOUR"
#night_clubs
POIs_ext_2=GetPOIs(api_key, location_med, Points[4], days, CLT, NAT, REC, SPEED)
POIs_ext_2["type"]="CLUB"
#museums
POIs_ext_3=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
POIs_ext_3["type"]="MUSE"
#parks
POIs_ext_4=GetPOIs(api_key, location_med, Points[8], days, CLT, NAT, REC, SPEED)
POIs_ext_4["type"]="PRK"
610/144: POIs_ext.append([POIs_ext_2],[POIs_ext_3], [POIs_ext_4])
610/145:
#Tourists attraction
POIs_ext=GetPOIs(api_key, location_med, Points[2], days, CLT, NAT, REC, SPEED)
POIs_ext["type"]="TOUR"
#night_clubs
POIs_ext_2=GetPOIs(api_key, location_med, Points[4], days, CLT, NAT, REC, SPEED)
POIs_ext_2["type"]="CLUB"
#museums
POIs_ext_3=GetPOIs(api_key, location_med, Points[6], days, CLT, NAT, REC, SPEED)
POIs_ext_3["type"]="MUSE"
#parks
POIs_ext_4=GetPOIs(api_key, location_med, Points[8], days, CLT, NAT, REC, SPEED)
POIs_ext_4["type"]="PRK"
610/146:
frames = [POIs_ext, POIs_ext_2,POIs_ext_3, POIs_ext_4]
POIs_all=pd.concat(frames)
610/147: POIs_all
610/148: POIs_all.reset_index()
610/149: POIs_all.reset_index(drop=True)
610/150: POIs_all=POIs_all.reset_index(drop=True)
610/151:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': POIs_ext.type, 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name)
    + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-1]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
610/152: df_unicos=add_POIs_df(df, POIs_ext)
610/153: df_unicos=add_POIs_df(df, POIs_all)
610/154:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
610/155: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
610/156:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/157:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/158: Area
610/159: dat1
610/160: df_unicos=add_POIs_df(df, POIs_all)
610/161: df_unicos
610/162: POIs_all
610/163:
def add_POIs_df(df, POIs_ext):
    df=df.sort_values("Position", ascending=True)
    df["Quotes_total"]="<b>" + df['Position'].astype(str) + "</b>"+ "<br>" + df['Quotes']
    df["labels"]="LIB"

    #hh=df.groupby(['LON_google']).agg(lambda col: '\n'.join(col))
    df_unicos=df.drop_duplicates(subset ="LON_google") 
    df_unicos= df_unicos.append(pd.DataFrame({'LAT_google': POIs_ext.LAT, 'LON_google': POIs_ext.LON,
    'labels': POIs_ext.type, 'lugares': '<a href="'+ (POIs_ext.Website)  +'"target="_blank"> ' + (POIs_ext.Name)
    + ' </a>' +
    "<br><b>Rating: </b>" + (POIs_ext.Rating) +
    #"<br><b>Website: </b>" + (POIs_ext.Website) +
    "<br><b>Popularity: </b><br>" + (POIs_ext.Popularity), 'Quotes': POIs_ext.iloc[:,-2]}), ignore_index=True)

    s=df.assign(count=1).groupby(['LON_google','LAT_google']).agg({'count':'sum',
    'Quotes_total':lambda x : '<br>'.join(set(x))}).reset_index()
    
    for n in range(s.shape[0]):
        for m in range(s.shape[0]):
            if df_unicos["LON_google"].iloc[m] == s["LON_google"].iloc[n]:
                df_unicos["Quotes"].iloc[m] = s["Quotes_total"].iloc[n]
    
    return df_unicos
610/164: df_unicos=add_POIs_df(df, POIs_all)
610/165: POIs_ext_4
610/166: df_unicos
610/167:
def path_dots(df_unicos, Book_name, Hotel_Choosen):

    df_unicos=df_unicos.append(pd.DataFrame({'LAT_google': Hotel_Choosen.LAT, 'LON_google': Hotel_Choosen.LON,
    'labels': "HOTEL", 'lugares': '<a href="'+ (Hotel_Choosen.Website)  +'"target="_blank"> ' + (Hotel_Choosen.Name) + ' </a>' +
    "<br><b>Rating: </b>" + str(Hotel_Choosen.Rating) +
    #"<br><b>Website: </b>" + (Hotel_Choosen.Website) +
    "<br><b>Popularity: </b>" + str(Hotel_Choosen.Popularity), 'Quotes': Hotel_Choosen[-1]}, index=[0]), ignore_index=True)
    df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
   # df_unicos = df_unicos.iloc[np.arange(-1, len(df_unicos)-1)]
    #   #Charge POIs
    #POIs_ext=POIs_ext.head(5)
   
    sources=df_unicos.iloc[:,3:5].values.tolist()
    distance_matrix = great_circle_distance_matrix(sources)
    Matriz_dist=pd.DataFrame(distance_matrix)
    #Matriz_dist.to_csv("matriz_dist_"+ Book_name +".csv")
    new_order=[0]
    distance=[0]
    Bridge=Matriz_dist
    for i in range(len(Matriz_dist)-1):
        #index=Bridge.index[i]
        pos=new_order[i]
        Bridge=Bridge.sort_values(pos)
        new_order.append(Bridge.index[1])
        distance.append(Bridge.iloc[1][pos])
        Bridge=Bridge.drop(Bridge.index[0])
        #print(new_order, len(Bridge))
    df_unicos['new_order']=new_order
    df_unicos['distance']=distance
    df_unicos=df_unicos.reset_index()
    dat1 = pd.DataFrame([])
    for n in range(df_unicos.shape[0]):
        for m in range(df_unicos.shape[0]):
            if df_unicos.index[m] == new_order[n]:
                dat1 = dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[m], 'LON': df_unicos.LON_google[m], 
                'order': df_unicos.new_order[n], 'Distance [m]': int(df_unicos.distance[n]), 'lugares': df_unicos.lugares[m],
                'quotes': df_unicos.Quotes[m], 'Position_book': df_unicos.Position[m], 'Type': df_unicos.labels[m]}, index=[0]), ignore_index=True)
    
    dat1=dat1.append(pd.DataFrame({'LAT': df_unicos.LAT_google[0], 'LON': df_unicos.LON_google[0], 
                'order': df_unicos.new_order[0], 'Distance [m]': int(df_unicos.distance[0]), 'lugares': df_unicos.lugares[0],
                'quotes': df_unicos.Quotes[0], 'Position_book': df_unicos.Position[0], 'Type': df_unicos.labels[0]}, index=[0]), ignore_index=True)

    dat1.to_csv("Data/Clean_data/Path/Path_" + Book_name + ".csv")
    print("saved")
    return dat1
610/168: dat1=path_dots(df_unicos, base_2, Hotel_Choosen)
610/169: dat1
610/170:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/171:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/172: Area
610/173:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
610/174: len(Schedule_day[0][1])
610/175:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1.head(10), base_2, "ddd")
610/176: Area
610/177:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='green', icon="glyphicon-park", icon_color="white")
        elif typ=="MUSE":
            Icon= folium.Icon(color='green', icon="glyphicon-museum", icon_color="white")
         elif typ=="CLUB":
            Icon= folium.Icon(color='green', icon="glyphicon-club", icon_color="white")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/179:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/180:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='green', icon="glyphicon-park", icon_color="white")
        elif typ=="MUSE":
            Icon= folium.Icon(color='green', icon="glyphicon-museum", icon_color="white")
        elif typ=="CLUB":
            Icon= folium.Icon(color='green', icon="glyphicon-club", icon_color="white")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/181:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/182: Area
610/183:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-grain", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="glyphicon-museum", icon_color="white")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-cocktail", icon_color="white")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/184:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/185: Area
610/186:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='white', icon="glyphicon-camera", icon_color="green")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="glyphicon-museum", icon_color="white")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="white")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/187:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/188: Area
610/189:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="glyphicon-museum", icon_color="brown")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="red")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/190:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/191: Area
610/192:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="glyphicon-bank", icon_color="brown")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="red")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/193:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/194: Area
610/195:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='gray', icon="glyphicon-bank", icon_color="brown")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="red")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/196:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/197: Area
610/198:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="glyphicon-bank", icon_color="gray")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="red")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/199:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/200: Area
610/201:
def plot_path(dat1, Book_name, title):
    import folium
    from folium import plugins
    lugares3=dat1
    #ff=len(lugares3)
    # let's start again with a clean copy of the map of San Francisco
    Figure=folium.Figure(width=550, height=550)
    
    Area = folium.Map(location=[lugares3["LAT"].iloc[0], lugares3["LON"].iloc[0]], control_scale=True, zoom_start=12)
    Dots = plugins.MarkerCluster().add_to(Area)

    # loop through the dataframe and add each data point to the mark cluster
    for lat, lng, label, label_2, typ in zip(lugares3["LAT"], lugares3["LON"], lugares3["quotes"], lugares3["lugares"],
    lugares3["Type"]):
    
        html="<b>" + label_2 +"</b>" + "<br>" + label
        iframe = folium.IFrame(html,
                       width=500,
                       height=100)
        if typ=="LIB":
            Icon= folium.Icon(color='red', icon="book", prefix='fa', icon_color="white")
        elif typ=="HOTEL":
            Icon= folium.Icon(color='blue', icon="hotel", prefix='fa', icon_color="white")
        elif typ=="TOUR":
            Icon= folium.Icon(color='green', icon="glyphicon-camera", icon_color="white")   
        elif typ=="PRK":
            Icon= folium.Icon(color='white', icon="glyphicon-tree-conifer", icon_color="green")
        elif typ=="MUSE":
            Icon= folium.Icon(color='white', icon="university", prefix='fa', icon_color="gray")
        elif typ=="CLUB":
            Icon= folium.Icon(color='white', icon="glyphicon-glass", icon_color="red")        
             
        if type(lat)!=type(None):
            folium.Marker(
            location=[lat, lng],
            icon=Icon,
            popup=folium.Popup(iframe,max_width=500),
        ).add_to(Dots)
    #partenza hotel
    loc_start=lugares3.iloc[0:2,0:2]
    loc_start=loc_start.values.tolist()
    folium.PolyLine(loc_start, color='blue', weight=10, opacity=0.5).add_to(Area)
    #Percorso luoghi
    loc=lugares3.iloc[1:-1,0:2]
    loc=loc.values.tolist()
    folium.PolyLine(loc, color='red', weight=10, opacity=0.5).add_to(Area)
    #Ritorno hotel
    loc_end=lugares3.iloc[-2:-1, 0:2]
    loc_end=loc_end.append(lugares3.iloc[0:1, 0:2])
    loc_end=loc_end.values.tolist()
    folium.PolyLine(loc_end, color='blue', weight=10, opacity=0.5).add_to(Area)

    title_html = '''
     <head><style> html { overflow-y: hidden; } </style></head>
     <h3 align="center" style="font-size:18px"><b>''' + title + '''</b></h3>
     ''' 
    Figure.add_child(Area)
    Area.get_root().html.add_child(folium.Element(title_html))
    # mini_map = plugins.MiniMap(toggle_display=True)
    # # add the mini map to the big map
    # Area.add_child(mini_map)
    Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
    return Area
610/202:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "ddd")
610/203: Area
610/204:
 #Clusterdays
def divide_days(df,days):

    dat_dummy=df
    kmeans = KMeans(n_clusters=days).fit(dat_dummy.iloc[:,3:5])
    dat_dummy["day"]=kmeans.fit_predict(dat_dummy.iloc[:,3:5])
    # centroids = kmeans.cluster_centers_
    df_by_day = dat_dummy.groupby('day')
    Schedule_day=(list(df_by_day))
    # ax=plt.scatter(dat_dummy['LON_google'], dat_dummy['LAT_google'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
    # #ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
    # ax
    return Schedule_day
610/205:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])

        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro 
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT)
        +'<br>Speed: ' + str(SPEED))


        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + 'CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)
        + '.html')
        
    return(Day_IT)
610/206: xx=generate_days(Schedule_day)
610/207:
#Save Area.save('Maps/Clean_maps/Maps_path/Map_path_' + Book_name +'.html')
Area=plot_path(dat1, base_2, "Total map")
610/208: Area
610/209:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])

        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro + ' total days: ' + days
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT) +' Recreation: ' + str(REC)
        +'<br>Speed: ' + str(SPEED) +' Budget: ' + str(BUDGET))


        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + '_CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)+'_BUDGET_' + str(BUDGET) 
        + '_tot_days_' + days
        + '.html')

    return(Day_IT)
610/210: xx=generate_days(Schedule_day)
610/211:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])

        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro + ' total days: ' + days
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT) +' Recreation: ' + str(REC)
        +'<br>Speed: ' + str(SPEED) +' Budget: ' + str(BUDGET))


        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + '_CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)+'_BUDGET_' + str(BUDGET) 
        + '_tot_days_' + str(days)
        + '.html')

    return(Day_IT)
610/212: xx=generate_days(Schedule_day)
610/213:
def generate_days(Schedule_day):
    Day_IT = []
    total_days=len(Schedule_day)
    Schedule_day=divide_days(df_unicos,days)
    for d in range(0, total_days):
        #Day_IT.append([Schedule_day[d][1]])
        data_day=path_dots(Schedule_day[d][1], "Day_" + str(d+1) + "_" + titolo_libro, Hotel_Choosen)
        Day_IT.append([data_day])

        Area=plot_path(data_day, base_2, 'Day_' + str(d+1) + '_' + titolo_libro + ' total days: ' + str(days)
        +'<br>Culture: ' + str(CLT) +' Nature: ' + str(NAT) +' Recreation: ' + str(REC)
        +'<br>Speed: ' + str(SPEED) +' Budget: ' + str(BUDGET))


        Area.save('Maps/Clean_maps/Maps_path/Day_' + str(d+1) + '_' + titolo_libro 
        + '_CLT_' + str(CLT) +'_NAT_' + str(NAT)+'_REC_' + str(REC)+'_SPEED_' + str(SPEED)+'_BUDGET_' + str(BUDGET) 
        + '_tot_days_' + str(days)
        + '.html')

    return(Day_IT)
610/214: xx=generate_days(Schedule_day)
   1:
import numpy as np
import pandas as pd
import folium
import googlemaps
from Book_extraction_single import search_for_file_path
g_key=googlemaps.Client(key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I")
import matplotlib.pyplot as plt
# import mlrose
# from ortools.constraint_solver import routing_enums_pb2
# from ortools.constraint_solver import pywrapcp
from python_tsp.distances import great_circle_distance_matrix
from python_tsp.exact import solve_tsp_dynamic_programming
api_key="AIzaSyAJ0DKhauX591z08eBbYxtcVjbFOZLfd2I"
from GooglePlaces import GooglePlaces
from sklearn.cluster import KMeans
Points=["lodging","bar","tourist_attraction", "restaurant", "night_club", "art", "museum", "church", "park"]
from Analysis_data import GetPlaces
import datetime 
from datetime import date, time, datetime
   2:
#INPUTS
titolo_libro="Angels & Demons"
start_time = date(year=2021, month=4, day=25)
finish_time = date(year=2021, month=4, day=30)
difference = finish_time - start_time
days=difference.days
#1 to 5
#number of POIs choosen from the days of availability
#Priority to popularity and ratings
CLT=5
#Museums, art, Churches
NAT=1
#Parks
REC=2
#Pubs, Night clubs
#1 relax, 2 mid, 3 full speed
#time for each visit in f(speed) and start-end points
SPEED=3
#BUDGET $=order POIs from lowest to highest, $$=mid, avg between all, $$$= highest
BUDGET=2
   3:
#SELECT DATA FILE (BOOK)
path_file_2, base_2=search_for_file_path ()
   4:
df=pd.read_csv(path_file_2)

#define loc med from df selected
   5: #df
   6: %history -g -f "Ultimo_update.txt"
